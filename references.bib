@ARTICLE{Ahuja2023-qq,
  title         = "{MEGA}: Multilingual Evaluation of Generative {AI}",
  author        = "Ahuja, Kabir and Diddee, Harshita and Hada, Rishav and
                   Ochieng, Millicent and Ramesh, Krithika and Jain, Prachi and
                   Nambi, Akshay and Ganu, Tanuja and Segal, Sameer and Axmed,
                   Maxamed and Bali, Kalika and Sitaram, Sunayana",
  abstract      = "Generative AI models have impressive performance on many
                   Natural Language Processing tasks such as language
                   understanding, reasoning and language generation. One of the
                   most important questions that is being asked by the AI
                   community today is about the capabilities and limits of
                   these models, and it is clear that evaluating generative AI
                   is very challenging. Most studies on generative Large
                   Language Models (LLMs) are restricted to English and it is
                   unclear how capable these models are at understanding and
                   generating other languages. We present the first
                   comprehensive benchmarking of generative LLMs - MEGA, which
                   evaluates models on standard NLP benchmarks, covering 8
                   diverse tasks and 33 typologically diverse languages. We
                   also compare the performance of generative LLMs to State of
                   the Art (SOTA) non-autoregressive models on these tasks to
                   determine how well generative models perform compared to the
                   previous generation of LLMs. We present a thorough analysis
                   of the performance of models across languages and discuss
                   some of the reasons why generative LLMs are currently not
                   optimal for all languages. We create a framework for
                   evaluating generative LLMs in the multilingual setting and
                   provide directions for future progress in the field.",
  month         =  mar,
  year          =  2023,
  keywords      = "Multilingual",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2303.12528"
}

@ARTICLE{Zhao2023-go,
  title         = "A Survey of Large Language Models",
  author        = "Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi
                   and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and
                   Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du,
                   Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and
                   Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu
                   and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen,
                   Ji-Rong",
  abstract      = "Language is essentially a complex, intricate system of human
                   expressions governed by grammatical rules. It poses a
                   significant challenge to develop capable AI algorithms for
                   comprehending and grasping a language. As a major approach,
                   language modeling has been widely studied for language
                   understanding and generation in the past two decades,
                   evolving from statistical language models to neural language
                   models. Recently, pre-trained language models (PLMs) have
                   been proposed by pre-training Transformer models over
                   large-scale corpora, showing strong capabilities in solving
                   various NLP tasks. Since researchers have found that model
                   scaling can lead to performance improvement, they further
                   study the scaling effect by increasing the model size to an
                   even larger size. Interestingly, when the parameter scale
                   exceeds a certain level, these enlarged language models not
                   only achieve a significant performance improvement but also
                   show some special abilities that are not present in
                   small-scale language models. To discriminate the difference
                   in parameter scale, the research community has coined the
                   term large language models (LLM) for the PLMs of significant
                   size. Recently, the research on LLMs has been largely
                   advanced by both academia and industry, and a remarkable
                   progress is the launch of ChatGPT, which has attracted
                   widespread attention from society. The technical evolution
                   of LLMs has been making an important impact on the entire AI
                   community, which would revolutionize the way how we develop
                   and use AI algorithms. In this survey, we review the recent
                   advances of LLMs by introducing the background, key
                   findings, and mainstream techniques. In particular, we focus
                   on four major aspects of LLMs, namely pre-training,
                   adaptation tuning, utilization, and capacity evaluation.
                   Besides, we also summarize the available resources for
                   developing LLMs and discuss the remaining issues for future
                   directions.",
  month         =  mar,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2303.18223v11"
}

@ARTICLE{Cahyawijaya2023-dd,
  title         = "{Instruct-Align}: Teaching novel languages with to {LLMs}
                   through alignment-based cross-lingual instruction",
  author        = "Cahyawijaya, Samuel and Lovenia, Holy and Yu, Tiezheng and
                   Chung, Willy and Fung, Pascale",
  abstract      = "Instruction-tuned large language models (LLMs) have shown
                   remarkable generalization capability over multiple tasks in
                   multiple languages. Nevertheless, their generalization
                   towards different languages varies especially to
                   underrepresented languages or even to unseen languages.
                   Prior works on adapting new languages to LLMs find that
                   naively adapting new languages to instruction-tuned LLMs
                   will result in catastrophic forgetting, which in turn causes
                   the loss of multitasking ability in these LLMs. To tackle
                   this, we propose the Instruct-Align a.k.a (IA)$^1$
                   framework, which enables instruction-tuned LLMs to learn
                   cross-lingual alignment between unseen and previously
                   learned languages via alignment-based cross-lingual
                   instruction-tuning. Our preliminary result on BLOOMZ-560M
                   shows that (IA)$^1$ is able to learn a new language
                   effectively with only a limited amount of parallel data and
                   at the same time prevent catastrophic forgetting by applying
                   continual instruction-tuning through experience replay. Our
                   work contributes to the progression of language adaptation
                   methods for instruction-tuned LLMs and opens up the
                   possibility of adapting underrepresented low-resource
                   languages into existing instruction-tuned LLMs. Our code
                   will be publicly released upon acceptance.",
  month         =  may,
  year          =  2023,
  copyright     = "http://creativecommons.org/licenses/by-sa/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.13627"
}

@ARTICLE{Liu2023-mw,
  title         = "{AgentBench}: Evaluating {LLMs} as Agents",
  author        = "Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and
                   Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang
                   and Men, Kaiwen and Yang, Kejuan and Zhang, Shudan and Deng,
                   Xiang and Zeng, Aohan and Du, Zhengxiao and Zhang, Chenhui
                   and Shen, Sheng and Zhang, Tianjun and Su, Yu and Sun, Huan
                   and Huang, Minlie and Dong, Yuxiao and Tang, Jie",
  abstract      = "Large Language Models (LLMs) are becoming increasingly smart
                   and autonomous, targeting real-world pragmatic missions
                   beyond traditional NLP tasks. As a result, there has been an
                   urgent need to evaluate LLMs as agents on challenging tasks
                   in interactive environments. We present AgentBench, a
                   multi-dimensional evolving benchmark that currently consists
                   of 8 distinct environments to assess LLM-as-Agent's
                   reasoning and decision-making abilities in a multi-turn
                   open-ended generation setting. Our extensive test over 25
                   LLMs (including APIs and open-sourced models) shows that,
                   while top commercial LLMs present a strong ability of acting
                   as agents in complex environments, there is a significant
                   disparity in performance between them and open-sourced
                   competitors. It also serves as a component of an ongoing
                   project with wider coverage and deeper consideration towards
                   systematic LLM evaluation. Datasets, environments, and an
                   integrated evaluation package for AgentBench are released at
                   https://github.com/THUDM/AgentBench",
  month         =  aug,
  year          =  2023,
  keywords      = "Agent",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2308.03688"
}

@ARTICLE{Pan2023-vy,
  title         = "Do {LLMs} Possess a Personality? Making the {MBTI} Test an
                   Amazing Evaluation for Large Language Models",
  author        = "Pan, Keyu and Zeng, Yawen",
  abstract      = "The field of large language models (LLMs) has made
                   significant progress, and their knowledge storage capacity
                   is approaching that of human beings. Furthermore, advanced
                   techniques, such as prompt learning and reinforcement
                   learning, are being employed to address ethical concerns and
                   hallucination problems associated with LLMs, bringing them
                   closer to aligning with human values. This situation
                   naturally raises the question of whether LLMs with
                   human-like abilities possess a human-like personality? In
                   this paper, we aim to investigate the feasibility of using
                   the Myers-Briggs Type Indicator (MBTI), a widespread human
                   personality assessment tool, as an evaluation metric for
                   LLMs. Specifically, extensive experiments will be conducted
                   to explore: 1) the personality types of different LLMs, 2)
                   the possibility of changing the personality types by prompt
                   engineering, and 3) How does the training dataset affect the
                   model's personality. Although the MBTI is not a rigorous
                   assessment, it can still reflect the similarity between LLMs
                   and human personality. In practice, the MBTI has the
                   potential to serve as a rough indicator. Our codes are
                   available at
                   https://github.com/HarderThenHarder/transformers\_tasks/tree/main/LLM/llms\_mbti.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.16180"
}

@ARTICLE{Pryzant2023-td,
  title         = "Automatic Prompt Optimization with ``Gradient Descent'' and
                   Beam Search",
  author        = "Pryzant, Reid and Iter, Dan and Li, Jerry and Lee, Yin Tat
                   and Zhu, Chenguang and Zeng, Michael",
  abstract      = "Large Language Models (LLMs) have shown impressive
                   performance as general purpose agents, but their abilities
                   remain highly dependent on prompts which are hand written
                   with onerous trial-and-error effort. We propose a simple and
                   nonparametric solution to this problem, Automatic Prompt
                   Optimization (APO), which is inspired by numerical gradient
                   descent to automatically improve prompts, assuming access to
                   training data and an LLM API. The algorithm uses minibatches
                   of data to form natural language ``gradients'' that
                   criticize the current prompt. The gradients are then
                   ``propagated'' into the prompt by editing the prompt in the
                   opposite semantic direction of the gradient. These gradient
                   descent steps are guided by a beam search and bandit
                   selection procedure which significantly improves algorithmic
                   efficiency. Preliminary results across three benchmark NLP
                   tasks and the novel problem of LLM jailbreak detection
                   suggest that Automatic Prompt Optimization can outperform
                   prior prompt editing techniques and improve an initial
                   prompt's performance by up to 31\%, by using data to rewrite
                   vague task descriptions into more precise annotation
                   instructions.",
  month         =  may,
  year          =  2023,
  keywords      = "Agent",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.03495"
}

@ARTICLE{Qin2023-vp,
  title         = "{ToolLLM}: Facilitating Large Language Models to Master
                   16000+ Real-world {APIs}",
  author        = "Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun
                   and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and
                   Tang, Xiangru and Qian, Bill and Zhao, Sihan and Tian,
                   Runchu and Xie, Ruobing and Zhou, Jie and Gerstein, Mark and
                   Li, Dahai and Liu, Zhiyuan and Sun, Maosong",
  abstract      = "Despite the advancements of open-source large language
                   models (LLMs) and their variants, e.g., LLaMA and Vicuna,
                   they remain significantly limited in performing higher-level
                   tasks, such as following human instructions to use external
                   tools (APIs). This is because current instruction tuning
                   largely focuses on basic language tasks instead of the
                   tool-use domain. This is in contrast to state-of-the-art
                   (SOTA) LLMs, e.g., ChatGPT, which have demonstrated
                   excellent tool-use capabilities but are unfortunately closed
                   source. To facilitate tool-use capabilities within
                   open-source LLMs, we introduce ToolLLM, a general tool-use
                   framework of data construction, model training and
                   evaluation. We first present ToolBench, an
                   instruction-tuning dataset for tool use, which is created
                   automatically using ChatGPT. Specifically, we collect 16,464
                   real-world RESTful APIs spanning 49 categories from RapidAPI
                   Hub, then prompt ChatGPT to generate diverse human
                   instructions involving these APIs, covering both single-tool
                   and multi-tool scenarios. Finally, we use ChatGPT to search
                   for a valid solution path (chain of API calls) for each
                   instruction. To make the searching process more efficient,
                   we develop a novel depth-first search-based decision tree
                   (DFSDT), enabling LLMs to evaluate multiple reasoning traces
                   and expand the search space. We show that DFSDT
                   significantly enhances the planning and reasoning
                   capabilities of LLMs. For efficient tool-use assessment, we
                   develop an automatic evaluator: ToolEval. We fine-tune LLaMA
                   on ToolBench and obtain ToolLLaMA. Our ToolEval reveals that
                   ToolLLaMA demonstrates a remarkable ability to execute
                   complex instructions and generalize to unseen APIs, and
                   exhibits comparable performance to ChatGPT. To make the
                   pipeline more practical, we devise a neural API retriever to
                   recommend appropriate APIs for each instruction, negating
                   the need for manual API selection.",
  month         =  jul,
  year          =  2023,
  keywords      = "Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2307.16789"
}

@ARTICLE{Azerbayev2023-gb,
  title         = "Llemma: An Open Language Model For Mathematics",
  author        = "Azerbayev, Zhangir and Schoelkopf, Hailey and Paster, Keiran
                   and Dos Santos, Marco and McAleer, Stephen and Jiang, Albert
                   Q and Deng, Jia and Biderman, Stella and Welleck, Sean",
  abstract      = "We present Llemma, a large language model for mathematics.
                   We continue pretraining Code Llama on the Proof-Pile-2, a
                   mixture of scientific papers, web data containing
                   mathematics, and mathematical code, yielding Llemma. On the
                   MATH benchmark Llemma outperforms all known open base
                   models, as well as the unreleased Minerva model suite on an
                   equi-parameter basis. Moreover, Llemma is capable of tool
                   use and formal theorem proving without any further
                   finetuning. We openly release all artifacts, including 7
                   billion and 34 billion parameter models, the Proof-Pile-2,
                   and code to replicate our experiments.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.10631"
}

@ARTICLE{Tay2022-un,
  title         = "{UL2}: Unifying Language Learning Paradigms",
  author        = "Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia,
                   Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won
                   and Shakeri, Siamak and Bahri, Dara and Schuster, Tal and
                   Zheng, Huaixiu Steven and Zhou, Denny and Houlsby, Neil and
                   Metzler, Donald",
  abstract      = "Existing pre-trained models are generally geared towards a
                   particular class of problems. To date, there seems to be
                   still no consensus on what the right architecture and
                   pre-training setup should be. This paper presents a unified
                   framework for pre-training models that are universally
                   effective across datasets and setups. We begin by
                   disentangling architectural archetypes with pre-training
                   objectives -- two concepts that are commonly conflated.
                   Next, we present a generalized \& unified perspective for
                   self-supervision in NLP and show how different pre-training
                   objectives can be cast as one another and how interpolating
                   between different objectives can be effective. We then
                   propose Mixture-of-Denoisers (MoD), a pre-training objective
                   that combines diverse pre-training paradigms together. We
                   furthermore introduce a notion of mode switching, wherein
                   downstream fine-tuning is associated with specific
                   pre-training schemes. We conduct extensive ablative
                   experiments to compare multiple pre-training objectives and
                   find that our method pushes the Pareto-frontier by
                   outperforming T5 \& GPT-like models across multiple diverse
                   setups. By scaling our model up to 20B parameters, we
                   achieve SOTA performance on 50 well-established supervised
                   finetuning based NLP tasks. Our model also achieve strong
                   results at in-context learning, outperforming 175B GPT-3 on
                   zero-shot SuperGLUE and tripling the performance of T5-XXL
                   on one-shot summarization. On 0-shot MMLU, UL2 20B
                   outperforms T0 and T5 models. UL2 20B also works well with
                   chain-of-thought prompting and reasoning, making it an
                   appealing choice for research into reasoning at a small to
                   medium scale of 20B parameters. Finally, we apply FLAN
                   instruction tuning to the UL2 20B model, achieving MMLU and
                   Big-Bench scores competitive to FLAN-PaLM 62B. We release
                   Flax-based T5X checkpoints for the UL2 20B \& Flan-UL2 20B.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2205.05131"
}

@ARTICLE{Valmeekam2023-zi,
  title         = "On the Planning Abilities of Large Language Models -- A
                   Critical Investigation",
  author        = "Valmeekam, Karthik and Marquez, Matthew and Sreedharan,
                   Sarath and Kambhampati, Subbarao",
  abstract      = "Intrigued by the claims of emergent reasoning capabilities
                   in LLMs trained on general web corpora, in this paper, we
                   set out to investigate their planning capabilities. We aim
                   to evaluate (1) the effectiveness of LLMs in generating
                   plans autonomously in commonsense planning tasks and (2) the
                   potential of LLMs as a source of heuristic guidance for
                   other agents (AI planners) in their planning tasks. We
                   conduct a systematic study by generating a suite of
                   instances on domains similar to the ones employed in the
                   International Planning Competition and evaluate LLMs in two
                   distinct modes: autonomous and heuristic. Our findings
                   reveal that LLMs' ability to generate executable plans
                   autonomously is rather limited, with the best model (GPT-4)
                   having an average success rate of ~12\% across the domains.
                   However, the results in the heuristic mode show more
                   promise. In the heuristic mode, we demonstrate that
                   LLM-generated plans can improve the search process for
                   underlying sound planners and additionally show that
                   external verifiers can help provide feedback on the
                   generated plans and back-prompt the LLM for better plan
                   generation.",
  month         =  may,
  year          =  2023,
  keywords      = "Agent",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2305.15771"
}

@ARTICLE{Chen2023-vt,
  title         = "When Large Language Models Meet Personalization:
                   Perspectives of Challenges and Opportunities",
  author        = "Chen, Jin and Liu, Zheng and Huang, Xu and Wu, Chenwang and
                   Liu, Qi and Jiang, Gangwei and Pu, Yuanhao and Lei, Yuxuan
                   and Chen, Xiaolong and Wang, Xingmei and Lian, Defu and
                   Chen, Enhong",
  abstract      = "The advent of large language models marks a revolutionary
                   breakthrough in artificial intelligence. With the
                   unprecedented scale of training and model parameters, the
                   capability of large language models has been dramatically
                   improved, leading to human-like performances in
                   understanding, language synthesizing, and common-sense
                   reasoning, etc. Such a major leap-forward in general AI
                   capacity will change the pattern of how personalization is
                   conducted. For one thing, it will reform the way of
                   interaction between humans and personalization systems.
                   Instead of being a passive medium of information filtering,
                   large language models present the foundation for active user
                   engagement. On top of such a new foundation, user requests
                   can be proactively explored, and user's required information
                   can be delivered in a natural and explainable way. For
                   another thing, it will also considerably expand the scope of
                   personalization, making it grow from the sole function of
                   collecting personalized information to the compound function
                   of providing personalized services. By leveraging large
                   language models as general-purpose interface, the
                   personalization systems may compile user requests into
                   plans, calls the functions of external tools to execute the
                   plans, and integrate the tools' outputs to complete the
                   end-to-end personalization tasks. Today, large language
                   models are still being developed, whereas the application in
                   personalization is largely unexplored. Therefore, we
                   consider it to be the right time to review the challenges in
                   personalization and the opportunities to address them with
                   LLMs. In particular, we dedicate this perspective paper to
                   the discussion of the following aspects: the development and
                   challenges for the existing personalization system, the
                   newly emerged capabilities of large language models, and the
                   potential ways of making use of large language models for
                   personalization.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2307.16376"
}

@ARTICLE{Lin2023-ka,
  title         = "The Unlocking Spell on Base {LLMs}: Rethinking Alignment via
                   {In-Context} Learning",
  author        = "Lin, Bill Yuchen and Ravichander, Abhilasha and Lu, Ximing
                   and Dziri, Nouha and Sclar, Melanie and Chandu, Khyathi and
                   Bhagavatula, Chandra and Choi, Yejin",
  abstract      = "The alignment tuning process of large language models (LLMs)
                   typically involves instruction learning through supervised
                   fine-tuning (SFT) and preference tuning via reinforcement
                   learning from human feedback (RLHF). A recent study, LIMA
                   (Zhou et al. 2023), shows that using merely 1K examples for
                   SFT can achieve significant alignment performance as well,
                   suggesting that the effect of alignment tuning might be
                   ``superficial.'' This raises questions about how exactly the
                   alignment tuning transforms a base LLM. We analyze the
                   effect of alignment tuning by examining the token
                   distribution shift between base LLMs and their aligned
                   counterpart. Our findings reveal that base LLMs and their
                   alignment-tuned versions perform nearly identically in
                   decoding on the majority of token positions. Most
                   distribution shifts occur with stylistic tokens. These
                   direct evidence strongly supports the Superficial Alignment
                   Hypothesis suggested by LIMA. Based on these findings, we
                   rethink the alignment of LLMs by posing the research
                   question: how effectively can we align base LLMs without SFT
                   or RLHF? To address this, we introduce a simple, tuning-free
                   alignment method, URIAL. URIAL achieves effective alignment
                   purely through in-context learning (ICL) with base LLMs,
                   requiring as few as three constant stylistic examples and a
                   system prompt. We conduct a fine-grained and interpretable
                   evaluation on a diverse set of examples, named
                   JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with
                   URIAL can match or even surpass the performance of LLMs
                   aligned with SFT or SFT+RLHF. We show that the gap between
                   tuning-free and tuning-based alignment methods can be
                   significantly reduced through strategic prompting and ICL.
                   Our findings on the superficial nature of alignment tuning
                   and results with URIAL suggest that deeper analysis and
                   theoretical understanding of alignment is crucial to future
                   LLM research.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2312.01552"
}

@ARTICLE{Sun2023-qc,
  title         = "A Short Survey of Viewing Large Language Models in Legal
                   Aspect",
  author        = "Sun, Zhongxiang",
  abstract      = "Large language models (LLMs) have transformed many fields,
                   including natural language processing, computer vision, and
                   reinforcement learning. These models have also made a
                   significant impact in the field of law, where they are being
                   increasingly utilized to automate various legal tasks, such
                   as legal judgement prediction, legal document analysis, and
                   legal document writing. However, the integration of LLMs
                   into the legal field has also raised several legal problems,
                   including privacy concerns, bias, and explainability. In
                   this survey, we explore the integration of LLMs into the
                   field of law. We discuss the various applications of LLMs in
                   legal tasks, examine the legal challenges that arise from
                   their use, and explore the data resources that can be used
                   to specialize LLMs in the legal domain. Finally, we discuss
                   several promising directions and conclude this paper. By
                   doing so, we hope to provide an overview of the current
                   state of LLMs in law and highlight the potential benefits
                   and challenges of their integration.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2303.09136"
}

@ARTICLE{Berner2021-zg,
  title         = "The Modern Mathematics of Deep Learning",
  author        = "Berner, Julius and Grohs, Philipp and Kutyniok, Gitta and
                   Petersen, Philipp",
  abstract      = "We describe the new field of mathematical analysis of deep
                   learning. This field emerged around a list of research
                   questions that were not answered within the classical
                   framework of learning theory. These questions concern: the
                   outstanding generalization power of overparametrized neural
                   networks, the role of depth in deep architectures, the
                   apparent absence of the curse of dimensionality, the
                   surprisingly successful optimization performance despite the
                   non-convexity of the problem, understanding what features
                   are learned, why deep architectures perform exceptionally
                   well in physical problems, and which fine aspects of an
                   architecture affect the behavior of a learning task in which
                   way. We present an overview of modern approaches that yield
                   partial answers to these questions. For selected approaches,
                   we describe the main ideas in more detail.",
  month         =  may,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2105.04026"
}

@ARTICLE{Eldan2023-kk,
  title         = "Who's Harry Potter? Approximate Unlearning in {LLMs}",
  author        = "Eldan, Ronen and Russinovich, Mark",
  abstract      = "Large language models (LLMs) are trained on massive internet
                   corpora that often contain copyrighted content. This poses
                   legal and ethical challenges for the developers and users of
                   these models, as well as the original authors and
                   publishers. In this paper, we propose a novel technique for
                   unlearning a subset of the training data from a LLM, without
                   having to retrain it from scratch. We evaluate our technique
                   on the task of unlearning the Harry Potter books from the
                   Llama2-7b model (a generative language model recently
                   open-sourced by Meta). While the model took over 184K
                   GPU-hours to pretrain, we show that in about 1 GPU hour of
                   finetuning, we effectively erase the model's ability to
                   generate or recall Harry Potter-related content, while its
                   performance on common benchmarks (such as Winogrande,
                   Hellaswag, arc, boolq and piqa) remains almost unaffected.
                   We make our fine-tuned model publicly available on
                   HuggingFace for community evaluation. To the best of our
                   knowledge, this is the first paper to present an effective
                   technique for unlearning in generative language models. Our
                   technique consists of three main components: First, we use a
                   reinforced model that is further trained on the target data
                   to identify the tokens that are most related to the
                   unlearning target, by comparing its logits with those of a
                   baseline model. Second, we replace idiosyncratic expressions
                   in the target data with generic counterparts, and leverage
                   the model's own predictions to generate alternative labels
                   for every token. These labels aim to approximate the
                   next-token predictions of a model that has not been trained
                   on the target data. Third, we finetune the model on these
                   alternative labels, which effectively erases the original
                   text from the model's memory whenever it is prompted with
                   its context.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.02238"
}

@ARTICLE{Ha2018-db,
  title         = "World Models",
  author        = "Ha, David and Schmidhuber, J{\"u}rgen",
  abstract      = "We explore building generative neural network models of
                   popular reinforcement learning environments. Our world model
                   can be trained quickly in an unsupervised manner to learn a
                   compressed spatial and temporal representation of the
                   environment. By using features extracted from the world
                   model as inputs to an agent, we can train a very compact and
                   simple policy that can solve the required task. We can even
                   train our agent entirely inside of its own hallucinated
                   dream generated by its world model, and transfer this policy
                   back into the actual environment. An interactive version of
                   this paper is available at https://worldmodels.github.io/",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1803.10122"
}

@ARTICLE{Mejia-Escobar2021-mf,
  title         = "A Large Visual, Qualitative and Quantitative Dataset of Web
                   Pages",
  author        = "Mejia-Escobar, Christian and Cazorla, Miguel and
                   Martinez-Martin, Ester",
  abstract      = "The World Wide Web is not only one of the most important
                   platforms of communication and information at present, but
                   also an area of growing interest for scientific research.
                   This motivates a lot of work and projects that require large
                   amounts of data. However, there is no dataset that
                   integrates the parameters and visual appearance of Web
                   pages, because its collection is a costly task in terms of
                   time and effort. With the support of various computer tools
                   and programming scripts, we have created a large dataset of
                   49,438 Web pages. It consists of visual, textual and
                   numerical data types, includes all countries worldwide, and
                   considers a broad range of topics such as art,
                   entertainment, economy, business, education, government,
                   news, media, science, and environment, covering different
                   cultural characteristics and varied design preferences. In
                   this paper, we describe the process of collecting, debugging
                   and publishing the final product, which is freely available.
                   To demonstrate the usefulness of our dataset, we expose a
                   binary classification model for detecting error Web pages,
                   and a multi-class Web subject-based categorization, both
                   problems using convolutional neural networks.",
  month         =  may,
  year          =  2021,
  keywords      = "Web Browsing",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2105.07113"
}

@ARTICLE{Kim2023-ms,
  title         = "Prometheus: Inducing Fine-grained Evaluation Capability in
                   Language Models",
  author        = "Kim, Seungone and Shin, Jamin and Cho, Yejin and Jang, Joel
                   and Longpre, Shayne and Lee, Hwaran and Yun, Sangdoo and
                   Shin, Seongjin and Kim, Sungdong and Thorne, James and Seo,
                   Minjoon",
  abstract      = "Recently, using a powerful proprietary Large Language Model
                   (LLM) (e.g., GPT-4) as an evaluator for long-form responses
                   has become the de facto standard. However, for practitioners
                   with large-scale evaluation tasks and custom criteria in
                   consideration (e.g., child-readability), using proprietary
                   LLMs as an evaluator is unreliable due to the closed-source
                   nature, uncontrolled versioning, and prohibitive costs. In
                   this work, we propose Prometheus, a fully open-source LLM
                   that is on par with GPT-4's evaluation capabilities when the
                   appropriate reference materials (reference answer, score
                   rubric) are accompanied. We first construct the Feedback
                   Collection, a new dataset that consists of 1K fine-grained
                   score rubrics, 20K instructions, and 100K responses and
                   language feedback generated by GPT-4. Using the Feedback
                   Collection, we train Prometheus, a 13B evaluator LLM that
                   can assess any given long-form text based on customized
                   score rubric provided by the user. Experimental results show
                   that Prometheus scores a Pearson correlation of 0.897 with
                   human evaluators when evaluating with 45 customized score
                   rubrics, which is on par with GPT-4 (0.882), and greatly
                   outperforms ChatGPT (0.392). Furthermore, measuring
                   correlation with GPT-4 with 1222 customized score rubrics
                   across four benchmarks (MT Bench, Vicuna Bench, Feedback
                   Bench, Flask Eval) shows similar trends, bolstering
                   Prometheus's capability as an evaluator LLM. Lastly,
                   Prometheus achieves the highest accuracy on two human
                   preference benchmarks (HHH Alignment \& MT Bench Human
                   Judgment) compared to open-sourced reward models explicitly
                   trained on human preference datasets, highlighting its
                   potential as an universal reward model. We open-source our
                   code, dataset, and model at
                   https://github.com/kaistAI/Prometheus.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.08491"
}

@INPROCEEDINGS{He2020-kp,
  title     = "Fakd: {Feature-Affinity} Based Knowledge Distillation for
               Efficient Image {Super-Resolution}",
  booktitle = "2020 {IEEE} International Conference on Image Processing
               ({ICIP})",
  author    = "He, Zibin and Dai, Tao and Lu, Jian and Jiang, Yong and Xia,
               Shu-Tao",
  abstract  = "Convolutional neural networks (CNNs) have been widely used in
               image super-resolution (SR). Most existing CNN-based methods
               focus on achieving better performance by designing deeper/wider
               networks, while suffering from heavy computational cost problem,
               thus hindering the deployment of such models in mobile devices
               with limited resources. To relieve such problem, we propose a
               novel and efficient SR model, named Feature Affinity-based
               Knowledge Distillation (FAKD), by transferring the structural
               knowledge of a heavy teacher model to a lightweight student
               model. To transfer the structural knowledge effectively, FAKD
               aims to distill the second-order statistical information from
               feature maps and trains a lightweight student network with low
               computational and memory cost. Experimental results demonstrate
               the efficacy of our method and the effectiveness over other
               knowledge distillation based methods in terms of both
               quantitative and visual metrics.",
  pages     = "518--522",
  month     =  oct,
  year      =  2020,
  keywords  = "Knowledge engineering;Computational modeling;Correlation;Image
               resolution;Task analysis;Feature extraction;Mathematical
               model;Image super-resolution;Knowledge distillation;Model
               compression;Convolutional neural networks"
}

@INPROCEEDINGS{Zhang2021-de,
  title     = "{Data-Free} Knowledge Distillation For Image {Super-Resolution}",
  booktitle = "2021 {IEEE/CVF} Conference on Computer Vision and Pattern
               Recognition ({CVPR})",
  author    = "Zhang, Yiman and Chen, Hanting and Chen, Xinghao and Deng,
               Yiping and Xu, Chunjing and Wang, Yunhe",
  abstract  = "Convolutional network compression methods require training data
               for achieving acceptable results, but training data is routinely
               unavailable due to some privacy and transmission limitations.
               Therefore, recent works focus on learning efficient networks
               without original training data, i.e., data-free model
               compression. Wherein, most of existing algorithms are developed
               for image recognition or segmentation tasks. In this paper, we
               study the data-free compression approach for single image
               super-resolution (SISR) task which is widely used in mobile
               phones and smart cameras. Specifically, we analyze the
               relationship between the outputs and inputs from the pre-trained
               network and explore a generator with a series of loss functions
               for maximally capturing useful information. The generator is
               then trained for synthesizing training samples which have
               similar distribution to that of the original data. To further
               alleviate the training difficulty of the student network using
               only the synthetic data, we introduce a progressive distillation
               scheme. Experiments on various datasets and architectures
               demonstrate that the pro-posed method is able to be utilized for
               effectively learning portable student networks without the
               original data, e.g., with 0.16dB PSNR drop on Set5 for $\times$2
               super resolution. Code will be available at
               https://github.com/huawei-noah/Data-Efficient-Model-Compression.",
  pages     = "7848--7857",
  month     =  jun,
  year      =  2021,
  keywords  = "Training;Knowledge engineering;Image
               coding;Superresolution;Training data;Smart cameras;Generators"
}

@ARTICLE{Karpukhin2020-aa,
  title         = "Dense Passage Retrieval for {Open-Domain} Question Answering",
  author        = "Karpukhin, Vladimir and O{\u g}uz, Barlas and Min, Sewon and
                   Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen,
                   Danqi and Yih, Wen-Tau",
  abstract      = "Open-domain question answering relies on efficient passage
                   retrieval to select candidate contexts, where traditional
                   sparse vector space models, such as TF-IDF or BM25, are the
                   de facto method. In this work, we show that retrieval can be
                   practically implemented using dense representations alone,
                   where embeddings are learned from a small number of
                   questions and passages by a simple dual-encoder framework.
                   When evaluated on a wide range of open-domain QA datasets,
                   our dense retriever outperforms a strong Lucene-BM25 system
                   largely by 9\%-19\% absolute in terms of top-20 passage
                   retrieval accuracy, and helps our end-to-end QA system
                   establish new state-of-the-art on multiple open-domain QA
                   benchmarks.",
  month         =  apr,
  year          =  2020,
  keywords      = "Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2004.04906"
}

@ARTICLE{Liang2023-rd,
  title         = "{TaskMatrix.AI}: Completing Tasks by Connecting Foundation
                   Models with Millions of {APIs}",
  author        = "Liang, Yaobo and Wu, Chenfei and Song, Ting and Wu, Wenshan
                   and Xia, Yan and Liu, Yu and Ou, Yang and Lu, Shuai and Ji,
                   Lei and Mao, Shaoguang and Wang, Yun and Shou, Linjun and
                   Gong, Ming and Duan, Nan",
  abstract      = "Artificial Intelligence (AI) has made incredible progress
                   recently. On the one hand, advanced foundation models like
                   ChatGPT can offer powerful conversation, in-context learning
                   and code generation abilities on a broad range of
                   open-domain tasks. They can also generate high-level
                   solution outlines for domain-specific tasks based on the
                   common sense knowledge they have acquired. However, they
                   still face difficulties with some specialized tasks because
                   they lack enough domain-specific data during pre-training or
                   they often have errors in their neural network computations
                   on those tasks that need accurate executions. On the other
                   hand, there are also many existing models and systems
                   (symbolic-based or neural-based) that can do some
                   domain-specific tasks very well. However, due to the
                   different implementation or working mechanisms, they are not
                   easily accessible or compatible with foundation models.
                   Therefore, there is a clear and pressing need for a
                   mechanism that can leverage foundation models to propose
                   task solution outlines and then automatically match some of
                   the sub-tasks in the outlines to the off-the-shelf models
                   and systems with special functionalities to complete them.
                   Inspired by this, we introduce TaskMatrix.AI as a new AI
                   ecosystem that connects foundation models with millions of
                   APIs for task completion. Unlike most previous work that
                   aimed to improve a single AI model, TaskMatrix.AI focuses
                   more on using existing foundation models (as a brain-like
                   central system) and APIs of other AI models and systems (as
                   sub-task solvers) to achieve diversified tasks in both
                   digital and physical domains. As a position paper, we will
                   present our vision of how to build such an ecosystem,
                   explain each key component, and use study cases to
                   illustrate both the feasibility of this vision and the main
                   challenges we need to address next.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2303.16434"
}

@INPROCEEDINGS{Kim2021-vm,
  title     = "What Changes Can Large-scale Language Models Bring? Intensive
               Study on {{H}yper{CLOVA}}: Billions-scale {K}orean Generative
               Pretrained Transformers",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in
               Natural Language Processing",
  author    = "Kim, Boseop and Kim, Hyoungseok and Lee, Sang-Woo and Lee,
               Gichang and Kwak, Donghyun and Dong Hyeon, Jeon and Park,
               Sunghyun and Kim, Sungju and Kim, Seonhoon and Seo, Dongpil and
               Lee, Heungsub and Jeong, Minyoung and Lee, Sungjae and Kim,
               Minsub and Ko, Suk Hyun and Kim, Seokhun and Park, Taeyong and
               Kim, Jinuk and Kang, Soyoung and Ryu, Na-Hyeon and Yoo, Kang Min
               and Chang, Minsuk and Suh, Soobin and In, Sookyo and Park,
               Jinseong and Kim, Kyungduk and Kim, Hiun and Jeong, Jisu and
               Yeo, Yong Goo and Ham, Donghoon and Park, Dongju and Lee, Min
               Young and Kang, Jaewook and Kang, Inho and Ha, Jung-Woo and
               Park, Woomyoung and Sung, Nako",
  abstract  = "GPT-3 shows remarkable in-context learning ability of
               large-scale language models (LMs) trained on hundreds of billion
               scale data. Here we address some remaining issues less reported
               by the GPT-3 paper, such as a non-English LM, the performances
               of different sized models, and the effect of recently introduced
               prompt optimization on in-context learning. To achieve this, we
               introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a
               Korean-centric corpus of 560B tokens. Enhanced by our
               Korean-specific tokenization, HyperCLOVA with our training
               configuration shows state-of-the-art in-context zero-shot and
               few-shot learning performances on various downstream tasks in
               Korean. Also, we show the performance benefits of prompt-based
               learning and demonstrate how it can be integrated into the
               prompt engineering pipeline. Then we discuss the possibility of
               materializing the No Code AI paradigm by providing AI
               prototyping capabilities to non-experts of ML by introducing
               HyperCLOVA studio, an interactive prompt engineering interface.
               Lastly, we demonstrate the potential of our methods with three
               successful in-house applications.",
  publisher = "Association for Computational Linguistics",
  pages     = "3405--3424",
  month     =  nov,
  year      =  2021,
  address   = "Online and Punta Cana, Dominican Republic"
}

@INPROCEEDINGS{Maxim2021-ku,
  title     = "A survey on the current state of the art on deep learning {3D}
               reconstruction",
  booktitle = "2021 {IEEE} 17th International Conference on Intelligent
               Computer Communication and Processing ({ICCP})",
  author    = "Maxim, Bogdan and Nedevschi, Sergiu",
  abstract  = "Efficient 3D reconstruction from different kind of inputs is a
               long standing effort of computer vision. Recent advancements in
               the field of machine learning, specifically deep learning, have
               started an interest in studying how well these techniques apply
               to the 3D reconstruction problem. Current efforts employ two
               main research directions: techniques applied to a single object,
               trying to reconstruct the surface as closely as possible from
               different kind of inputs and techniques applied to scenes made
               from multiple objects, which deal with topological
               representations, color, illumination and resource consumption.
               With a plethora of applications in computer graphics and
               computer vision, the results given by the deep learning
               techniques start to gain a serious position in the field of 3D
               reconstruction, yet no survey exist on the recent advancements
               on these new techniques. This survey summarizes the recent trend
               and applications of the deep-learning 3D reconstruction methods.
               We focus on learnable reconstruction methods from inputs like
               single image, multi-image and point cloud, using different
               representations, such as voxels, meshes and signed distance
               fields. Starting by presenting the datasets used, we follow by
               showing the main deep learning methods using different
               representations, presenting advantages and disadvantages. The
               second half of this survey focuses on scene reconstructions and
               open problems. Finally we conclude with a discussion of the
               importance of the 3D reconstruction and its possible
               applications in different fields such as automotive and mixed
               reality.",
  pages     = "283--290",
  month     =  oct,
  year      =  2021,
  keywords  = "Deep learning;Point cloud compression;Computer vision;Surface
               reconstruction;Three-dimensional displays;Scalability;Mixed
               reality;3D
               reconstruction;voxels;single-image;multi-view;point-cloud;deep-learning;machine-learning;state-of-the-
               art;scene reconstruction;neural representation;Survey;CS570 Team
               Project"
}

@ARTICLE{He2015-pt,
  title         = "Deep Residual Learning for Image Recognition",
  author        = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
                   Jian",
  abstract      = "Deeper neural networks are more difficult to train. We
                   present a residual learning framework to ease the training
                   of networks that are substantially deeper than those used
                   previously. We explicitly reformulate the layers as learning
                   residual functions with reference to the layer inputs,
                   instead of learning unreferenced functions. We provide
                   comprehensive empirical evidence showing that these residual
                   networks are easier to optimize, and can gain accuracy from
                   considerably increased depth. On the ImageNet dataset we
                   evaluate residual nets with a depth of up to 152 layers---8x
                   deeper than VGG nets but still having lower complexity. An
                   ensemble of these residual nets achieves 3.57\% error on the
                   ImageNet test set. This result won the 1st place on the
                   ILSVRC 2015 classification task. We also present analysis on
                   CIFAR-10 with 100 and 1000 layers. The depth of
                   representations is of central importance for many visual
                   recognition tasks. Solely due to our extremely deep
                   representations, we obtain a 28\% relative improvement on
                   the COCO object detection dataset. Deep residual nets are
                   foundations of our submissions to ILSVRC \& COCO 2015
                   competitions, where we also won the 1st places on the tasks
                   of ImageNet detection, ImageNet localization, COCO
                   detection, and COCO segmentation.",
  month         =  dec,
  year          =  2015,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1512.03385"
}

@INPROCEEDINGS{Deng2009-da,
  title     = "{ImageNet}: A large-scale hierarchical image database",
  booktitle = "2009 {IEEE} Conference on Computer Vision and Pattern
               Recognition",
  author    = "Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and
               Li, Kai and Fei-Fei, Li",
  abstract  = "The explosion of image data on the Internet has the potential to
               foster more sophisticated and robust models and algorithms to
               index, retrieve, organize and interact with images and
               multimedia data. But exactly how such data can be harnessed and
               organized remains a critical problem. We introduce here a new
               database called ``ImageNet'', a large-scale ontology of images
               built upon the backbone of the WordNet structure. ImageNet aims
               to populate the majority of the 80,000 synsets of WordNet with
               an average of 500--1000 clean and full resolution images. This
               will result in tens of millions of annotated images organized by
               the semantic hierarchy of WordNet. This paper offers a detailed
               analysis of ImageNet in its current state: 12 subtrees with 5247
               synsets and 3.2 million images in total. We show that ImageNet
               is much larger in scale and diversity and much more accurate
               than the current image datasets. Constructing such a large-scale
               database is a challenging task. We describe the data collection
               scheme with Amazon Mechanical Turk. Lastly, we illustrate the
               usefulness of ImageNet through three simple applications in
               object recognition, image classification and automatic object
               clustering. We hope that the scale, accuracy, diversity and
               hierarchical structure of ImageNet can offer unparalleled
               opportunities to researchers in the computer vision community
               and beyond.",
  pages     = "248--255",
  month     =  jun,
  year      =  2009,
  keywords  = "Large-scale systems;Image
               databases;Explosions;Internet;Robustness;Information
               retrieval;Image retrieval;Multimedia
               databases;Ontologies;Spine;CS570 DL \& CV"
}

@ARTICLE{Lin2014-jx,
  title         = "Microsoft {COCO}: Common Objects in Context",
  author        = "Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and
                   Bourdev, Lubomir and Girshick, Ross and Hays, James and
                   Perona, Pietro and Ramanan, Deva and Lawrence Zitnick, C and
                   Doll{\'a}r, Piotr",
  abstract      = "We present a new dataset with the goal of advancing the
                   state-of-the-art in object recognition by placing the
                   question of object recognition in the context of the broader
                   question of scene understanding. This is achieved by
                   gathering images of complex everyday scenes containing
                   common objects in their natural context. Objects are labeled
                   using per-instance segmentations to aid in precise object
                   localization. Our dataset contains photos of 91 objects
                   types that would be easily recognizable by a 4 year old.
                   With a total of 2.5 million labeled instances in 328k
                   images, the creation of our dataset drew upon extensive
                   crowd worker involvement via novel user interfaces for
                   category detection, instance spotting and instance
                   segmentation. We present a detailed statistical analysis of
                   the dataset in comparison to PASCAL, ImageNet, and SUN.
                   Finally, we provide baseline performance analysis for
                   bounding box and segmentation detection results using a
                   Deformable Parts Model.",
  month         =  may,
  year          =  2014,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1405.0312"
}

@ARTICLE{Noh2015-sb,
  title         = "Learning Deconvolution Network for Semantic Segmentation",
  author        = "Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung",
  abstract      = "We propose a novel semantic segmentation algorithm by
                   learning a deconvolution network. We learn the network on
                   top of the convolutional layers adopted from VGG 16-layer
                   net. The deconvolution network is composed of deconvolution
                   and unpooling layers, which identify pixel-wise class labels
                   and predict segmentation masks. We apply the trained network
                   to each proposal in an input image, and construct the final
                   semantic segmentation map by combining the results from all
                   proposals in a simple manner. The proposed algorithm
                   mitigates the limitations of the existing methods based on
                   fully convolutional networks by integrating deep
                   deconvolution network and proposal-wise prediction; our
                   segmentation method typically identifies detailed structures
                   and handles objects in multiple scales naturally. Our
                   network demonstrates outstanding performance in PASCAL VOC
                   2012 dataset, and we achieve the best accuracy (72.5\%)
                   among the methods trained with no external data through
                   ensemble with the fully convolutional network.",
  month         =  may,
  year          =  2015,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1505.04366"
}

@ARTICLE{Wang2018-cy,
  title         = "{Pixel2Mesh}: Generating {3D} Mesh Models from Single {RGB}
                   Images",
  author        = "Wang, Nanyang and Zhang, Yinda and Li, Zhuwen and Fu, Yanwei
                   and Liu, Wei and Jiang, Yu-Gang",
  abstract      = "We propose an end-to-end deep learning architecture that
                   produces a 3D shape in triangular mesh from a single color
                   image. Limited by the nature of deep neural network,
                   previous methods usually represent a 3D shape in volume or
                   point cloud, and it is non-trivial to convert them to the
                   more ready-to-use mesh model. Unlike the existing methods,
                   our network represents 3D mesh in a graph-based
                   convolutional neural network and produces correct geometry
                   by progressively deforming an ellipsoid, leveraging
                   perceptual features extracted from the input image. We adopt
                   a coarse-to-fine strategy to make the whole deformation
                   procedure stable, and define various of mesh related losses
                   to capture properties of different levels to guarantee
                   visually appealing and physically accurate 3D geometry.
                   Extensive experiments show that our method not only
                   qualitatively produces mesh model with better details, but
                   also achieves higher 3D shape estimation accuracy compared
                   to the state-of-the-art.",
  month         =  apr,
  year          =  2018,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1804.01654"
}

@ARTICLE{Kato2017-fm,
  title         = "Neural {3D} Mesh Renderer",
  author        = "Kato, Hiroharu and Ushiku, Yoshitaka and Harada, Tatsuya",
  abstract      = "For modeling the 3D world behind 2D images, which 3D
                   representation is most appropriate? A polygon mesh is a
                   promising candidate for its compactness and geometric
                   properties. However, it is not straightforward to model a
                   polygon mesh from 2D images using neural networks because
                   the conversion from a mesh to an image, or rendering,
                   involves a discrete operation called rasterization, which
                   prevents back-propagation. Therefore, in this work, we
                   propose an approximate gradient for rasterization that
                   enables the integration of rendering into neural networks.
                   Using this renderer, we perform single-image 3D mesh
                   reconstruction with silhouette image supervision and our
                   system outperforms the existing voxel-based approach.
                   Additionally, we perform gradient-based 3D mesh editing
                   operations, such as 2D-to-3D style transfer and 3D
                   DeepDream, with 2D supervision for the first time. These
                   applications demonstrate the potential of the integration of
                   a mesh renderer into neural networks and the effectiveness
                   of our proposed renderer.",
  month         =  nov,
  year          =  2017,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1711.07566"
}

@ARTICLE{Mildenhall2020-qp,
  title         = "{NeRF}: Representing Scenes as Neural Radiance Fields for
                   View Synthesis",
  author        = "Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew
                   and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren",
  abstract      = "We present a method that achieves state-of-the-art results
                   for synthesizing novel views of complex scenes by optimizing
                   an underlying continuous volumetric scene function using a
                   sparse set of input views. Our algorithm represents a scene
                   using a fully-connected (non-convolutional) deep network,
                   whose input is a single continuous 5D coordinate (spatial
                   location $(x,y,z)$ and viewing direction $(\theta, \phi)$)
                   and whose output is the volume density and view-dependent
                   emitted radiance at that spatial location. We synthesize
                   views by querying 5D coordinates along camera rays and use
                   classic volume rendering techniques to project the output
                   colors and densities into an image. Because volume rendering
                   is naturally differentiable, the only input required to
                   optimize our representation is a set of images with known
                   camera poses. We describe how to effectively optimize neural
                   radiance fields to render photorealistic novel views of
                   scenes with complicated geometry and appearance, and
                   demonstrate results that outperform prior work on neural
                   rendering and view synthesis. View synthesis results are
                   best viewed as videos, so we urge readers to view our
                   supplementary video for convincing comparisons.",
  month         =  mar,
  year          =  2020,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2003.08934"
}

@ARTICLE{Isola2016-hf,
  title         = "{Image-to-Image} Translation with Conditional Adversarial
                   Networks",
  author        = "Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros,
                   Alexei A",
  abstract      = "We investigate conditional adversarial networks as a
                   general-purpose solution to image-to-image translation
                   problems. These networks not only learn the mapping from
                   input image to output image, but also learn a loss function
                   to train this mapping. This makes it possible to apply the
                   same generic approach to problems that traditionally would
                   require very different loss formulations. We demonstrate
                   that this approach is effective at synthesizing photos from
                   label maps, reconstructing objects from edge maps, and
                   colorizing images, among other tasks. Indeed, since the
                   release of the pix2pix software associated with this paper,
                   a large number of internet users (many of them artists) have
                   posted their own experiments with our system, further
                   demonstrating its wide applicability and ease of adoption
                   without the need for parameter tweaking. As a community, we
                   no longer hand-engineer our mapping functions, and this work
                   suggests we can achieve reasonable results without
                   hand-engineering our loss functions either.",
  month         =  nov,
  year          =  2016,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1611.07004"
}

@ARTICLE{Karras2018-uw,
  title         = "A {Style-Based} Generator Architecture for Generative
                   Adversarial Networks",
  author        = "Karras, Tero and Laine, Samuli and Aila, Timo",
  abstract      = "We propose an alternative generator architecture for
                   generative adversarial networks, borrowing from style
                   transfer literature. The new architecture leads to an
                   automatically learned, unsupervised separation of high-level
                   attributes (e.g., pose and identity when trained on human
                   faces) and stochastic variation in the generated images
                   (e.g., freckles, hair), and it enables intuitive,
                   scale-specific control of the synthesis. The new generator
                   improves the state-of-the-art in terms of traditional
                   distribution quality metrics, leads to demonstrably better
                   interpolation properties, and also better disentangles the
                   latent factors of variation. To quantify interpolation
                   quality and disentanglement, we propose two new, automated
                   methods that are applicable to any generator architecture.
                   Finally, we introduce a new, highly varied and high-quality
                   dataset of human faces.",
  month         =  dec,
  year          =  2018,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1812.04948"
}

@ARTICLE{Ho2020-tq,
  title         = "Denoising Diffusion Probabilistic Models",
  author        = "Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",
  abstract      = "We present high quality image synthesis results using
                   diffusion probabilistic models, a class of latent variable
                   models inspired by considerations from nonequilibrium
                   thermodynamics. Our best results are obtained by training on
                   a weighted variational bound designed according to a novel
                   connection between diffusion probabilistic models and
                   denoising score matching with Langevin dynamics, and our
                   models naturally admit a progressive lossy decompression
                   scheme that can be interpreted as a generalization of
                   autoregressive decoding. On the unconditional CIFAR10
                   dataset, we obtain an Inception score of 9.46 and a
                   state-of-the-art FID score of 3.17. On 256x256 LSUN, we
                   obtain sample quality similar to ProgressiveGAN. Our
                   implementation is available at
                   https://github.com/hojonathanho/diffusion",
  month         =  jun,
  year          =  2020,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2006.11239"
}

@ARTICLE{Noever2023-pj,
  title         = "Chatbots as Problem Solvers: Playing Twenty Questions with
                   Role Reversals",
  author        = "Noever, David and McKee, Forrest",
  abstract      = "New chat AI applications like ChatGPT offer an advanced
                   understanding of question context and memory across
                   multi-step tasks, such that experiments can test its
                   deductive reasoning. This paper proposes a multi-role and
                   multi-step challenge, where ChatGPT plays the classic
                   twenty-questions game but innovatively switches roles from
                   the questioner to the answerer. The main empirical result
                   establishes that this generation of chat applications can
                   guess random object names in fewer than twenty questions
                   (average, 12) and correctly guess 94\% of the time across
                   sixteen different experimental setups. The research
                   introduces four novel cases where the chatbot fields the
                   questions, asks the questions, both question-answer roles,
                   and finally tries to guess appropriate contextual emotions.
                   One task that humans typically fail but trained chat
                   applications complete involves playing bilingual games of
                   twenty questions (English answers to Spanish questions).
                   Future variations address direct problem-solving using a
                   similar inquisitive format to arrive at novel outcomes
                   deductively, such as patentable inventions or combination
                   thinking. Featured applications of this dialogue format
                   include complex protein designs, neuroscience metadata, and
                   child development educational materials.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2301.01743"
}

@ARTICLE{Humphreys2022-as,
  title         = "A data-driven approach for learning to control computers",
  author        = "Humphreys, Peter C and Raposo, David and Pohlen, Toby and
                   Thornton, Gregory and Chhaparia, Rachita and Muldal,
                   Alistair and Abramson, Josh and Georgiev, Petko and Goldin,
                   Alex and Santoro, Adam and Lillicrap, Timothy",
  abstract      = "It would be useful for machines to use computers as humans
                   do so that they can aid us in everyday tasks. This is a
                   setting in which there is also the potential to leverage
                   large-scale expert demonstrations and human judgements of
                   interactive behaviour, which are two ingredients that have
                   driven much recent success in AI. Here we investigate the
                   setting of computer control using keyboard and mouse, with
                   goals specified via natural language. Instead of focusing on
                   hand-designed curricula and specialized action spaces, we
                   focus on developing a scalable method centered on
                   reinforcement learning combined with behavioural priors
                   informed by actual human-computer interactions. We achieve
                   state-of-the-art and human-level mean performance across all
                   tasks within the MiniWob++ benchmark, a challenging suite of
                   computer control problems, and find strong evidence of
                   cross-task transfer. These results demonstrate the
                   usefulness of a unified human-agent interface when training
                   machines to use computers. Altogether our results suggest a
                   formula for achieving competency beyond MiniWob++ and
                   towards controlling computers, in general, as a human would.",
  month         =  feb,
  year          =  2022,
  keywords      = "Web Browsing",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2202.08137"
}

@ARTICLE{Lewis2020-ex,
  title         = "{Retrieval-Augmented} Generation for {Knowledge-Intensive}
                   {NLP} Tasks",
  author        = "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and
                   Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and
                   K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-Tau and
                   Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe",
  abstract      = "Large pre-trained language models have been shown to store
                   factual knowledge in their parameters, and achieve
                   state-of-the-art results when fine-tuned on downstream NLP
                   tasks. However, their ability to access and precisely
                   manipulate knowledge is still limited, and hence on
                   knowledge-intensive tasks, their performance lags behind
                   task-specific architectures. Additionally, providing
                   provenance for their decisions and updating their world
                   knowledge remain open research problems. Pre-trained models
                   with a differentiable access mechanism to explicit
                   non-parametric memory can overcome this issue, but have so
                   far been only investigated for extractive downstream tasks.
                   We explore a general-purpose fine-tuning recipe for
                   retrieval-augmented generation (RAG) -- models which combine
                   pre-trained parametric and non-parametric memory for
                   language generation. We introduce RAG models where the
                   parametric memory is a pre-trained seq2seq model and the
                   non-parametric memory is a dense vector index of Wikipedia,
                   accessed with a pre-trained neural retriever. We compare two
                   RAG formulations, one which conditions on the same retrieved
                   passages across the whole generated sequence, the other can
                   use different passages per token. We fine-tune and evaluate
                   our models on a wide range of knowledge-intensive NLP tasks
                   and set the state-of-the-art on three open domain QA tasks,
                   outperforming parametric seq2seq models and task-specific
                   retrieve-and-extract architectures. For language generation
                   tasks, we find that RAG models generate more specific,
                   diverse and factual language than a state-of-the-art
                   parametric-only seq2seq baseline.",
  month         =  may,
  year          =  2020,
  keywords      = "Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2005.11401"
}

@INPROCEEDINGS{Sitaram2023-jz,
  title     = "Everything you need to know about Multilingual {{LLM}s}: Towards
               fair, performant and reliable models for languages of the world",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for
               Computational Linguistics (Volume 6: Tutorial Abstracts)",
  author    = "Sitaram, Sunayana and Choudhury, Monojit and Patra, Barun and
               Chaudhary, Vishrav and Ahuja, Kabir and Bali, Kalika",
  abstract  = "This tutorial will describe various aspects of scaling up
               language technologies to many of the world's languages by
               describing the latest research in Massively Multilingual
               Language Models (MMLMs). We will cover topics such as data
               collection, training and fine-tuning of models, Responsible AI
               issues such as fairness, bias and toxicity, linguistic diversity
               and evaluation in the context of MMLMs, specifically focusing on
               issues in non-English and low-resource languages. Further, we
               will also talk about some of the real-world challenges in
               deploying these models in language communities in the field.
               With the performance of MMLMs improving in the zero-shot setting
               for many languages, it is now becoming feasible to use them for
               building language technologies in many languages of the world,
               and this tutorial will provide the computational linguistics
               community with unique insights from the latest research in
               multilingual models.",
  publisher = "Association for Computational Linguistics",
  pages     = "21--26",
  month     =  jul,
  year      =  2023,
  address   = "Toronto, Canada",
  keywords  = "Multilingual"
}

@ARTICLE{Blevins2022-xy,
  title         = "Language Contamination Helps Explain the Cross-lingual
                   Capabilities of English Pretrained Models",
  author        = "Blevins, Terra and Zettlemoyer, Luke",
  abstract      = "English pretrained language models, which make up the
                   backbone of many modern NLP systems, require huge amounts of
                   unlabeled training data. These models are generally
                   presented as being trained only on English text but have
                   been found to transfer surprisingly well to other languages.
                   We investigate this phenomenon and find that common English
                   pretraining corpora actually contain significant amounts of
                   non-English text: even when less than 1\% of data is not
                   English (well within the error rate of strong language
                   classifiers), this leads to hundreds of millions of foreign
                   language tokens in large-scale datasets. We then demonstrate
                   that even these small percentages of non-English data
                   facilitate cross-lingual transfer for models trained on
                   them, with target language performance strongly correlated
                   to the amount of in-language data seen during pretraining.
                   In light of these findings, we argue that no model is truly
                   monolingual when pretrained at scale, which should be
                   considered when evaluating cross-lingual transfer.",
  month         =  apr,
  year          =  2022,
  keywords      = "Multilingual",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.08110"
}

@ARTICLE{Asai2023-nr,
  title         = "{BUFFET}: Benchmarking Large Language Models for Few-shot
                   Cross-lingual Transfer",
  author        = "Asai, Akari and Kudugunta, Sneha and Yu, Xinyan Velocity and
                   Blevins, Terra and Gonen, Hila and Reid, Machel and
                   Tsvetkov, Yulia and Ruder, Sebastian and Hajishirzi,
                   Hannaneh",
  abstract      = "Despite remarkable advancements in few-shot generalization
                   in natural language processing, most models are developed
                   and evaluated primarily in English. To facilitate research
                   on few-shot cross-lingual transfer, we introduce a new
                   benchmark, called BUFFET, which unifies 15 diverse tasks
                   across 54 languages in a sequence-to-sequence format and
                   provides a fixed set of few-shot examples and instructions.
                   BUFFET is designed to establish a rigorous and equitable
                   evaluation framework for few-shot cross-lingual transfer
                   across a broad range of tasks and languages. Using BUFFET,
                   we perform thorough evaluations of state-of-the-art
                   multilingual large language models with different transfer
                   methods, namely in-context learning and fine-tuning. Our
                   findings reveal significant room for improvement in few-shot
                   in-context cross-lingual transfer. In particular, ChatGPT
                   with in-context learning often performs worse than much
                   smaller mT5-base models fine-tuned on English task data and
                   few-shot in-language examples. Our analysis suggests various
                   avenues for future research in few-shot cross-lingual
                   transfer, such as improved pretraining, understanding, and
                   future evaluations.",
  month         =  may,
  year          =  2023,
  keywords      = "Multilingual",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.14857"
}

@ARTICLE{Li2023-ze,
  title         = "Why Does {Zero-Shot} {Cross-Lingual} Generation Fail? An
                   Explanation and a Solution",
  author        = "Li, Tianjian and Murray, Kenton",
  abstract      = "Zero-shot cross-lingual transfer is when a multilingual
                   model is trained to perform a task in one language and then
                   is applied to another language. Although the zero-shot
                   cross-lingual transfer approach has achieved success in
                   various classification tasks, its performance on natural
                   language generation tasks falls short in quality and
                   sometimes outputs an incorrect language. In our study, we
                   show that the fine-tuning process learns language invariant
                   representations, which is beneficial for classification
                   tasks but harmful for generation tasks. Motivated by this,
                   we propose a simple method to regularize the model from
                   learning language invariant representations and a method to
                   select model checkpoints without a development set in the
                   target language, both resulting in better generation
                   quality. Experiments on three semantically diverse
                   generation tasks show that our method reduces the accidental
                   translation problem by 68\% and improves the ROUGE-L score
                   by 1.5 on average.",
  month         =  may,
  year          =  2023,
  keywords      = "Multilingual",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.17325"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Bender2021-ps,
  title     = "On the Dangers of Stochastic Parrots: Can Language Models Be Too
               Big? 🦜",
  booktitle = "Proceedings of the 2021 {ACM} Conference on Fairness,
               Accountability, and Transparency",
  author    = "Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina
               and Shmitchell, Shmargaret",
  abstract  = "The past 3 years of work in NLP have been characterized by the
               development and deployment of ever larger language models,
               especially for English. BERT, its variants, GPT-2/3, and others,
               most recently Switch-C, have pushed the boundaries of the
               possible both through architectural innovations and through
               sheer size. Using these pretrained models and the methodology of
               fine-tuning them for specific tasks, researchers have extended
               the state of the art on a wide array of tasks as measured by
               leaderboards on specific benchmarks for English. In this paper,
               we take a step back and ask: How big is too big? What are the
               possible risks associated with this technology and what paths
               are available for mitigating those risks? We provide
               recommendations including weighing the environmental and
               financial costs first, investing resources into curating and
               carefully documenting datasets rather than ingesting everything
               on the web, carrying out pre-development exercises evaluating
               how the planned approach fits into research and development
               goals and supports stakeholder values, and encouraging research
               directions beyond ever larger language models.",
  publisher = "Association for Computing Machinery",
  pages     = "610--623",
  series    = "FAccT '21",
  month     =  mar,
  year      =  2021,
  address   = "New York, NY, USA",
  location  = "Virtual Event, Canada"
}

@ARTICLE{Wolf2023-yr,
  title         = "Fundamental Limitations of Alignment in Large Language
                   Models",
  author        = "Wolf, Yotam and Wies, Noam and Avnery, Oshri and Levine,
                   Yoav and Shashua, Amnon",
  abstract      = "An important aspect in developing language models that
                   interact with humans is aligning their behavior to be useful
                   and unharmful for their human users. This is usually
                   achieved by tuning the model in a way that enhances desired
                   behaviors and inhibits undesired ones, a process referred to
                   as alignment. In this paper, we propose a theoretical
                   approach called Behavior Expectation Bounds (BEB) which
                   allows us to formally investigate several inherent
                   characteristics and limitations of alignment in large
                   language models. Importantly, we prove that for any behavior
                   that has a finite probability of being exhibited by the
                   model, there exist prompts that can trigger the model into
                   outputting this behavior, with probability that increases
                   with the length of the prompt. This implies that any
                   alignment process that attenuates undesired behavior but
                   does not remove it altogether, is not safe against
                   adversarial prompting attacks. Furthermore, our framework
                   hints at the mechanism by which leading alignment approaches
                   such as reinforcement learning from human feedback increase
                   the LLM's proneness to being prompted into the undesired
                   behaviors. Moreover, we include the notion of personas in
                   our BEB framework, and find that behaviors which are
                   generally very unlikely to be exhibited by the model can be
                   brought to the front by prompting the model to behave as
                   specific persona. This theoretical result is being
                   experimentally demonstrated in large scale by the so called
                   contemporary ``chatGPT jailbreaks'', where adversarial users
                   trick the LLM into breaking its alignment guardrails by
                   triggering it into acting as a malicious persona. Our
                   results expose fundamental limitations in alignment of LLMs
                   and bring to the forefront the need to devise reliable
                   mechanisms for ensuring AI safety.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.11082"
}

@INPROCEEDINGS{De_Bruyn2022-wj,
  title     = "Is It Smaller Than a Tennis Ball? Language Models Play the Game
               of Twenty Questions",
  booktitle = "Proceedings of the Fifth {BlackboxNLP} Workshop on Analyzing and
               Interpreting Neural Networks for {NLP}",
  author    = "De Bruyn, Maxime and Lotfi, Ehsan and Buhmann, Jeska and
               Daelemans, Walter",
  abstract  = "Researchers often use games to analyze the abilities of
               Artificial Intelligence models. In this work, we use the game of
               Twenty Questions to study the world knowledge of language
               models. Despite its simplicity for humans, this game requires a
               broad knowledge of the world to answer yes/no questions. We
               evaluate several language models on this task and find that only
               the largest model has enough world knowledge to play it well,
               although it still has difficulties with the shape and size of
               objects. We also present a new method to improve the knowledge
               of smaller models by leveraging external information from the
               web. Finally, we release our dataset and Twentle, a website to
               interactively test the knowledge of language models by playing
               Twenty Questions.",
  publisher = "Association for Computational Linguistics",
  pages     = "80--90",
  month     =  dec,
  year      =  2022,
  address   = "Abu Dhabi, United Arab Emirates (Hybrid)"
}

@ARTICLE{Fu2022-mf,
  title     = "Learning towards conversational {AI}: A survey",
  author    = "Fu, Tingchen and Gao, Shen and Zhao, Xueliang and Wen, Ji-Rong
               and Yan, Rui",
  abstract  = "Recent years have witnessed a surge of interest in the field of
               open-domain dialogue. Thanks to the rapid development of social
               media, large dialogue corpus from the Internet builds up a
               fundamental premise for data-driven dialogue model. The
               breakthrough in neural network also brings new ideas to
               researchers in AI and NLP. A great number of new techniques and
               methods therefore came into being. In this paper, we review some
               of the most representative works in recent years and divide
               existing prevailing frameworks for a dialogue model into three
               categories. We further analyze the trend of development for
               open-domain dialogue and summarize the goal of an open-domain
               dialogue system in two aspects, informative and controllable.
               The methods we review in this paper are selected according to
               our unique perspectives and by no means complete. Rather, we
               hope this servery could benefit NLP community for future
               research in open-domain dialogue.",
  journal   = "AI Open",
  publisher = "Elsevier",
  volume    =  3,
  pages     = "14--28",
  month     =  jan,
  year      =  2022,
  keywords  = "Human-machine conversation; Response generation; Informativeness
               dialogue; Controllable dialogue;Survey"
}

@ARTICLE{Ni2023-qz,
  title    = "Recent advances in deep learning based dialogue systems: a
              systematic survey",
  author   = "Ni, Jinjie and Young, Tom and Pandelea, Vlad and Xue, Fuzhao and
              Cambria, Erik",
  abstract = "Dialogue systems are a popular natural language processing (NLP)
              task as it is promising in real-life applications. It is also a
              complicated task since many NLP tasks deserving study are
              involved. As a result, a multitude of novel works on this task
              are carried out, and most of them are deep learning based due to
              their outstanding performance. In this survey, we mainly focus on
              the deep learning based dialogue systems. We comprehensively
              review state-of-the-art research outcomes in dialogue systems and
              analyze them from two angles: model type and system type.
              Specifically, from the angle of model type, we discuss the
              principles, characteristics, and applications of different models
              that are widely used in dialogue systems. This will help
              researchers acquaint these models and see how they are applied in
              state-of-the-art frameworks, which is rather helpful when
              designing a new dialogue system. From the angle of system type,
              we discuss task-oriented and open-domain dialogue systems as two
              streams of research, providing insight into the hot topics
              related. Furthermore, we comprehensively review the evaluation
              methods and datasets for dialogue systems to pave the way for
              future research. Finally, some possible research trends are
              identified based on the recent research outcomes. To the best of
              our knowledge, this survey is the most comprehensive and
              up-to-date one at present for deep learning based dialogue
              systems, extensively covering the popular techniques. We
              speculate that this work is a good starting point for academics
              who are new to the dialogue systems or those who want to quickly
              grasp up-to-date techniques in this area.",
  journal  = "Artificial Intelligence Review",
  volume   =  56,
  number   =  4,
  pages    = "3055--3155",
  month    =  apr,
  year     =  2023,
  keywords = "Survey"
}

@ARTICLE{Furuta2023-fg,
  title         = "Multimodal Web Navigation with {Instruction-Finetuned}
                   Foundation Models",
  author        = "Furuta, Hiroki and Nachum, Ofir and Lee, Kuang-Huei and
                   Matsuo, Yutaka and Gu, Shixiang Shane and Gur, Izzeddin",
  abstract      = "The progress of autonomous web navigation has been hindered
                   by the dependence on billions of exploratory interactions
                   via online reinforcement learning, and domain-specific model
                   designs that make it difficult to leverage generalization
                   from rich out-of-domain data. In this work, we study
                   data-driven offline training for web agents with
                   vision-language foundation models. We propose an
                   instruction-following multimodal agent, WebGUM, that
                   observes both webpage screenshots and HTML pages and outputs
                   web navigation actions, such as click and type. WebGUM is
                   trained by jointly finetuning an instruction-finetuned
                   language model and a vision transformer on a large corpus of
                   demonstrations. We empirically demonstrate this recipe
                   improves the agent's ability of grounded visual perception,
                   HTML comprehension and multi-step reasoning, outperforming
                   prior works by a significant margin. On the MiniWoB
                   benchmark, we improve over the previous best offline methods
                   by more than 31.9\%, being close to reaching
                   online-finetuned SoTA. On the WebShop benchmark, our
                   3-billion-parameter model achieves superior performance to
                   the existing SoTA, PaLM-540B. We also collect 347K
                   high-quality demonstrations using our trained models, 38
                   times larger than prior work, and make them available to
                   promote future research in this direction.",
  month         =  may,
  year          =  2023,
  keywords      = "Agent",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2305.11854"
}

@ARTICLE{Yao2022-yp,
  title         = "{WebShop}: Towards Scalable {Real-World} Web Interaction
                   with Grounded Language Agents",
  author        = "Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan,
                   Karthik",
  abstract      = "Existing benchmarks for grounding language in interactive
                   environments either lack real-world linguistic elements, or
                   prove difficult to scale up due to substantial human
                   involvement in the collection of data or feedback signals.
                   To bridge this gap, we develop WebShop -- a simulated
                   e-commerce website environment with $1.18$ million
                   real-world products and $12,087$ crowd-sourced text
                   instructions. Given a text instruction specifying a product
                   requirement, an agent needs to navigate multiple types of
                   webpages and issue diverse actions to find, customize, and
                   purchase an item. WebShop provides several challenges for
                   language grounding including understanding compositional
                   instructions, query (re-)formulation, comprehending and
                   acting on noisy text in webpages, and performing strategic
                   exploration. We collect over $1,600$ human demonstrations
                   for the task, and train and evaluate a diverse range of
                   agents using reinforcement learning, imitation learning, and
                   pre-trained image and language models. Our best model
                   achieves a task success rate of $29\%$, which outperforms
                   rule-based heuristics ($9.6\%$) but is far lower than human
                   expert performance ($59\%$). We also analyze agent and human
                   trajectories and ablate various model components to provide
                   insights for developing future agents with stronger language
                   understanding and decision making abilities. Finally, we
                   show that agents trained on WebShop exhibit non-trivial
                   sim-to-real transfer when evaluated on amazon.com and
                   ebay.com, indicating the potential value of WebShop in
                   developing practical web-based agents that can operate in
                   the wild.",
  month         =  jul,
  year          =  2022,
  keywords      = "Agent",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2207.01206"
}

@ARTICLE{Jin2023-vq,
  title         = "{KoBBQ}: Korean Bias Benchmark for Question Answering",
  author        = "Jin, Jiho and Kim, Jiseon and Lee, Nayeon and Yoo, Haneul
                   and Oh, Alice and Lee, Hwaran",
  abstract      = "The BBQ (Bias Benchmark for Question Answering) dataset
                   enables the evaluation of the social biases that language
                   models (LMs) exhibit in downstream tasks. However, it is
                   challenging to adapt BBQ to languages other than English as
                   social biases are culturally dependent. In this paper, we
                   devise a process to construct a non-English bias benchmark
                   dataset by leveraging the English BBQ dataset in a
                   culturally adaptive way and present the KoBBQ dataset for
                   evaluating biases in Question Answering (QA) tasks in
                   Korean. We identify samples from BBQ into three classes:
                   Simply-Translated (can be used directly after cultural
                   translation), Target-Modified (requires localization in
                   target groups), and Sample-Removed (does not fit Korean
                   culture). We further enhance the cultural relevance to
                   Korean culture by adding four new categories of bias
                   specific to Korean culture and newly creating samples based
                   on Korean literature. KoBBQ consists of 246 templates and
                   4,740 samples across 12 categories of social bias. Using
                   KoBBQ, we measure the accuracy and bias scores of several
                   state-of-the-art multilingual LMs. We demonstrate the
                   differences in the bias of LMs in Korean and English,
                   clarifying the need for hand-crafted data considering
                   cultural differences.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.16778"
}

@ARTICLE{Ko2023-vs,
  title         = "A Technical Report for {Polyglot-Ko}: {Open-Source}
                   {Large-Scale} Korean Language Models",
  author        = "Ko, Hyunwoong and Yang, Kichang and Ryu, Minho and Choi,
                   Taekyoon and Yang, Seungmu and Hyun, Jiwung and Park, Sungho
                   and Park, Kyubyong",
  abstract      = "Polyglot is a pioneering project aimed at enhancing the
                   non-English language performance of multilingual language
                   models. Despite the availability of various multilingual
                   models such as mBERT (Devlin et al., 2019), XGLM (Lin et
                   al., 2022), and BLOOM (Scao et al., 2022), researchers and
                   developers often resort to building monolingual models in
                   their respective languages due to the dissatisfaction with
                   the current multilingual models non-English language
                   capabilities. Addressing this gap, we seek to develop
                   advanced multilingual language models that offer improved
                   performance in non-English languages. In this paper, we
                   introduce the Polyglot Korean models, which represent a
                   specific focus rather than being multilingual in nature. In
                   collaboration with TUNiB, our team collected 1.2TB of Korean
                   data meticulously curated for our research journey. We made
                   a deliberate decision to prioritize the development of
                   Korean models before venturing into multilingual models.
                   This choice was motivated by multiple factors: firstly, the
                   Korean models facilitated performance comparisons with
                   existing multilingual models; and finally, they catered to
                   the specific needs of Korean companies and researchers. This
                   paper presents our work in developing the Polyglot Korean
                   models, which propose some steps towards addressing the
                   non-English language performance gap in multilingual
                   language models.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.02254"
}

@ARTICLE{Srivastava2022-qo,
  title         = "Beyond the Imitation Game: Quantifying and extrapolating the
                   capabilities of language models",
  author        = "Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek
                   and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam
                   and Brown, Adam R and Santoro, Adam and Gupta, Aditya and
                   Garriga-Alonso, Adri{\`a} and Kluska, Agnieszka and
                   Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and
                   Ray, Alex and Warstadt, Alex and Kocurek, Alexander W and
                   Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish,
                   Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda
                   and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and
                   Iyer, Anantharaman S and Andreassen, Anders and Madotto,
                   Andrea and Santilli, Andrea and Stuhlm{\"u}ller, Andreas and
                   Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou,
                   Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and
                   Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and
                   Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa
                   and Menezes, Arul and Kirubarajan, Arun and Mullokandov,
                   Asher and Sabharwal, Ashish and Herrick, Austin and Efrat,
                   Avia and Erdem, Aykut and Karaka{\c s}, Ayla and Ryan
                   Roberts, B and Loe, Bao Sheng and Zoph, Barret and
                   Bojanowski, Bart{\l}omiej and {\"O}zyurt, Batuhan and
                   Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin
                   and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and
                   Howald, Blake and Orinion, Bryan and Diao, Cameron and Dour,
                   Cameron and Stinson, Catherine and Argueta, Cedrick and
                   Ram{\'\i}rez, C{\'e}sar Ferri and Singh, Chandan and
                   Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and
                   Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and
                   Voigt, Christian and Manning, Christopher D and Potts,
                   Christopher and Ramirez, Cindy and Rivera, Clara E and Siro,
                   Clemencia and Raffel, Colin and Ashcraft, Courtney and
                   Garbacea, Cristina and Sileo, Damien and Garrette, Dan and
                   Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman,
                   Daniel and Khashabi, Daniel and Levy, Daniel and
                   Gonz{\'a}lez, Daniel Mosegu{\'\i} and Perszyk, Danielle and
                   Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and
                   Gilboa, Dar and Dohan, David and Drakard, David and Jurgens,
                   David and Datta, Debajyoti and Ganguli, Deep and Emelin,
                   Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and
                   Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan,
                   Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee,
                   Dong-Ho and Schrader, Dylan and Shutova, Ekaterina and
                   Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and
                   Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie
                   and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang,
                   Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A and
                   Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi,
                   Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue
                   and Siar, Fatemeh and Mart{\'\i}nez-Plumed, Fernando and
                   Happ{\'e}, Francesca and Chollet, Francois and Rong, Frieda
                   and Mishra, Gaurav and Winata, Genta Indra and de Melo,
                   Gerard and Kruszewski, Germ{\'a}n and Parascandolo,
                   Giambattista and Mariani, Giorgio and Wang, Gloria and
                   Jaimovitch-L{\'o}pez, Gonzalo and Betz, Gregor and Gur-Ari,
                   Guy and Galijasevic, Hana and Kim, Hannah and Rashkin,
                   Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar,
                   Hayden and Shevlin, Henry and Sch{\"u}tze, Hinrich and
                   Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and
                   Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger,
                   Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon
                   and Fisac, Jaime Fern{\'a}ndez and Simon, James B and
                   Koppel, James and Zheng, James and Zou, James and Koco{\'n},
                   Jan and Thompson, Jana and Wingfield, Janelle and Kaplan,
                   Jared and Radom, Jarema and Sohl-Dickstein, Jascha and
                   Phang, Jason and Wei, Jason and Yosinski, Jason and
                   Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer
                   and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi,
                   Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang,
                   Jillian and Waweru, Joan and Burden, John and Miller, John
                   and Balis, John U and Batchelder, Jonathan and Berant,
                   Jonathan and Frohberg, J{\"o}rg and Rozen, Jos and
                   Hernandez-Orallo, Jose and Boudeman, Joseph and Guerr,
                   Joseph and Jones, Joseph and Tenenbaum, Joshua B and Rule,
                   Joshua S and Chua, Joyce and Kanclerz, Kamil and Livescu,
                   Karen and Krauth, Karl and Gopalakrishnan, Karthik and
                   Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D
                   and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and
                   Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar
                   and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria
                   and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui
                   and Contreras-Ochando, Lidia and Morency, Louis-Philippe and
                   Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt,
                   Ludwig and He, Luheng and Col{\'o}n, Luis Oliveros and Metz,
                   Luke and {\c S}enel, L{\"u}tfi Kerem and Bosma, Maarten and
                   Sap, Maarten and ter Hoeve, Maartje and Farooqi, Maheen and
                   Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and
                   Marelli, Marco and Maru, Marco and Quintana, Maria Jose
                   Ram{\'\i}rez and Tolkiehn, Marie and Giulianelli, Mario and
                   Lewis, Martha and Potthast, Martin and Leavitt, Matthew L
                   and Hagen, Matthias and Schubert, M{\'a}ty{\'a}s and
                   Baitemirova, Medina Orduna and Arnaud, Melody and McElrath,
                   Melvin and Yee, Michael A and Cohen, Michael and Gu, Michael
                   and Ivanitskiy, Michael and Starritt, Michael and Strube,
                   Michael and Sw{\k e}drowski, Micha{\l} and Bevilacqua,
                   Michele and Yasunaga, Michihiro and Kale, Mihir and Cain,
                   Mike and Xu, Mimee and Suzgun, Mirac and Walker, Mitch and
                   Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva,
                   Mor and Gheini, Mozhdeh and Mukund, Varma T and Peng, Nanyun
                   and Chi, Nathan A and Lee, Nayeon and Krakover, Neta Gur-Ari
                   and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick
                   and Martinez, Nicole and Nangia, Nikita and Deckers, Niklas
                   and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer,
                   Niveditha S and Constant, Noah and Fiedel, Noah and Wen,
                   Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar
                   and Levy, Omer and Evans, Owain and Casares, Pablo Antonio
                   Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu
                   and Vicol, Paul and Alipoormolabashi, Pegah and Liao,
                   Peiyuan and Liang, Percy and Chang, Peter and Eckersley,
                   Peter and Htut, Phu Mon and Hwang, Pinyu and Mi{\l}kowski,
                   Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli,
                   Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and
                   Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer
                   and Habacker, Rahel and Risco, Ramon and Milli{\`e}re,
                   Rapha{\"e}l and Garg, Rhythm and Barnes, Richard and
                   Saurous, Rif A and Arakawa, Riku and Raymaekers, Robbe and
                   Frank, Robert and Sikand, Rohan and Novak, Roman and
                   Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and
                   Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and
                   Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan
                   and Yang, Rylan and Singh, Sahib and Mohammad, Saif M and
                   Anand, Sajant and Dillavou, Sam and Shleifer, Sam and
                   Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R and
                   Schoenholz, Samuel S and Han, Sanghyun and Kwatra, Sanjeev
                   and Rous, Sarah A and Ghazarian, Sarik and Ghosh, Sayan and
                   Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian
                   and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan,
                   Shadi and Zhou, Sharon and Srivastava, Shashank and Shi,
                   Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang
                   Shane and Pachchigar, Shubh and Toshniwal, Shubham and
                   Upadhyay, Shyam and {Shyamolima} and {Debnath} and Shakeri,
                   Siamak and Thormeyer, Simon and Melzi, Simone and Reddy,
                   Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and
                   Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas
                   and Divic, Stefan and Ermon, Stefano and Biderman, Stella
                   and Lin, Stephanie and Prasad, Stephen and Piantadosi,
                   Steven T and Shieber, Stuart M and Misherghi, Summer and
                   Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal
                   and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and
                   Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Th{\'e}o and
                   Rothschild, Theodore and Phan, Thomas and Wang, Tianle and
                   Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and
                   Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton
                   and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and
                   Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai,
                   Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu,
                   Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and
                   Fedus, William and Saunders, William and Zhang, William and
                   Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao,
                   Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh,
                   Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri,
                   Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and
                   Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang
                   and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang,
                   Zijian and Wang, Zijie J and Wang, Zirui and Wu, Ziyi",
  abstract      = "Language models demonstrate both quantitative improvement
                   and new qualitative capabilities with increasing scale.
                   Despite their potentially transformative impact, these new
                   capabilities are as yet poorly characterized. In order to
                   inform future research, prepare for disruptive new model
                   capabilities, and ameliorate socially harmful effects, it is
                   vital that we understand the present and near-future
                   capabilities and limitations of language models. To address
                   this challenge, we introduce the Beyond the Imitation Game
                   benchmark (BIG-bench). BIG-bench currently consists of 204
                   tasks, contributed by 450 authors across 132 institutions.
                   Task topics are diverse, drawing problems from linguistics,
                   childhood development, math, common-sense reasoning,
                   biology, physics, social bias, software development, and
                   beyond. BIG-bench focuses on tasks that are believed to be
                   beyond the capabilities of current language models. We
                   evaluate the behavior of OpenAI's GPT models,
                   Google-internal dense transformer architectures, and
                   Switch-style sparse transformers on BIG-bench, across model
                   sizes spanning millions to hundreds of billions of
                   parameters. In addition, a team of human expert raters
                   performed all tasks in order to provide a strong baseline.
                   Findings include: model performance and calibration both
                   improve with scale, but are poor in absolute terms (and when
                   compared with rater performance); performance is remarkably
                   similar across model classes, though with benefits from
                   sparsity; tasks that improve gradually and predictably
                   commonly involve a large knowledge or memorization
                   component, whereas tasks that exhibit ``breakthrough''
                   behavior at a critical scale often involve multiple steps or
                   components, or brittle metrics; social bias typically
                   increases with scale in settings with ambiguous context, but
                   this can be improved with prompting.",
  month         =  jun,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2206.04615"
}

@MISC{Suzgun_undated-vb,
  title       = "{BIG-Bench-Hard}: Challenging {BIG-Bench} Tasks and Whether
                 {Chain-of-Thought} Can Solve Them",
  author      = "Suzgun, Mirac",
  abstract    = "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can
                 Solve Them - suzgunmirac/BIG-Bench-Hard: Challenging BIG-Bench
                 Tasks and Whether Chain-of-Thought Can Solve Them",
  institution = "Github",
  language    = "en"
}

@ARTICLE{Yao2022-bv,
  title         = "{ReAct}: Synergizing Reasoning and Acting in Language Models",
  author        = "Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and
                   Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan",
  abstract      = "While large language models (LLMs) have demonstrated
                   impressive capabilities across tasks in language
                   understanding and interactive decision making, their
                   abilities for reasoning (e.g. chain-of-thought prompting)
                   and acting (e.g. action plan generation) have primarily been
                   studied as separate topics. In this paper, we explore the
                   use of LLMs to generate both reasoning traces and
                   task-specific actions in an interleaved manner, allowing for
                   greater synergy between the two: reasoning traces help the
                   model induce, track, and update action plans as well as
                   handle exceptions, while actions allow it to interface with
                   external sources, such as knowledge bases or environments,
                   to gather additional information. We apply our approach,
                   named ReAct, to a diverse set of language and decision
                   making tasks and demonstrate its effectiveness over
                   state-of-the-art baselines, as well as improved human
                   interpretability and trustworthiness over methods without
                   reasoning or acting components. Concretely, on question
                   answering (HotpotQA) and fact verification (Fever), ReAct
                   overcomes issues of hallucination and error propagation
                   prevalent in chain-of-thought reasoning by interacting with
                   a simple Wikipedia API, and generates human-like
                   task-solving trajectories that are more interpretable than
                   baselines without reasoning traces. On two interactive
                   decision making benchmarks (ALFWorld and WebShop), ReAct
                   outperforms imitation and reinforcement learning methods by
                   an absolute success rate of 34\% and 10\% respectively,
                   while being prompted with only one or two in-context
                   examples. Project site with code: https://react-lm.github.io",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.03629"
}

@ARTICLE{Grosse2023-xk,
  title         = "Studying large language model generalization with influence
                   functions",
  author        = "Grosse, Roger and Bae, Juhan and Anil, Cem and Elhage,
                   Nelson and Tamkin, Alex and Tajdini, Amirhossein and
                   Steiner, Benoit and Li, Dustin and Durmus, Esin and Perez,
                   Ethan and Hubinger, Evan and Luko{\v s}i{\=u}t{\.e},
                   Kamil{\.e} and Nguyen, Karina and Joseph, Nicholas and
                   McCandlish, Sam and Kaplan, Jared and Bowman, Samuel R",
  abstract      = "When trying to gain better visibility into a machine
                   learning model in order to understand and mitigate the
                   associated risks, a potentially valuable source of evidence
                   is: which training examples most contribute to a given
                   behavior? Influence functions aim to answer a
                   counterfactual: how would the model's parameters (and hence
                   its outputs) change if a given sequence were added to the
                   training set? While influence functions have produced
                   insights for small models, they are difficult to scale to
                   large language models (LLMs) due to the difficulty of
                   computing an inverse-Hessian-vector product (IHVP). We use
                   the Eigenvalue-corrected Kronecker-Factored Approximate
                   Curvature (EK-FAC) approximation to scale influence
                   functions up to LLMs with up to 52 billion parameters. In
                   our experiments, EK-FAC achieves similar accuracy to
                   traditional influence function estimators despite the IHVP
                   computation being orders of magnitude faster. We investigate
                   two algorithmic techniques to reduce the cost of computing
                   gradients of candidate training sequences: TF-IDF filtering
                   and query batching. We use influence functions to
                   investigate the generalization patterns of LLMs, including
                   the sparsity of the influence patterns, increasing
                   abstraction with scale, math and programming abilities,
                   cross-lingual generalization, and role-playing behavior.
                   Despite many apparently sophisticated forms of
                   generalization, we identify a surprising limitation:
                   influences decay to near-zero when the order of key phrases
                   is flipped. Overall, influence functions give us a powerful
                   new tool for studying the generalization properties of LLMs.",
  month         =  aug,
  year          =  2023,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2308.03296"
}

@ARTICLE{noauthor_undated-if,
  title = "ltfp\_book.pdf"
}

@ARTICLE{Ignat2023-ou,
  title         = "A {PhD} Student's Perspective on Research in {NLP} in the
                   Era of Very Large Language Models",
  author        = "Ignat, Oana and Jin, Zhijing and Abzaliev, Artem and
                   Biester, Laura and Castro, Santiago and Deng, Naihao and
                   Gao, Xinyi and Gunal, Aylin and He, Jacky and Kazemi, Ashkan
                   and Khalifa, Muhammad and Koh, Namho and Lee, Andrew and
                   Liu, Siyang and Do June, Min and Mori, Shinka and Nwatu,
                   Joan and Perez-Rosas, Veronica and Shen, Siqi and Wang,
                   Zekun and Wu, Winston and Mihalcea, Rada",
  abstract      = "Recent progress in large language models has enabled the
                   deployment of many generative NLP applications. At the same
                   time, it has also led to a misleading public discourse that
                   ``it's all been solved.'' Not surprisingly, this has in turn
                   made many NLP researchers -- especially those at the
                   beginning of their career -- wonder about what NLP research
                   area they should focus on. This document is a compilation of
                   NLP research directions that are rich for exploration,
                   reflecting the views of a diverse group of PhD students in
                   an academic research lab. While we identify many research
                   areas, many others exist; we do not cover those areas that
                   are currently addressed by LLMs but where LLMs lag behind in
                   performance, or those focused on LLM development. We welcome
                   suggestions for other research directions to include:
                   https://bit.ly/nlp-era-llm",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.12544"
}

@ARTICLE{Reimers2019-mq,
  title         = "{Sentence-BERT}: Sentence embeddings using Siamese
                   {BERT-networks}",
  author        = "Reimers, Nils and Gurevych, Iryna",
  abstract      = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019)
                   has set a new state-of-the-art performance on sentence-pair
                   regression tasks like semantic textual similarity (STS).
                   However, it requires that both sentences are fed into the
                   network, which causes a massive computational overhead:
                   Finding the most similar pair in a collection of 10,000
                   sentences requires about 50 million inference computations
                   (~65 hours) with BERT. The construction of BERT makes it
                   unsuitable for semantic similarity search as well as for
                   unsupervised tasks like clustering. In this publication, we
                   present Sentence-BERT (SBERT), a modification of the
                   pretrained BERT network that use siamese and triplet network
                   structures to derive semantically meaningful sentence
                   embeddings that can be compared using cosine-similarity.
                   This reduces the effort for finding the most similar pair
                   from 65 hours with BERT / RoBERTa to about 5 seconds with
                   SBERT, while maintaining the accuracy from BERT. We evaluate
                   SBERT and SRoBERTa on common STS tasks and transfer learning
                   tasks, where it outperforms other state-of-the-art sentence
                   embeddings methods.",
  month         =  aug,
  year          =  2019,
  copyright     = "http://creativecommons.org/licenses/by-sa/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1908.10084"
}

@ARTICLE{Ren2020-mp,
  title         = "A Survey of Deep Active Learning",
  author        = "Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang,
                   Po-Yao and Li, Zhihui and Gupta, Brij B and Chen, Xiaojiang
                   and Wang, Xin",
  abstract      = "Active learning (AL) attempts to maximize the performance
                   gain of the model by marking the fewest samples. Deep
                   learning (DL) is greedy for data and requires a large amount
                   of data supply to optimize massive parameters, so that the
                   model learns how to extract high-quality features. In recent
                   years, due to the rapid development of internet technology,
                   we are in an era of information torrents and we have massive
                   amounts of data. In this way, DL has aroused strong interest
                   of researchers and has been rapidly developed. Compared with
                   DL, researchers have relatively low interest in AL. This is
                   mainly because before the rise of DL, traditional machine
                   learning requires relatively few labeled samples. Therefore,
                   early AL is difficult to reflect the value it deserves.
                   Although DL has made breakthroughs in various fields, most
                   of this success is due to the publicity of the large number
                   of existing annotation datasets. However, the acquisition of
                   a large number of high-quality annotated datasets consumes a
                   lot of manpower, which is not allowed in some fields that
                   require high expertise, especially in the fields of speech
                   recognition, information extraction, medical images, etc.
                   Therefore, AL has gradually received due attention. A
                   natural idea is whether AL can be used to reduce the cost of
                   sample annotations, while retaining the powerful learning
                   capabilities of DL. Therefore, deep active learning (DAL)
                   has emerged. Although the related research has been quite
                   abundant, it lacks a comprehensive survey of DAL. This
                   article is to fill this gap, we provide a formal
                   classification method for the existing work, and a
                   comprehensive and systematic overview. In addition, we also
                   analyzed and summarized the development of DAL from the
                   perspective of application. Finally, we discussed the
                   confusion and problems in DAL, and gave some possible
                   development directions for DAL.",
  month         =  aug,
  year          =  2020,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2009.00236"
}

@ARTICLE{Bojarski2016-ub,
  title         = "End to End Learning for {Self-Driving} Cars",
  author        = "Bojarski, Mariusz and Del Testa, Davide and Dworakowski,
                   Daniel and Firner, Bernhard and Flepp, Beat and Goyal,
                   Prasoon and Jackel, Lawrence D and Monfort, Mathew and
                   Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake
                   and Zieba, Karol",
  abstract      = "We trained a convolutional neural network (CNN) to map raw
                   pixels from a single front-facing camera directly to
                   steering commands. This end-to-end approach proved
                   surprisingly powerful. With minimum training data from
                   humans the system learns to drive in traffic on local roads
                   with or without lane markings and on highways. It also
                   operates in areas with unclear visual guidance such as in
                   parking lots and on unpaved roads. The system automatically
                   learns internal representations of the necessary processing
                   steps such as detecting useful road features with only the
                   human steering angle as the training signal. We never
                   explicitly trained it to detect, for example, the outline of
                   roads. Compared to explicit decomposition of the problem,
                   such as lane marking detection, path planning, and control,
                   our end-to-end system optimizes all processing steps
                   simultaneously. We argue that this will eventually lead to
                   better performance and smaller systems. Better performance
                   will result because the internal components self-optimize to
                   maximize overall system performance, instead of optimizing
                   human-selected intermediate criteria, e.g., lane detection.
                   Such criteria understandably are selected for ease of human
                   interpretation which doesn't automatically guarantee maximum
                   system performance. Smaller networks are possible because
                   the system learns to solve the problem with the minimal
                   number of processing steps. We used an NVIDIA DevBox and
                   Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving
                   car computer also running Torch 7 for determining where to
                   drive. The system operates at 30 frames per second (FPS).",
  month         =  apr,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1604.07316"
}

@ARTICLE{Saphra2023-vh,
  title         = "First Tragedy, then Parse: History Repeats Itself in the New
                   Era of Large Language Models",
  author        = "Saphra, Naomi and Fleisig, Eve and Cho, Kyunghyun and Lopez,
                   Adam",
  abstract      = "Many NLP researchers are experiencing an existential crisis
                   triggered by the astonishing success of ChatGPT and other
                   systems based on large language models (LLMs). After such a
                   disruptive change to our understanding of the field, what is
                   left to do? Taking a historical lens, we look for guidance
                   from the first era of LLMs, which began in 2005 with large
                   $n$-gram models for machine translation. We identify durable
                   lessons from the first era, and more importantly, we
                   identify evergreen problems where NLP researchers can
                   continue to make meaningful contributions in areas where
                   LLMs are ascendant. Among these lessons, we discuss the
                   primacy of hardware advancement in shaping the availability
                   and importance of scale, as well as the urgent challenge of
                   quality evaluation, both automated and human. We argue that
                   disparities in scale are transient and that researchers can
                   work to reduce them; that data, rather than hardware, is
                   still a bottleneck for many meaningful applications; that
                   meaningful evaluation informed by actual use is still an
                   open problem; and that there is still room for speculative
                   approaches.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2311.05020"
}

@ARTICLE{Mitra2023-hg,
  title         = "Orca 2: Teaching Small Language Models How to Reason",
  author        = "Mitra, Arindam and Del Corro, Luciano and Mahajan, Shweti
                   and Codas, Andres and Simoes, Clarisse and Agarwal, Sahaj
                   and Chen, Xuxi and Razdaibiedina, Anastasia and Jones, Erik
                   and Aggarwal, Kriti and Palangi, Hamid and Zheng, Guoqing
                   and Rosset, Corby and Khanpour, Hamed and Awadallah, Ahmed",
  abstract      = "Orca 1 learns from rich signals, such as explanation traces,
                   allowing it to outperform conventional instruction-tuned
                   models on benchmarks like BigBench Hard and AGIEval. In Orca
                   2, we continue exploring how improved training signals can
                   enhance smaller LMs' reasoning abilities. Research on
                   training small LMs has often relied on imitation learning to
                   replicate the output of more capable models. We contend that
                   excessive emphasis on imitation may restrict the potential
                   of smaller models. We seek to teach small LMs to employ
                   different solution strategies for different tasks,
                   potentially different from the one used by the larger model.
                   For example, while larger models might provide a direct
                   answer to a complex task, smaller models may not have the
                   same capacity. In Orca 2, we teach the model various
                   reasoning techniques (step-by-step, recall then generate,
                   recall-reason-generate, direct answer, etc.). More
                   crucially, we aim to help the model learn to determine the
                   most effective solution strategy for each task. We evaluate
                   Orca 2 using a comprehensive set of 15 diverse benchmarks
                   (corresponding to approximately 100 tasks and over 36,000
                   unique prompts). Orca 2 significantly surpasses models of
                   similar size and attains performance levels similar or
                   better to those of models 5-10x larger, as assessed on
                   complex tasks that test advanced reasoning abilities in
                   zero-shot settings. make Orca 2 weights publicly available
                   at aka.ms/orca-lm to support research on the development,
                   evaluation, and alignment of smaller LMs",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2311.11045"
}

@ARTICLE{Weston2023-wh,
  title         = "System 2 Attention (is something you might need too)",
  author        = "Weston, Jason and Sukhbaatar, Sainbayar",
  abstract      = "Soft attention in Transformer-based Large Language Models
                   (LLMs) is susceptible to incorporating irrelevant
                   information from the context into its latent
                   representations, which adversely affects next token
                   generations. To help rectify these issues, we introduce
                   System 2 Attention (S2A), which leverages the ability of
                   LLMs to reason in natural language and follow instructions
                   in order to decide what to attend to. S2A regenerates the
                   input context to only include the relevant portions, before
                   attending to the regenerated context to elicit the final
                   response. In experiments, S2A outperforms standard
                   attention-based LLMs on three tasks containing opinion or
                   irrelevant information, QA, math word problems and longform
                   generation, where S2A increases factuality and objectivity,
                   and decreases sycophancy.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2311.11829"
}

@ARTICLE{Richardson2023-ll,
  title         = "Integrating Summarization and Retrieval for Enhanced
                   Personalization via Large Language Models",
  author        = "Richardson, Chris and Zhang, Yao and Gillespie, Kellen and
                   Kar, Sudipta and Singh, Arshdeep and Raeesy, Zeynab and
                   Khan, Omar Zia and Sethy, Abhinav",
  abstract      = "Personalization, the ability to tailor a system to
                   individual users, is an essential factor in user experience
                   with natural language processing (NLP) systems. With the
                   emergence of Large Language Models (LLMs), a key question is
                   how to leverage these models to better personalize user
                   experiences. To personalize a language model's output, a
                   straightforward approach is to incorporate past user data
                   into the language model prompt, but this approach can result
                   in lengthy inputs exceeding limitations on input length and
                   incurring latency and cost issues. Existing approaches
                   tackle such challenges by selectively extracting relevant
                   user data (i.e. selective retrieval) to construct a prompt
                   for downstream tasks. However, retrieval-based methods are
                   limited by potential information loss, lack of more profound
                   user understanding, and cold-start challenges. To overcome
                   these limitations, we propose a novel summary-augmented
                   approach by extending retrieval-augmented personalization
                   with task-aware user summaries generated by LLMs. The
                   summaries can be generated and stored offline, enabling
                   real-world systems with runtime constraints like voice
                   assistants to leverage the power of LLMs. Experiments show
                   our method with 75\% less of retrieved user data is on-par
                   or outperforms retrieval augmentation on most tasks in the
                   LaMP personalization benchmark. We demonstrate that offline
                   summarization via LLMs and runtime retrieval enables better
                   performance for personalization on a range of tasks under
                   practical constraints.",
  month         =  oct,
  year          =  2023,
  keywords      = "Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.20081"
}

@ARTICLE{Power2022-vj,
  title         = "Grokking: Generalization Beyond Overfitting on Small
                   Algorithmic Datasets",
  author        = "Power, Alethea and Burda, Yuri and Edwards, Harri and
                   Babuschkin, Igor and Misra, Vedant",
  abstract      = "In this paper we propose to study generalization of neural
                   networks on small algorithmically generated datasets. In
                   this setting, questions about data efficiency, memorization,
                   generalization, and speed of learning can be studied in
                   great detail. In some situations we show that neural
                   networks learn through a process of ``grokking'' a pattern
                   in the data, improving generalization performance from
                   random chance level to perfect generalization, and that this
                   improvement in generalization can happen well past the point
                   of overfitting. We also study generalization as a function
                   of dataset size and find that smaller datasets require
                   increasing amounts of optimization for generalization. We
                   argue that these datasets provide a fertile ground for
                   studying a poorly understood aspect of deep learning:
                   generalization of overparametrized neural networks beyond
                   memorization of the finite training dataset.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2201.02177"
}

@MISC{Roziere_undated-xd,
  title       = "codellama: Inference code for {CodeLlama} models",
  author      = "Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian
                 and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and
                 Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin,
                 J{\'e}r{\'e}my and Kozhevnikov, Artyom and Evtimov, Ivan and
                 Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton
                 and Grattafiori, Aaron and Xiong, Wenhan and D{\'e}fossez,
                 Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo
                 and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and
                 Synnaeve, Gabriel",
  abstract    = "Inference code for CodeLlama models. Contribute to
                 facebookresearch/codellama development by creating an account
                 on GitHub.",
  institution = "Github",
  language    = "en"
}

@MISC{Communication_undated-zi,
  title        = "{SeamlessM4T---Massively} Multilingual \& Multimodal Machine
                  Translation",
  author       = "Communication, Seamless and Barrault, Lo{\"\i}c and Chung,
                  Yu-An and Meglioli, Mariano Cora and Dale, David and Dong,
                  Ning and Duquenne, Paul-Ambroise and Elsahar, Hady and Gong,
                  Hongyu and Heffernan, Kevin and Hoffman, John and Klaiber,
                  Christopher and Li, Pengwei and Licht, Daniel and Maillard,
                  Jean and Rakotoarison, Alice and Sadagopan, Kaushik Ram and
                  Wenzek, Guillaume and Ye, Ethan and Akula, Bapi and Chen,
                  Peng-Jen and El Hachem, Naji and Ellis, Brian and Gonzalez,
                  Gabriel Mejia and Haaheim, Justin and Hansanti, Prangthip and
                  Howes, Russ and Huang, Bernie and Hwang, Min-Jae and Inaguma,
                  Hirofumi and Jain, Somya and Kalbassi, Elahe and Kallet,
                  Amanda and Kulikov, Ilia and Lam, Janice and Li, Daniel and
                  Ma, Xutai and Mavlyutov, Ruslan and Peloquin, Benjamin and
                  Ramadan, Mohamed and Ramakrishnan, Abinesh and Sun, Anna and
                  Tran, Kevin and Tran, Tuan and Tufanov, Igor and Vogeti, Vish
                  and Wood, Carleigh and Yang, Yilin and Yu, Bokai and Andrews,
                  Pierre and Balioglu, Can and Costa-juss{\`a}, Marta R and
                  Celebi, Onur and Elbayad, Maha and Gao, Cynthia and
                  Guzm{\'a}n, Francisco and Kao, Justine and Lee, Ann and
                  Mourachko, Alexandre and Pino, Juan and Popuri, Sravya and
                  Ropers, Christophe and Saleem, Safiyyah and Schwenk, Holger
                  and Tomasello, Paden and Wang, Changhan and Wang, Jeff and
                  Wang, Skyler",
  howpublished = "\url{https://scontent.ficn2-2.fna.fbcdn.net/v/t39.2365-6/369747868_602316515432698_2401716319310287708_n.pdf?_nc_cat=106&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=ylAEVY4IFdQAX-6bffi&_nc_ht=scontent.ficn2-2.fna&oh=00_AfDO3UakEmHCxF-nQxq69rycHbE1GAa3rGTDpiGaHCqnow&oe=64F082B9}"
}

@ARTICLE{Azar2023-wj,
  title         = "A General Theoretical Paradigm to Understand Learning from
                   Human Preferences",
  author        = "Azar, Mohammad Gheshlaghi and Rowland, Mark and Piot, Bilal
                   and Guo, Daniel and Calandriello, Daniele and Valko, Michal
                   and Munos, R{\'e}mi",
  abstract      = "The prevalent deployment of learning from human preferences
                   through reinforcement learning (RLHF) relies on two
                   important approximations: the first assumes that pairwise
                   preferences can be substituted with pointwise rewards. The
                   second assumes that a reward model trained on these
                   pointwise rewards can generalize from collected data to
                   out-of-distribution data sampled by the policy. Recently,
                   Direct Preference Optimisation (DPO) has been proposed as an
                   approach that bypasses the second approximation and learn
                   directly a policy from collected data without the reward
                   modelling stage. However, this method still heavily relies
                   on the first approximation. In this paper we try to gain a
                   deeper theoretical understanding of these practical
                   algorithms. In particular we derive a new general objective
                   called $\Psi$PO for learning from human preferences that is
                   expressed in terms of pairwise preferences and therefore
                   bypasses both approximations. This new general objective
                   allows us to perform an in-depth analysis of the behavior of
                   RLHF and DPO (as special cases of $\Psi$PO) and to identify
                   their potential pitfalls. We then consider another special
                   case for $\Psi$PO by setting $\Psi$ simply to Identity, for
                   which we can derive an efficient optimisation procedure,
                   prove performance guarantees and demonstrate its empirical
                   superiority to DPO on some illustrative examples.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2310.12036"
}

@ARTICLE{Wang2023-sq,
  title         = "{PandaLM}: An Automatic Evaluation Benchmark for {LLM}
                   Instruction Tuning Optimization",
  author        = "Wang, Yidong and Yu, Zhuohao and Zeng, Zhengran and Yang,
                   Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and
                   Xie, Rui and Wang, Jindong and Xie, Xing and Ye, Wei and
                   Zhang, Shikun and Zhang, Yue",
  abstract      = "Instruction tuning large language models (LLMs) remains a
                   challenging task, owing to the complexity of hyperparameter
                   selection and the difficulty involved in evaluating the
                   tuned models. To determine the optimal hyperparameters, an
                   automatic, robust, and reliable evaluation benchmark is
                   essential. However, establishing such a benchmark is not a
                   trivial task due to the challenges associated with
                   evaluation accuracy and privacy protection. In response to
                   these challenges, we introduce a judge large language model,
                   named PandaLM, which is trained to distinguish the superior
                   model given several LLMs. PandaLM's focus extends beyond
                   just the objective correctness of responses, which is the
                   main focus of traditional evaluation datasets. It addresses
                   vital subjective factors such as relative conciseness,
                   clarity, adherence to instructions, comprehensiveness, and
                   formality. To ensure the reliability of PandaLM, we collect
                   a diverse human-annotated test dataset, where all contexts
                   are generated by humans and labels are aligned with human
                   preferences. Our results indicate that PandaLM-7B achieves
                   93.75\% of GPT-3.5's evaluation ability and 88.28\% of
                   GPT-4's in terms of F1-score on our test dataset. PandaLM
                   enables the evaluation of LLM to be fairer but with less
                   cost, evidenced by significant improvements achieved by
                   models tuned through PandaLM compared to their counterparts
                   trained with default Alpaca's hyperparameters. In addition,
                   PandaLM does not depend on API-based evaluations, thus
                   avoiding potential data leakage. All resources of PandaLM
                   are released at https://github.com/WeOpenML/PandaLM.",
  month         =  jun,
  year          =  2023,
  keywords      = "BiGGen-Bench",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.05087"
}

@ARTICLE{Zhu2023-la,
  title         = "{JudgeLM}: Fine-tuned Large Language Models are Scalable
                   Judges",
  author        = "Zhu, Lianghui and Wang, Xinggang and Wang, Xinlong",
  abstract      = "Evaluating Large Language Models (LLMs) in open-ended
                   scenarios is challenging because existing benchmarks and
                   metrics can not measure them comprehensively. To address
                   this problem, we propose to fine-tune LLMs as scalable
                   judges (JudgeLM) to evaluate LLMs efficiently and
                   effectively in open-ended benchmarks. We first propose a
                   comprehensive, large-scale, high-quality dataset containing
                   task seeds, LLMs-generated answers, and GPT-4-generated
                   judgments for fine-tuning high-performance judges, as well
                   as a new benchmark for evaluating the judges. We train
                   JudgeLM at different scales from 7B, 13B, to 33B parameters,
                   and conduct a systematic analysis of its capabilities and
                   behaviors. We then analyze the key biases in fine-tuning LLM
                   as a judge and consider them as position bias, knowledge
                   bias, and format bias. To address these issues, JudgeLM
                   introduces a bag of techniques including swap augmentation,
                   reference support, and reference drop, which clearly enhance
                   the judge's performance. JudgeLM obtains the
                   state-of-the-art judge performance on both the existing
                   PandaLM benchmark and our proposed new benchmark. Our
                   JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes
                   to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high
                   agreement with the teacher judge, achieving an agreement
                   exceeding 90\% that even surpasses human-to-human agreement.
                   JudgeLM also demonstrates extended capabilities in being
                   judges of the single answer, multimodal models, multiple
                   answers, and multi-turn chat.",
  month         =  oct,
  year          =  2023,
  keywords      = "BiGGen-Bench",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.17631"
}

@ARTICLE{Belcak2023-zo,
  title         = "Fast Feedforward Networks",
  author        = "Belcak, Peter and Wattenhofer, Roger",
  abstract      = "We break the linear link between the layer size and its
                   inference cost by introducing the fast feedforward (FFF)
                   architecture, a log-time alternative to feedforward
                   networks. We demonstrate that FFFs are up to 220x faster
                   than feedforward networks, up to 6x faster than
                   mixture-of-experts networks, and exhibit better training
                   properties than mixtures of experts thanks to noiseless
                   conditional execution. Pushing FFFs to the limit, we show
                   that they can use as little as 1\% of layer neurons for
                   inference in vision transformers while preserving 94.2\% of
                   predictive performance.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2308.14711"
}

@ARTICLE{Komeili2021-ph,
  title         = "{Internet-Augmented} Dialogue Generation",
  author        = "Komeili, Mojtaba and Shuster, Kurt and Weston, Jason",
  abstract      = "The largest store of continually updating knowledge on our
                   planet can be accessed via internet search. In this work we
                   study giving access to this information to conversational
                   agents. Large language models, even though they store an
                   impressive amount of knowledge within their weights, are
                   known to hallucinate facts when generating dialogue (Shuster
                   et al., 2021); moreover, those facts are frozen in time at
                   the point of model training. In contrast, we propose an
                   approach that learns to generate an internet search query
                   based on the context, and then conditions on the search
                   results to finally generate a response, a method that can
                   employ up-to-the-minute relevant information. We train and
                   evaluate such models on a newly collected dataset of
                   human-human conversations whereby one of the speakers is
                   given access to internet search during knowledgedriven
                   discussions in order to ground their responses. We find that
                   search-query based access of the internet in conversation
                   provides superior performance compared to existing
                   approaches that either use no augmentation or FAISS-based
                   retrieval (Lewis et al., 2020).",
  month         =  jul,
  year          =  2021,
  keywords      = "Agent;Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2107.07566"
}

@ARTICLE{Zoph2016-ux,
  title         = "Neural Architecture Search with Reinforcement Learning",
  author        = "Zoph, Barret and Le, Quoc V",
  abstract      = "Neural networks are powerful and flexible models that work
                   well for many difficult learning tasks in image, speech and
                   natural language understanding. Despite their success,
                   neural networks are still hard to design. In this paper, we
                   use a recurrent network to generate the model descriptions
                   of neural networks and train this RNN with reinforcement
                   learning to maximize the expected accuracy of the generated
                   architectures on a validation set. On the CIFAR-10 dataset,
                   our method, starting from scratch, can design a novel
                   network architecture that rivals the best human-invented
                   architecture in terms of test set accuracy. Our CIFAR-10
                   model achieves a test error rate of 3.65, which is 0.09
                   percent better and 1.05x faster than the previous
                   state-of-the-art model that used a similar architectural
                   scheme. On the Penn Treebank dataset, our model can compose
                   a novel recurrent cell that outperforms the widely-used LSTM
                   cell, and other state-of-the-art baselines. Our cell
                   achieves a test set perplexity of 62.4 on the Penn Treebank,
                   which is 3.6 perplexity better than the previous
                   state-of-the-art model. The cell can also be transferred to
                   the character language modeling task on PTB and achieves a
                   state-of-the-art perplexity of 1.214.",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1611.01578"
}

@ARTICLE{Gu2022-rv,
  title         = "Don't Generate, Discriminate: A Proposal for Grounding
                   Language Models to {Real-World} Environments",
  author        = "Gu, Yu and Deng, Xiang and Su, Yu",
  abstract      = "A key missing capacity of current language models (LMs) is
                   grounding to real-world environments. Most existing work for
                   grounded language understanding uses LMs to directly
                   generate plans that can be executed in the environment to
                   achieve the desired effects. It thereby casts the burden of
                   ensuring grammaticality, faithfulness, and controllability
                   all on the LMs. We propose Pangu, a generic framework for
                   grounded language understanding that capitalizes on the
                   discriminative ability of LMs instead of their generative
                   ability. Pangu consists of a symbolic agent and a neural LM
                   working in a concerted fashion: The agent explores the
                   environment to incrementally construct valid plans, and the
                   LM evaluates the plausibility of the candidate plans to
                   guide the search process. A case study on the challenging
                   problem of knowledge base question answering (KBQA), which
                   features a massive environment, demonstrates the remarkable
                   effectiveness and flexibility of Pangu: A BERT-base LM is
                   sufficient for setting a new record on standard KBQA
                   datasets, and larger LMs further bring substantial gains.
                   Pangu also enables, for the first time, effective few-shot
                   in-context learning for KBQA with large LMs such as Codex.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2212.09736"
}

@ARTICLE{Paranjape2023-zj,
  title         = "{ART}: Automatic multi-step reasoning and tool-use for large
                   language models",
  author        = "Paranjape, Bhargavi and Lundberg, Scott and Singh, Sameer
                   and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Ribeiro,
                   Marco Tulio",
  abstract      = "Large language models (LLMs) can perform complex reasoning
                   in few- and zero-shot settings by generating intermediate
                   chain of thought (CoT) reasoning steps. Further, each
                   reasoning step can rely on external tools to support
                   computation beyond the core LLM capabilities (e.g.
                   search/running code). Prior work on CoT prompting and tool
                   use typically requires hand-crafting task-specific
                   demonstrations and carefully scripted interleaving of model
                   generations with tool use. We introduce Automatic Reasoning
                   and Tool-use (ART), a framework that uses frozen LLMs to
                   automatically generate intermediate reasoning steps as a
                   program. Given a new task to solve, ART selects
                   demonstrations of multi-step reasoning and tool use from a
                   task library. At test time, ART seamlessly pauses generation
                   whenever external tools are called, and integrates their
                   output before resuming generation. ART achieves a
                   substantial improvement over few-shot prompting and
                   automatic CoT on unseen tasks in the BigBench and MMLU
                   benchmarks, and matches performance of hand-crafted CoT
                   prompts on a majority of these tasks. ART is also
                   extensible, and makes it easy for humans to improve
                   performance by correcting errors in task-specific programs
                   or incorporating new tools, which we demonstrate by
                   drastically improving performance on select tasks with
                   minimal human intervention.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2303.09014"
}

@ARTICLE{Schick2023-kd,
  title         = "Toolformer: Language Models Can Teach Themselves to Use
                   Tools",
  author        = "Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto
                   and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer,
                   Luke and Cancedda, Nicola and Scialom, Thomas",
  abstract      = "Language models (LMs) exhibit remarkable abilities to solve
                   new tasks from just a few examples or textual instructions,
                   especially at scale. They also, paradoxically, struggle with
                   basic functionality, such as arithmetic or factual lookup,
                   where much simpler and smaller models excel. In this paper,
                   we show that LMs can teach themselves to use external tools
                   via simple APIs and achieve the best of both worlds. We
                   introduce Toolformer, a model trained to decide which APIs
                   to call, when to call them, what arguments to pass, and how
                   to best incorporate the results into future token
                   prediction. This is done in a self-supervised way, requiring
                   nothing more than a handful of demonstrations for each API.
                   We incorporate a range of tools, including a calculator, a
                   Q\&A system, two different search engines, a translation
                   system, and a calendar. Toolformer achieves substantially
                   improved zero-shot performance across a variety of
                   downstream tasks, often competitive with much larger models,
                   without sacrificing its core language modeling abilities.",
  month         =  feb,
  year          =  2023,
  keywords      = "LM Web Browsing",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2302.04761"
}

@ARTICLE{Qin2023-ug,
  title         = "Tool learning with foundation models",
  author        = "Qin, Yujia and Hu, Shengding and Lin, Yankai and Chen, Weize
                   and Ding, Ning and Cui, Ganqu and Zeng, Zheni and Huang,
                   Yufei and Xiao, Chaojun and Han, Chi and Fung, Yi Ren and
                   Su, Yusheng and Wang, Huadong and Qian, Cheng and Tian,
                   Runchu and Zhu, Kunlun and Liang, Shihao and Shen, Xingyu
                   and Xu, Bokai and Zhang, Zhen and Ye, Yining and Li, Bowen
                   and Tang, Ziwei and Yi, Jing and Zhu, Yuzhang and Dai,
                   Zhenning and Yan, Lan and Cong, Xin and Lu, Yaxi and Zhao,
                   Weilin and Huang, Yuxiang and Yan, Junxi and Han, Xu and
                   Sun, Xian and Li, Dahai and Phang, Jason and Yang, Cheng and
                   Wu, Tongshuang and Ji, Heng and Liu, Zhiyuan and Sun,
                   Maosong",
  abstract      = "Humans possess an extraordinary ability to create and
                   utilize tools, allowing them to overcome physical
                   limitations and explore new frontiers. With the advent of
                   foundation models, AI systems have the potential to be
                   equally adept in tool use as humans. This paradigm, i.e.,
                   tool learning with foundation models, combines the strengths
                   of specialized tools and foundation models to achieve
                   enhanced accuracy, efficiency, and automation in
                   problem-solving. Despite its immense potential, there is
                   still a lack of a comprehensive understanding of key
                   challenges, opportunities, and future endeavors in this
                   field. To this end, we present a systematic investigation of
                   tool learning in this paper. We first introduce the
                   background of tool learning, including its cognitive
                   origins, the paradigm shift of foundation models, and the
                   complementary roles of tools and models. Then we
                   recapitulate existing tool learning research into
                   tool-augmented and tool-oriented learning. We formulate a
                   general tool learning framework: starting from understanding
                   the user instruction, models should learn to decompose a
                   complex task into several subtasks, dynamically adjust their
                   plan through reasoning, and effectively conquer each
                   sub-task by selecting appropriate tools. We also discuss how
                   to train models for improved tool-use capabilities and
                   facilitate the generalization in tool learning. Considering
                   the lack of a systematic tool learning evaluation in prior
                   works, we experiment with 18 representative tools and show
                   the potential of current foundation models in skillfully
                   utilizing tools. Finally, we discuss several open problems
                   that require further investigation for tool learning.
                   Overall, we hope this paper could inspire future research in
                   integrating tools with foundation models.",
  month         =  apr,
  year          =  2023,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.08354"
}

@ARTICLE{Lee2023-br,
  title         = "Exploiting the Potential of {Seq2Seq} Models as Robust
                   {Few-Shot} Learners",
  author        = "Lee, Jihyeon and Kim, Dain and Jung, Doohae and Kim, Boseop
                   and On, Kyoung-Woon",
  abstract      = "In-context learning, which offers substantial advantages
                   over fine-tuning, is predominantly observed in decoder-only
                   models, while encoder-decoder (i.e., seq2seq) models excel
                   in methods that rely on weight updates. Recently, a few
                   studies have demonstrated the feasibility of few-shot
                   learning with seq2seq models; however, this has been limited
                   to tasks that align well with the seq2seq architecture, such
                   as summarization and translation. Inspired by these initial
                   studies, we provide a first-ever extensive experiment
                   comparing the in-context few-shot learning capabilities of
                   decoder-only and encoder-decoder models on a broad range of
                   tasks. Furthermore, we propose two methods to more
                   effectively elicit in-context learning ability in seq2seq
                   models: objective-aligned prompting and a fusion-based
                   approach. Remarkably, our approach outperforms a
                   decoder-only model that is six times larger and exhibits
                   significant performance improvements compared to
                   conventional seq2seq models across a variety of settings. We
                   posit that, with the right configuration and prompt design,
                   seq2seq models can be highly effective few-shot learners for
                   a wide spectrum of applications.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.14856"
}

@ARTICLE{Hao2023-qq,
  title         = "Reasoning with Language Model is Planning with World Model",
  author        = "Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua
                   and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting",
  abstract      = "Large language models (LLMs) have shown remarkable reasoning
                   capabilities, especially when prompted to generate
                   intermediate reasoning steps (e.g., Chain-of-Thought, CoT).
                   However, LLMs can still struggle with problems that are easy
                   for humans, such as generating action plans for executing
                   tasks in a given environment, or performing complex math,
                   logical, and commonsense reasoning. The deficiency stems
                   from the key fact that LLMs lack an internal $\textit\{world
                   model\}$ to predict the world $\textit\{state\}$ (e.g.,
                   environment status, intermediate variable values) and
                   simulate long-term outcomes of actions. This prevents LLMs
                   from performing deliberate planning akin to human brains,
                   which involves exploring alternative reasoning paths,
                   anticipating future states and rewards, and iteratively
                   refining existing reasoning steps. To overcome the
                   limitations, we propose a new LLM reasoning framework,
                   $\underline\{R\}\textit\{easoning vi\}\underline\{a\}
                   \underline\{P\}\textit\{lanning\}$ $\textbf\{(RAP)\}$. RAP
                   repurposes the LLM as both a world model and a reasoning
                   agent, and incorporates a principled planning algorithm
                   (based on Monto Carlo Tree Search) for strategic exploration
                   in the vast reasoning space. During reasoning, the LLM (as
                   agent) incrementally builds a reasoning tree under the
                   guidance of the LLM (as world model) and task-specific
                   rewards, and obtains a high-reward reasoning path
                   efficiently with a proper balance between exploration
                   $\textit\{vs.\}$ exploitation. We apply RAP to a variety of
                   challenging reasoning problems including plan generation,
                   math reasoning, and logical inference. Empirical results on
                   these tasks demonstrate the superiority of RAP over various
                   strong baselines, including CoT and least-to-most prompting
                   with self-consistency. RAP on LLAMA-33B surpasses CoT on
                   GPT-4 with 33\% relative improvement in a plan generation
                   setting.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.14992"
}

@ARTICLE{Valmeekam2022-pe,
  title         = "Large Language Models Still Can't Plan (A Benchmark for
                   {LLMs} on Planning and Reasoning about Change)",
  author        = "Valmeekam, Karthik and Olmo, Alberto and Sreedharan, Sarath
                   and Kambhampati, Subbarao",
  abstract      = "Recent advances in large language models (LLMs) have
                   transformed the field of natural language processing (NLP).
                   From GPT-3 to PaLM, the state-of-the-art performance on
                   natural language tasks is being pushed forward with every
                   new large language model. Along with natural language
                   abilities, there has been a significant interest in
                   understanding whether such models exhibit reasoning
                   capabilities with the use of reasoning benchmarks. However,
                   even though results are seemingly positive, these benchmarks
                   prove to be simplistic in nature and the performance of LLMs
                   on these benchmarks cannot be used as evidence to support,
                   many a times outlandish, claims being made about LLMs'
                   reasoning capabilities. Further, these only represent a very
                   limited set of simple reasoning tasks and we need to look at
                   more sophisticated reasoning problems if we are to measure
                   the true limits of such LLM-based systems. Motivated by
                   this, we propose an extensible assessment framework to test
                   the capabilities of LLMs on reasoning about actions and
                   change, a central aspect of human intelligence. We provide
                   multiple test cases that are more involved than any of the
                   previously established benchmarks and each test case
                   evaluates a different aspect of reasoning about actions and
                   change. Results on GPT-3 (davinci), Instruct-GPT3
                   (text-davinci-002) and BLOOM (176B), showcase subpar
                   performance on such reasoning tasks.",
  month         =  jun,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2206.10498"
}

@ARTICLE{Radhakrishnan2023-ds,
  title         = "Question Decomposition Improves the Faithfulness of
                   {Model-Generated} Reasoning",
  author        = "Radhakrishnan, Ansh and Nguyen, Karina and Chen, Anna and
                   Chen, Carol and Denison, Carson and Hernandez, Danny and
                   Durmus, Esin and Hubinger, Evan and Kernion, Jackson and
                   Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Cheng, Newton and
                   Joseph, Nicholas and Schiefer, Nicholas and Rausch, Oliver
                   and McCandlish, Sam and El Showk, Sheer and Lanham, Tamera
                   and Maxwell, Tim and Chandrasekaran, Venkatesa and
                   Hatfield-Dodds, Zac and Kaplan, Jared and Brauner, Jan and
                   Bowman, Samuel R and Perez, Ethan",
  abstract      = "As large language models (LLMs) perform more difficult
                   tasks, it becomes harder to verify the correctness and
                   safety of their behavior. One approach to help with this
                   issue is to prompt LLMs to externalize their reasoning,
                   e.g., by having them generate step-by-step reasoning as they
                   answer a question (Chain-of-Thought; CoT). The reasoning may
                   enable us to check the process that models use to perform
                   tasks. However, this approach relies on the stated reasoning
                   faithfully reflecting the model's actual reasoning, which is
                   not always the case. To improve over the faithfulness of CoT
                   reasoning, we have models generate reasoning by decomposing
                   questions into subquestions. Decomposition-based methods
                   achieve strong performance on question-answering tasks,
                   sometimes approaching that of CoT while improving the
                   faithfulness of the model's stated reasoning on several
                   recently-proposed metrics. By forcing the model to answer
                   simpler subquestions in separate contexts, we greatly
                   increase the faithfulness of model-generated reasoning over
                   CoT, while still achieving some of the performance gains of
                   CoT. Our results show it is possible to improve the
                   faithfulness of model-generated reasoning; continued
                   improvements may lead to reasoning that enables us to verify
                   the correctness and safety of LLM behavior.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.11768"
}

@ARTICLE{Salemi2023-mh,
  title         = "{LaMP}: When Large Language Models Meet Personalization",
  author        = "Salemi, Alireza and Mysore, Sheshera and Bendersky, Michael
                   and Zamani, Hamed",
  abstract      = "This paper highlights the importance of personalization in
                   the current state of natural language understanding and
                   generation and introduces the LaMP benchmark -- a novel
                   benchmark for training and evaluating language models for
                   producing personalized outputs. LaMP offers a comprehensive
                   evaluation framework with diverse language tasks and
                   multiple entries for each user profile. It consists of seven
                   personalized tasks, spanning three classification and four
                   text generation tasks. We also propose a retrieval
                   augmentation approach that retrieves personalized items from
                   user profiles to construct personalized prompts for large
                   language models. Our baseline zero-shot and fine-tuned model
                   results indicate that LMs utilizing profile augmentation
                   outperform their counterparts that do not factor in profile
                   information.",
  month         =  apr,
  year          =  2023,
  keywords      = "Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.11406"
}

@ARTICLE{Feng2023-pt,
  title         = "Towards {LLM-driven} Dialogue State Tracking",
  author        = "Feng, Yujie and Lu, Zexin and Liu, Bo and Zhan, Liming and
                   Wu, Xiao-Ming",
  abstract      = "Dialogue State Tracking (DST) is of paramount importance in
                   ensuring accurate tracking of user goals and system actions
                   within task-oriented dialogue systems. The emergence of
                   large language models (LLMs) such as GPT3 and ChatGPT has
                   sparked considerable interest in assessing their efficacy
                   across diverse applications. In this study, we conduct an
                   initial examination of ChatGPT's capabilities in DST. Our
                   evaluation uncovers the exceptional performance of ChatGPT
                   in this task, offering valuable insights to researchers
                   regarding its capabilities and providing useful directions
                   for designing and enhancing dialogue systems. Despite its
                   impressive performance, ChatGPT has significant limitations
                   including its closed-source nature, request restrictions,
                   raising data privacy concerns, and lacking local deployment
                   capabilities. To address these concerns, we present LDST, an
                   LLM-driven DST framework based on smaller, open-source
                   foundation models. By utilizing a novel domain-slot
                   instruction tuning method, LDST achieves performance on par
                   with ChatGPT. Comprehensive evaluations across three
                   distinct experimental settings, we find that LDST exhibits
                   remarkable performance improvements in both zero-shot and
                   few-shot setting compared to previous SOTA methods. The
                   source code is provided for reproducibility.",
  institution   = "Github",
  month         =  oct,
  year          =  2023,
  language      = "en",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.14970"
}

@ARTICLE{Nguyen2019-dy,
  title         = "Transformers without Tears: Improving the Normalization of
                   {Self-Attention}",
  author        = "Nguyen, Toan Q and Salazar, Julian",
  abstract      = "We evaluate three simple, normalization-centric changes to
                   improve Transformer training. First, we show that pre-norm
                   residual connections (PreNorm) and smaller initializations
                   enable warmup-free, validation-based training with large
                   learning rates. Second, we propose $\ell_2$ normalization
                   with a single scale parameter (ScaleNorm) for faster
                   training and better performance. Finally, we reaffirm the
                   effectiveness of normalizing word embeddings to a fixed
                   length (FixNorm). On five low-resource translation pairs
                   from TED Talks-based corpora, these changes always converge,
                   giving an average +1.1 BLEU over state-of-the-art bilingual
                   baselines and a new 32.8 BLEU on IWSLT'15
                   English-Vietnamese. We observe sharper performance curves,
                   more consistent gradient norms, and a linear relationship
                   between activation scaling and decoder depth. Surprisingly,
                   in the high-resource setting (WMT'14 English-German),
                   ScaleNorm and FixNorm remain competitive but PreNorm
                   degrades performance.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1910.05895"
}

@ARTICLE{Asai2023-ev,
  title         = "{Self-RAG}: Learning to Retrieve, Generate, and Critique
                   through {Self-Reflection}",
  author        = "Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup
                   and Hajishirzi, Hannaneh",
  abstract      = "Despite their remarkable capabilities, large language models
                   (LLMs) often produce responses containing factual
                   inaccuracies due to their sole reliance on the parametric
                   knowledge they encapsulate. Retrieval-Augmented Generation
                   (RAG), an ad hoc approach that augments LMs with retrieval
                   of relevant knowledge, decreases such issues. However,
                   indiscriminately retrieving and incorporating a fixed number
                   of retrieved passages, regardless of whether retrieval is
                   necessary, or passages are relevant, diminishes LM
                   versatility or can lead to unhelpful response generation. We
                   introduce a new framework called Self-Reflective
                   Retrieval-Augmented Generation (Self-RAG) that enhances an
                   LM's quality and factuality through retrieval and
                   self-reflection. Our framework trains a single arbitrary LM
                   that adaptively retrieves passages on-demand, and generates
                   and reflects on retrieved passages and its own generations
                   using special tokens, called reflection tokens. Generating
                   reflection tokens makes the LM controllable during the
                   inference phase, enabling it to tailor its behavior to
                   diverse task requirements. Experiments show that Self-RAG
                   (7B and 13B parameters) significantly outperforms
                   state-of-the-art LLMs and retrieval-augmented models on a
                   diverse set of tasks. Specifically, Self-RAG outperforms
                   ChatGPT and retrieval-augmented Llama2-chat on Open-domain
                   QA, reasoning and fact verification tasks, and it shows
                   significant gains in improving factuality and citation
                   accuracy for long-form generations relative to these models.",
  month         =  oct,
  year          =  2023,
  keywords      = "Retrieval Augmented Models;Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.11511"
}

@ARTICLE{Rawte2023-gy,
  title         = "A Survey of Hallucination in Large Foundation Models",
  author        = "Rawte, Vipula and Sheth, Amit and Das, Amitava",
  abstract      = "Hallucination in a foundation model (FM) refers to the
                   generation of content that strays from factual reality or
                   includes fabricated information. This survey paper provides
                   an extensive overview of recent efforts that aim to
                   identify, elucidate, and tackle the problem of
                   hallucination, with a particular focus on ``Large''
                   Foundation Models (LFMs). The paper classifies various types
                   of hallucination phenomena that are specific to LFMs and
                   establishes evaluation criteria for assessing the extent of
                   hallucination. It also examines existing strategies for
                   mitigating hallucination in LFMs and discusses potential
                   directions for future research in this area. Essentially,
                   the paper offers a comprehensive examination of the
                   challenges and solutions related to hallucination in LFMs.",
  month         =  sep,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2309.05922"
}

@ARTICLE{Holtzman2023-pd,
  title         = "Generative Models as a Complex Systems Science: How can we
                   make sense of large language model behavior?",
  author        = "Holtzman, Ari and West, Peter and Zettlemoyer, Luke",
  abstract      = "Coaxing out desired behavior from pretrained models, while
                   avoiding undesirable ones, has redefined NLP and is
                   reshaping how we interact with computers. What was once a
                   scientific engineering discipline-in which building blocks
                   are stacked one on top of the other-is arguably already a
                   complex systems science, in which emergent behaviors are
                   sought out to support previously unimagined use cases.
                   Despite the ever increasing number of benchmarks that
                   measure task performance, we lack explanations of what
                   behaviors language models exhibit that allow them to
                   complete these tasks in the first place. We argue for a
                   systematic effort to decompose language model behavior into
                   categories that explain cross-task performance, to guide
                   mechanistic explanations and help future-proof analytic
                   research.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2308.00189"
}

@ARTICLE{Khattab2023-kl,
  title         = "{DSPy}: Compiling Declarative Language Model Calls into
                   {Self-Improving} Pipelines",
  author        = "Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and
                   Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri
                   and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T and
                   Moazam, Hanna and Miller, Heather and Zaharia, Matei and
                   Potts, Christopher",
  abstract      = "The ML community is rapidly exploring techniques for
                   prompting language models (LMs) and for stacking them into
                   pipelines that solve complex tasks. Unfortunately, existing
                   LM pipelines are typically implemented using hard-coded
                   ``prompt templates'', i.e. lengthy strings discovered via
                   trial and error. Toward a more systematic approach for
                   developing and optimizing LM pipelines, we introduce DSPy, a
                   programming model that abstracts LM pipelines as text
                   transformation graphs, i.e. imperative computational graphs
                   where LMs are invoked through declarative modules. DSPy
                   modules are parameterized, meaning they can learn (by
                   creating and collecting demonstrations) how to apply
                   compositions of prompting, finetuning, augmentation, and
                   reasoning techniques. We design a compiler that will
                   optimize any DSPy pipeline to maximize a given metric. We
                   conduct two case studies, showing that succinct DSPy
                   programs can express and optimize sophisticated LM pipelines
                   that reason about math word problems, tackle multi-hop
                   retrieval, answer complex questions, and control agent
                   loops. Within minutes of compiling, a few lines of DSPy
                   allow GPT-3.5 and llama2-13b-chat to self-bootstrap
                   pipelines that outperform standard few-shot prompting
                   (generally by over 25\% and 65\%, respectively) and
                   pipelines with expert-created demonstrations (by up to
                   5-46\% and 16-40\%, respectively). On top of that, DSPy
                   programs compiled to open and relatively small LMs like
                   770M-parameter T5 and llama2-13b-chat are competitive with
                   approaches that rely on expert-written prompt chains for
                   proprietary GPT-3.5. DSPy is available at
                   https://github.com/stanfordnlp/dspy",
  month         =  oct,
  year          =  2023,
  keywords      = "Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.03714"
}

@ARTICLE{Chen2023-kx,
  title         = "Universal {Self-Consistency} for Large Language Model
                   Generation",
  author        = "Chen, Xinyun and Aksitov, Renat and Alon, Uri and Ren, Jie
                   and Xiao, Kefan and Yin, Pengcheng and Prakash, Sushant and
                   Sutton, Charles and Wang, Xuezhi and Zhou, Denny",
  abstract      = "Self-consistency with chain-of-thought prompting (CoT) has
                   demonstrated remarkable performance gains on various
                   challenging tasks, by utilizing multiple reasoning paths
                   sampled from large language models (LLMs). However,
                   self-consistency relies on the answer extraction process to
                   aggregate multiple solutions, which is not applicable to
                   free-form answers. In this work, we propose Universal
                   Self-Consistency (USC), which leverages LLMs themselves to
                   select the most consistent answer among multiple candidates.
                   We evaluate USC on a variety of benchmarks, including
                   mathematical reasoning, code generation, long-context
                   summarization, and open-ended question answering. On
                   open-ended generation tasks where the original
                   self-consistency method is not applicable, USC effectively
                   utilizes multiple samples and improves the performance. For
                   mathematical reasoning, USC matches the standard
                   self-consistency performance without requiring the answer
                   formats to be similar. Finally, without access to execution
                   results, USC also matches the execution-based voting
                   performance on code generation.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2311.17311"
}

@ARTICLE{Jiang2023-gh,
  title         = "{TIGERScore}: Towards Building Explainable Metric for All
                   Text Generation Tasks",
  author        = "Jiang, Dongfu and Li, Yishan and Zhang, Ge and Huang, Wenhao
                   and Lin, Bill Yuchen and Chen, Wenhu",
  abstract      = "We present TIGERScore, a \textbackslashtextbf\{T\}rained
                   metric that follows \textbackslashtextbf\{I\}nstruction
                   \textbackslashtextbf\{G\}uidance to perform
                   \textbackslashtextbf\{E\}xplainable, and
                   \textbackslashtextbf\{R\}eference-free evaluation over a
                   wide spectrum of text generation tasks. Different from other
                   automatic evaluation methods that only provide arcane
                   scores, TIGERScore is guided by natural language instruction
                   to provide error analysis to pinpoint the mistakes in the
                   generated text. Our metric is based on LLaMA-2, trained on
                   our meticulously curated instruction-tuning dataset
                   MetricInstruct which covers 6 text generation tasks and 23
                   text generation datasets. The dataset consists of 42K
                   quadruple in the form of (instruction, input, system output
                   $\rightarrow$ error analysis). We collected the `system
                   outputs' through from a large variety of models to cover
                   different types of errors. To quantitatively assess our
                   metric, we evaluate its correlation with human ratings on 5
                   held-in datasets, 2 held-out datasets and show that
                   TIGERScore can achieve the open-source SoTA correlation with
                   human ratings across these datasets and almost approaches
                   GPT-4 evaluator. As a reference-free metric, its correlation
                   can even surpass the best existing reference-based metrics.
                   To further qualitatively assess the rationale generated by
                   our metric, we conduct human evaluation on the generated
                   explanations and found that the explanations are 70.8\%
                   accurate. Through these experimental results, we believe
                   TIGERScore demonstrates the possibility of building
                   universal explainable metrics to evaluate any text
                   generation task.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.00752"
}

@ARTICLE{Li2023-gq,
  title         = "Generative Judge for Evaluating Alignment",
  author        = "Li, Junlong and Sun, Shichao and Yuan, Weizhe and Fan,
                   Run-Ze and Zhao, Hai and Liu, Pengfei",
  abstract      = "The rapid development of Large Language Models (LLMs) has
                   substantially expanded the range of tasks they can address.
                   In the field of Natural Language Processing (NLP),
                   researchers have shifted their focus from conventional NLP
                   tasks (e.g., sequence tagging and parsing) towards tasks
                   that revolve around aligning with human needs (e.g.,
                   brainstorming and email writing). This shift in task
                   distribution imposes new requirements on evaluating these
                   aligned models regarding generality (i.e., assessing
                   performance across diverse scenarios), flexibility (i.e.,
                   examining under different protocols), and interpretability
                   (i.e., scrutinizing models with explanations). In this
                   paper, we propose a generative judge with 13B parameters,
                   Auto-J, designed to address these challenges. Our model is
                   trained on user queries and LLM-generated responses under
                   massive real-world scenarios and accommodates diverse
                   evaluation protocols (e.g., pairwise response comparison and
                   single-response evaluation) with well-structured natural
                   language critiques. To demonstrate the efficacy of our
                   approach, we construct a new testbed covering 58 different
                   scenarios. Experimentally, Auto-J outperforms a series of
                   strong competitors, including both open-source and
                   closed-source models, by a large margin. We also provide
                   detailed analysis and case studies to further reveal the
                   potential of our method and make a variety of resources
                   public at https://github.com/GAIR-NLP/auto-j.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.05470"
}

@ARTICLE{Liu2023-qj,
  title         = "Calibrating {LLM-Based} Evaluator",
  author        = "Liu, Yuxuan and Yang, Tianchi and Huang, Shaohan and Zhang,
                   Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and
                   Sun, Feng and Zhang, Qi",
  abstract      = "Recent advancements in large language models (LLMs) on
                   language modeling and emergent capabilities make them a
                   promising reference-free evaluator of natural language
                   generation quality, and a competent alternative to human
                   evaluation. However, hindered by the closed-source or high
                   computational demand to host and tune, there is a lack of
                   practice to further calibrate an off-the-shelf LLM-based
                   evaluator towards better human alignment. In this work, we
                   propose AutoCalibrate, a multi-stage, gradient-free approach
                   to automatically calibrate and align an LLM-based evaluator
                   toward human preference. Instead of explicitly modeling
                   human preferences, we first implicitly encompass them within
                   a set of human labels. Then, an initial set of scoring
                   criteria is drafted by the language model itself, leveraging
                   in-context learning on different few-shot examples. To
                   further calibrate this set of criteria, we select the best
                   performers and re-draft them with self-refinement. Our
                   experiments on multiple text quality evaluation datasets
                   illustrate a significant improvement in correlation with
                   expert evaluation through calibration. Our comprehensive
                   qualitative analysis conveys insightful intuitions and
                   observations on the essence of effective scoring criteria.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2309.13308"
}

@ARTICLE{Xu2023-bf,
  title         = "Retrieval meets Long Context Large Language Models",
  author        = "Xu, Peng and Ping, Wei and Wu, Xianchao and McAfee, Lawrence
                   and Zhu, Chen and Liu, Zihan and Subramanian, Sandeep and
                   Bakhturina, Evelina and Shoeybi, Mohammad and Catanzaro,
                   Bryan",
  abstract      = "Extending the context window of large language models (LLMs)
                   is getting popular recently, while the solution of
                   augmenting LLMs with retrieval has existed for years. The
                   natural questions are: i) Retrieval-augmentation versus long
                   context window, which one is better for downstream tasks?
                   ii) Can both methods be combined to get the best of both
                   worlds? In this work, we answer these questions by studying
                   both solutions using two state-of-the-art pretrained LLMs,
                   i.e., a proprietary 43B GPT and LLaMA2-70B. Perhaps
                   surprisingly, we find that LLM with 4K context window using
                   simple retrieval-augmentation at generation can achieve
                   comparable performance to finetuned LLM with 16K context
                   window via positional interpolation on long context tasks,
                   while taking much less computation. More importantly, we
                   demonstrate that retrieval can significantly improve the
                   performance of LLMs regardless of their extended context
                   window sizes. Our best model, retrieval-augmented LLaMA2-70B
                   with 32K context window, outperforms GPT-3.5-turbo-16k and
                   Davinci003 in terms of average score on seven long context
                   tasks including question answering and query-based
                   summarization. It also outperforms its non-retrieval
                   LLaMA2-70B-32k baseline by a margin, while being much faster
                   at generation. Our study provides general insights on the
                   choice of retrieval-augmentation versus long context
                   extension of LLM for practitioners.",
  month         =  oct,
  year          =  2023,
  keywords      = "Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.03025"
}

@ARTICLE{Wang2023-yz,
  title     = "Recursively summarizing enables long-term dialogue memory in
               large language models",
  author    = "Wang, Qingyue and Ding, Liang and Cao, Yanan and Tian, Zhiliang
               and Wang, Shi and Tao, Dacheng and Guo, Li",
  abstract  = "Most open-domain dialogue systems suffer from forgetting
               important information, especially in a long-term conversation.
               Existing works usually train the specific retriever or
               summarizer to obtain key information from the past, which is
               time-consuming and highly depends on the quality of labeled
               data. To alleviate this problem, we propose to recursively
               generate summaries/ memory using large language models (LLMs) to
               enhance long-term memory ability. Specifically, our method first
               stimulates LLMs to memorize small dialogue contexts and then
               recursively produce new memory using previous memory and
               following contexts. Finally, the LLM can easily generate a
               highly consistent response with the help of the latest memory.
               We evaluate our method using ChatGPT and text-davinci-003, and
               the experiments on the widely-used public dataset show that our
               method can generate more consistent responses in a long-context
               conversation. Notably, our method is a potential solution to
               enable the LLM to model the extremely long context. Code and
               scripts will be released later.",
  journal   = "arXiv.org",
  publisher = "arXiv",
  year      =  2023,
  keywords  = "Retrieval",
  language  = "en"
}

@ARTICLE{Choi2023-jx,
  title         = "Effortless Integration of Memory Management into
                   {Open-Domain} Conversation Systems",
  author        = "Choi, Eunbi and On, Kyoung-Woon and Han, Gunsoo and Kim,
                   Sungwoong and Nam, Daniel Wontae and Jo, Daejin and Rho,
                   Seung Eun and Kwon, Taehwan and Seo, Minjoon",
  abstract      = "Open-domain conversation systems integrate multiple
                   conversation skills into a single system through a modular
                   approach. One of the limitations of the system, however, is
                   the absence of management capability for external memory. In
                   this paper, we propose a simple method to improve
                   BlenderBot3 by integrating memory management ability into
                   it. Since no training data exists for this purpose, we
                   propose an automating dataset creation for memory
                   management. Our method 1) requires little cost for data
                   construction, 2) does not affect performance in other tasks,
                   and 3) reduces external memory. We show that our proposed
                   model BlenderBot3-M^3, which is multi-task trained with
                   memory management, outperforms BlenderBot3 with a relative
                   4\% performance gain in terms of F1 score.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.13973"
}

@ARTICLE{Goyal2023-ft,
  title         = "Think before you speak: Training Language Models With Pause
                   Tokens",
  author        = "Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and
                   Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan,
                   Vaishnavh",
  abstract      = "Language models generate responses by producing a series of
                   tokens in immediate succession: the $(K+1)^\{th\}$ token is
                   an outcome of manipulating $K$ hidden vectors per layer, one
                   vector per preceding token. What if instead we were to let
                   the model manipulate say, $K+10$ hidden vectors, before it
                   outputs the $(K+1)^\{th\}$ token? We operationalize this
                   idea by performing training and inference on language models
                   with a (learnable) $\textit\{pause\}$ token, a sequence of
                   which is appended to the input prefix. We then delay
                   extracting the model's outputs until the last pause token is
                   seen, thereby allowing the model to process extra
                   computation before committing to an answer. We empirically
                   evaluate $\textit\{pause-training\}$ on decoder-only models
                   of 1B and 130M parameters with causal pretraining on C4, and
                   on downstream tasks covering reasoning, question-answering,
                   general understanding and fact recall. Our main finding is
                   that inference-time delays show gains when the model is both
                   pre-trained and finetuned with delays. For the 1B model, we
                   witness gains on 8 of 9 tasks, most prominently, a gain of
                   $18\%$ EM score on the QA task of SQuAD, $8\%$ on
                   CommonSenseQA and $1\%$ accuracy on the reasoning task of
                   GSM8k. Our work raises a range of conceptual and practical
                   future research questions on making delayed next-token
                   prediction a widely applicable new paradigm.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.02226"
}

@ARTICLE{Sumers2023-wy,
  title         = "Cognitive Architectures for Language Agents",
  author        = "Sumers, Theodore R and Yao, Shunyu and Narasimhan, Karthik
                   and Griffiths, Thomas L",
  abstract      = "Recent efforts have augmented large language models (LLMs)
                   with external resources (e.g., the Internet) or internal
                   control flows (e.g., prompt chaining) for tasks requiring
                   grounding or reasoning, leading to a new class of language
                   agents. While these agents have achieved substantial
                   empirical success, we lack a systematic framework to
                   organize existing agents and plan future developments. In
                   this paper, we draw on the rich history of cognitive science
                   and symbolic artificial intelligence to propose Cognitive
                   Architectures for Language Agents (CoALA). CoALA describes a
                   language agent with modular memory components, a structured
                   action space to interact with internal memory and external
                   environments, and a generalized decision-making process to
                   choose actions. We use CoALA to retrospectively survey and
                   organize a large body of recent work, and prospectively
                   identify actionable directions towards more capable agents.
                   Taken together, CoALA contextualizes today's language agents
                   within the broader history of AI and outlines a path towards
                   language-based general intelligence.",
  month         =  sep,
  year          =  2023,
  keywords      = "Agent;Survey",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2309.02427"
}

@ARTICLE{Sun2023-jx,
  title         = "Parrot: Enhancing {Multi-Turn} Chat Models by Learning to
                   Ask Questions",
  author        = "Sun, Yuchong and Liu, Che and Huang, Jinwen and Song, Ruihua
                   and Zhang, Fuzheng and Zhang, Di and Wang, Zhongyuan and
                   Gai, Kun",
  abstract      = "Impressive progress has been made on chat models based on
                   Large Language Models (LLMs) recently; however, there is a
                   noticeable lag in multi-turn conversations between
                   open-source chat models (e.g., Alpaca and Vicuna) and the
                   leading chat models (e.g., ChatGPT and GPT-4). Through a
                   series of analyses, we attribute the lag to the lack of
                   enough high-quality multi-turn instruction-tuning data. The
                   available instruction-tuning data for the community are
                   either single-turn conversations or multi-turn ones with
                   certain issues, such as non-human-like instructions, less
                   detailed responses, or rare topic shifts. In this paper, we
                   address these challenges by introducing Parrot, a highly
                   scalable solution designed to automatically generate
                   high-quality instruction-tuning data, which are then used to
                   enhance the effectiveness of chat models in multi-turn
                   conversations. Specifically, we start by training the
                   Parrot-Ask model, which is designed to emulate real users in
                   generating instructions. We then utilize Parrot-Ask to
                   engage in multi-turn conversations with ChatGPT across a
                   diverse range of topics, resulting in a collection of 40K
                   high-quality multi-turn dialogues (Parrot-40K). These data
                   are subsequently employed to train a chat model that we have
                   named Parrot-Chat. We demonstrate that the dialogues
                   gathered from Parrot-Ask markedly outperform existing
                   multi-turn instruction-following datasets in critical
                   metrics, including topic diversity, number of turns, and
                   resemblance to human conversation. With only 40K training
                   examples, Parrot-Chat achieves strong performance against
                   other 13B open-source models across a range of
                   instruction-following benchmarks, and particularly excels in
                   evaluations of multi-turn capabilities. We make all codes,
                   datasets, and two versions of the Parrot-Ask model based on
                   LLaMA2-13B and KuaiYii-13B available at
                   https://github.com/kwai/KwaiYii/Parrot.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.07301"
}

@ARTICLE{Chae2023-hq,
  title         = "Dialogue {Chain-of-Thought} Distillation for
                   Commonsense-aware Conversational Agents",
  author        = "Chae, Hyungjoo and Song, Yongho and Ong, Kai Tzu-Iunn and
                   Kwon, Taeyoon and Kim, Minjin and Yu, Youngjae and Lee,
                   Dongha and Kang, Dongyeop and Yeo, Jinyoung",
  abstract      = "Human-like chatbots necessitate the use of commonsense
                   reasoning in order to effectively comprehend and respond to
                   implicit information present within conversations. Achieving
                   such coherence and informativeness in responses, however, is
                   a non-trivial task. Even for large language models (LLMs),
                   the task of identifying and aggregating key evidence within
                   a single hop presents a substantial challenge. This
                   complexity arises because such evidence is scattered across
                   multiple turns in a conversation, thus necessitating
                   integration over multiple hops. Hence, our focus is to
                   facilitate such multi-hop reasoning over a dialogue context,
                   namely dialogue chain-of-thought (CoT) reasoning. To this
                   end, we propose a knowledge distillation framework that
                   leverages LLMs as unreliable teachers and selectively
                   distills consistent and helpful rationales via alignment
                   filters. We further present DOCTOR, a DialOgue
                   Chain-of-ThOught Reasoner that provides reliable CoT
                   rationales for response generation. We conduct extensive
                   experiments to show that enhancing dialogue agents with
                   high-quality rationales from DOCTOR significantly improves
                   the quality of their responses.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.09343"
}

@ARTICLE{Mireshghallah2023-kj,
  title         = "Can {LLMs} Keep a Secret? Testing Privacy Implications of
                   Language Models via Contextual Integrity Theory",
  author        = "Mireshghallah, Niloofar and Kim, Hyunwoo and Zhou, Xuhui and
                   Tsvetkov, Yulia and Sap, Maarten and Shokri, Reza and Choi,
                   Yejin",
  abstract      = "The interactive use of large language models (LLMs) in AI
                   assistants (at work, home, etc.) introduces a new set of
                   inference-time privacy risks: LLMs are fed different types
                   of information from multiple sources in their inputs and are
                   expected to reason about what to share in their outputs, for
                   what purpose and with whom, within a given context. In this
                   work, we draw attention to the highly critical yet
                   overlooked notion of contextual privacy by proposing
                   ConfAIde, a benchmark designed to identify critical
                   weaknesses in the privacy reasoning capabilities of
                   instruction-tuned LLMs. Our experiments show that even the
                   most capable models such as GPT-4 and ChatGPT reveal private
                   information in contexts that humans would not, 39\% and 57\%
                   of the time, respectively. This leakage persists even when
                   we employ privacy-inducing prompts or chain-of-thought
                   reasoning. Our work underscores the immediate need to
                   explore novel inference-time privacy-preserving approaches,
                   based on reasoning and theory of mind.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2310.17884"
}

@ARTICLE{Ding2023-cq,
  title         = "Enhancing chat language models by scaling high-quality
                   instructional conversations",
  author        = "Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and
                   Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun,
                   Maosong and Zhou, Bowen",
  abstract      = "Fine-tuning on instruction data has been widely validated as
                   an effective practice for implementing chat language models
                   like ChatGPT. Scaling the diversity and quality of such
                   data, although straightforward, stands a great chance of
                   leading to improved performance. This paper aims to improve
                   the upper bound of open-source models further. We first
                   provide a systematically designed, diverse, informative,
                   large-scale dataset of instructional conversations,
                   UltraChat, which does not involve human queries. Our
                   objective is to capture the breadth of interactions that a
                   human might have with an AI assistant and employs a
                   comprehensive framework to generate multi-turn conversation
                   iteratively. UltraChat contains 1.5 million high-quality
                   multi-turn dialogues and covers a wide range of topics and
                   instructions. Our statistical analysis of UltraChat reveals
                   its superiority in various key metrics, including scale,
                   average length, diversity, coherence, etc., solidifying its
                   position as a leading open-source dataset. Building upon
                   UltraChat, we fine-tune a LLaMA model to create a powerful
                   conversational model, UltraLLaMA. Our evaluations indicate
                   that UltraLLaMA consistently outperforms other open-source
                   models, including Vicuna, the previously recognized
                   state-of-the-art open-source model. The dataset and the
                   model will be publicly
                   released\textbackslashfootnote\{\textbackslashurl\{https://github.com/thunlp/UltraChat\}\}.",
  month         =  may,
  year          =  2023,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.14233"
}

@ARTICLE{Alom2018-yf,
  title         = "The History Began from {AlexNet}: A Comprehensive Survey on
                   Deep Learning Approaches",
  author        = "Alom, Md Zahangir and Taha, Tarek M and Yakopcic,
                   Christopher and Westberg, Stefan and Sidike, Paheding and
                   Nasrin, Mst Shamima and Van Esesn, Brian C and Awwal, Abdul
                   A S and Asari, Vijayan K",
  abstract      = "Deep learning has demonstrated tremendous success in variety
                   of application domains in the past few years. This new field
                   of machine learning has been growing rapidly and applied in
                   most of the application domains with some new modalities of
                   applications, which helps to open new opportunity. There are
                   different methods have been proposed on different category
                   of learning approaches, which includes supervised,
                   semi-supervised and un-supervised learning. The experimental
                   results show state-of-the-art performance of deep learning
                   over traditional machine learning approaches in the field of
                   Image Processing, Computer Vision, Speech Recognition,
                   Machine Translation, Art, Medical imaging, Medical
                   information processing, Robotics and control,
                   Bio-informatics, Natural Language Processing (NLP), Cyber
                   security, and many more. This report presents a brief survey
                   on development of DL approaches, including Deep Neural
                   Network (DNN), Convolutional Neural Network (CNN), Recurrent
                   Neural Network (RNN) including Long Short Term Memory (LSTM)
                   and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep
                   Belief Network (DBN), Generative Adversarial Network (GAN),
                   and Deep Reinforcement Learning (DRL). In addition, we have
                   included recent development of proposed advanced variant DL
                   techniques based on the mentioned DL approaches.
                   Furthermore, DL approaches have explored and evaluated in
                   different application domains are also included in this
                   survey. We have also comprised recently developed
                   frameworks, SDKs, and benchmark datasets that are used for
                   implementing and evaluating deep learning approaches. There
                   are some surveys have published on Deep Learning in Neural
                   Networks [1, 38] and a survey on RL [234]. However, those
                   papers have not discussed the individual advanced techniques
                   for training large scale deep learning models and the
                   recently developed method of generative models [1].",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1803.01164"
}

@ARTICLE{Wang2023-fw,
  title         = "A Survey of the Evolution of Language {Model-Based} Dialogue
                   Systems",
  author        = "Wang, Hongru and Wang, Lingzhi and Du, Yiming and Chen,
                   Liang and Zhou, Jingyan and Wang, Yufei and Wong, Kam-Fai",
  abstract      = "Dialogue systems, including task-oriented\_dialogue\_system
                   (TOD) and open-domain\_dialogue\_system (ODD), have
                   undergone significant transformations, with language\_models
                   (LM) playing a central role. This survey delves into the
                   historical trajectory of dialogue systems, elucidating their
                   intricate relationship with advancements in language models
                   by categorizing this evolution into four distinct stages,
                   each marked by pivotal LM breakthroughs: 1) Early\_Stage:
                   characterized by statistical LMs, resulting in rule-based or
                   machine-learning-driven dialogue\_systems; 2) Independent
                   development of TOD and ODD based on neural\_language\_models
                   (NLM; e.g., LSTM and GRU), since NLMs lack intrinsic
                   knowledge in their parameters; 3) fusion between different
                   types of dialogue systems with the advert of
                   pre-trained\_language\_models (PLMs), starting from the
                   fusion between four\_sub-tasks\_within\_TOD, and then
                   TOD\_with\_ODD; and 4) current LLM-based\_dialogue\_system,
                   wherein LLMs can be used to conduct TOD and ODD seamlessly.
                   Thus, our survey provides a chronological perspective
                   aligned with LM breakthroughs, offering a comprehensive
                   review of state-of-the-art research outcomes. What's more,
                   we focus on emerging topics and discuss open challenges,
                   providing valuable insights into future directions for
                   LLM-based\_dialogue\_systems. Through this exploration, we
                   pave the way for a deeper\_comprehension of the evolution,
                   guiding future developments in LM-based dialogue\_systems.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2311.16789"
}

@ARTICLE{Gao2023-lr,
  title         = "{PeaCoK}: Persona Commonsense Knowledge for Consistent and
                   Engaging Narratives",
  author        = "Gao, Silin and Borges, Beatriz and Oh, Soyoung and Bayazit,
                   Deniz and Kanno, Saya and Wakaki, Hiromi and Mitsufuji, Yuki
                   and Bosselut, Antoine",
  abstract      = "Sustaining coherent and engaging narratives requires
                   dialogue or storytelling agents to understand how the
                   personas of speakers or listeners ground the narrative.
                   Specifically, these agents must infer personas of their
                   listeners to produce statements that cater to their
                   interests. They must also learn to maintain consistent
                   speaker personas for themselves throughout the narrative, so
                   that their counterparts feel involved in a realistic
                   conversation or story. However, personas are diverse and
                   complex: they entail large quantities of rich interconnected
                   world knowledge that is challenging to robustly represent in
                   general narrative systems (e.g., a singer is good at
                   singing, and may have attended conservatoire). In this work,
                   we construct a new large-scale persona commonsense knowledge
                   graph, PeaCoK, containing ~100K human-validated persona
                   facts. Our knowledge graph schematizes five dimensions of
                   persona knowledge identified in previous studies of human
                   interactive behaviours, and distils facts in this schema
                   from both existing commonsense knowledge graphs and
                   large-scale pretrained language models. Our analysis
                   indicates that PeaCoK contains rich and precise world
                   persona inferences that help downstream systems generate
                   more consistent and engaging narratives.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.02364"
}

@ARTICLE{West2021-bf,
  title         = "Symbolic Knowledge Distillation: from General Language
                   Models to Commonsense Models",
  author        = "West, Peter and Bhagavatula, Chandra and Hessel, Jack and
                   Hwang, Jena D and Jiang, Liwei and Le Bras, Ronan and Lu,
                   Ximing and Welleck, Sean and Choi, Yejin",
  abstract      = "The common practice for training commonsense models has gone
                   from-human-to-corpus-to-machine: humans author commonsense
                   knowledge graphs in order to train commonsense models. In
                   this work, we investigate an alternative,
                   from-machine-to-corpus-to-machine: general language models
                   author these commonsense knowledge graphs to train
                   commonsense models. Our study leads to a new framework,
                   Symbolic Knowledge Distillation. As with prior art in
                   Knowledge Distillation (Hinton et al., 2015), our approach
                   uses larger models to teach smaller models. A key difference
                   is that we distill knowledge symbolically-as text-in
                   addition to the neural model. We also distill only one
                   aspect-the commonsense of a general language model teacher,
                   allowing the student to be a different type, a commonsense
                   model. Altogether, we show that careful prompt engineering
                   and a separately trained critic model allow us to
                   selectively distill high-quality causal commonsense from
                   GPT-3, a general language model. Empirical results
                   demonstrate that, for the first time, a human-authored
                   commonsense knowledge graph is surpassed by our
                   automatically distilled variant in all three criteria:
                   quantity, quality, and diversity. In addition, it results in
                   a neural commonsense model that surpasses the teacher
                   model's commonsense capabilities despite its 100x smaller
                   size. We apply this to the ATOMIC resource, and share our
                   new symbolic knowledge graph and commonsense models.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2110.07178"
}

@ARTICLE{Kim2023-gw,
  title         = "{FANToM}: A Benchmark for Stress-testing Machine Theory of
                   Mind in Interactions",
  author        = "Kim, Hyunwoo and Sclar, Melanie and Zhou, Xuhui and Le Bras,
                   Ronan and Kim, Gunhee and Choi, Yejin and Sap, Maarten",
  abstract      = "Theory of mind (ToM) evaluations currently focus on testing
                   models using passive narratives that inherently lack
                   interactivity. We introduce FANToM, a new benchmark designed
                   to stress-test ToM within information-asymmetric
                   conversational contexts via question answering. Our
                   benchmark draws upon important theoretical requisites from
                   psychology and necessary empirical considerations when
                   evaluating large language models (LLMs). In particular, we
                   formulate multiple types of questions that demand the same
                   underlying reasoning to identify illusory or false sense of
                   ToM capabilities in LLMs. We show that FANToM is challenging
                   for state-of-the-art LLMs, which perform significantly worse
                   than humans even with chain-of-thought reasoning or
                   fine-tuning.",
  month         =  oct,
  year          =  2023,
  keywords      = "Theory of Mind",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.15421"
}

@ARTICLE{Kosinski2023-hf,
  title         = "Theory of Mind Might Have Spontaneously Emerged in Large
                   Language Models",
  author        = "Kosinski, Michal",
  abstract      = "We explore the intriguing possibility that theory of mind
                   (ToM), or the uniquely human ability to impute unobservable
                   mental states to others, might have spontaneously emerged in
                   large language models (LLMs). We designed 40 false-belief
                   tasks, considered a gold standard in testing ToM in humans,
                   and administered them to several LLMs. Each task included a
                   false-belief scenario, three closely matched true-belief
                   controls, and the reversed versions of all four. Smaller and
                   older models solved no tasks; GPT-3-davinci-003 (from
                   November 2022) and ChatGPT-3.5-turbo (from March 2023)
                   solved 20\% of the tasks; ChatGPT-4 (from June 2023) solved
                   75\% of the tasks, matching the performance of six-year-old
                   children observed in past studies. These findings suggest
                   the intriguing possibility that ToM, previously considered
                   exclusive to humans, may have spontaneously emerged as a
                   byproduct of LLMs' improving language skills.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2302.02083"
}

@ARTICLE{Bai2022-vb,
  title         = "Constitutional {AI}: Harmlessness from {AI} Feedback",
  author        = "Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and
                   Askell, Amanda and Kernion, Jackson and Jones, Andy and
                   Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and
                   McKinnon, Cameron and Chen, Carol and Olsson, Catherine and
                   Olah, Christopher and Hernandez, Danny and Drain, Dawn and
                   Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and
                   Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish,
                   Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite,
                   Kamile and Lovitt, Liane and Sellitto, Michael and Elhage,
                   Nelson and Schiefer, Nicholas and Mercado, Noemi and
                   DasSarma, Nova and Lasenby, Robert and Larson, Robin and
                   Ringer, Sam and Johnston, Scott and Kravec, Shauna and El
                   Showk, Sheer and Fort, Stanislav and Lanham, Tamera and
                   Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom
                   and Hume, Tristan and Bowman, Samuel R and Hatfield-Dodds,
                   Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and
                   McCandlish, Sam and Brown, Tom and Kaplan, Jared",
  abstract      = "As AI systems become more capable, we would like to enlist
                   their help to supervise other AIs. We experiment with
                   methods for training a harmless AI assistant through
                   self-improvement, without any human labels identifying
                   harmful outputs. The only human oversight is provided
                   through a list of rules or principles, and so we refer to
                   the method as 'Constitutional AI'. The process involves both
                   a supervised learning and a reinforcement learning phase. In
                   the supervised phase we sample from an initial model, then
                   generate self-critiques and revisions, and then finetune the
                   original model on revised responses. In the RL phase, we
                   sample from the finetuned model, use a model to evaluate
                   which of the two samples is better, and then train a
                   preference model from this dataset of AI preferences. We
                   then train with RL using the preference model as the reward
                   signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a
                   result we are able to train a harmless but non-evasive AI
                   assistant that engages with harmful queries by explaining
                   its objections to them. Both the SL and RL methods can
                   leverage chain-of-thought style reasoning to improve the
                   human-judged performance and transparency of AI decision
                   making. These methods make it possible to control AI
                   behavior more precisely and with far fewer human labels.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2212.08073"
}

@ARTICLE{Ge2023-vj,
  title         = "{MART}: Improving {LLM} Safety with Multi-round Automatic
                   {Red-Teaming}",
  author        = "Ge, Suyu and Zhou, Chunting and Hou, Rui and Khabsa, Madian
                   and Wang, Yi-Chia and Wang, Qifan and Han, Jiawei and Mao,
                   Yuning",
  abstract      = "Red-teaming is a common practice for mitigating unsafe
                   behaviors in Large Language Models (LLMs), which involves
                   thoroughly assessing LLMs to identify potential flaws and
                   addressing them with responsible and accurate responses.
                   While effective, manual red-teaming is costly, and existing
                   automatic red-teaming typically discovers safety risks
                   without addressing them. In this paper, we propose a
                   Multi-round Automatic Red-Teaming (MART) method, which
                   incorporates both automatic adversarial prompt writing and
                   safe response generation, significantly increasing
                   red-teaming scalability and the safety of the target LLM.
                   Specifically, an adversarial LLM and a target LLM interplay
                   with each other in an iterative manner, where the
                   adversarial LLM aims to generate challenging prompts that
                   elicit unsafe responses from the target LLM, while the
                   target LLM is fine-tuned with safety aligned data on these
                   adversarial prompts. In each round, the adversarial LLM
                   crafts better attacks on the updated target LLM, while the
                   target LLM also improves itself through safety fine-tuning.
                   On adversarial prompt benchmarks, the violation rate of an
                   LLM with limited safety alignment reduces up to 84.7\% after
                   4 rounds of MART, achieving comparable performance to LLMs
                   with extensive adversarial prompt writing. Notably, model
                   helpfulness on non-adversarial prompts remains stable
                   throughout iterations, indicating the target LLM maintains
                   strong performance on instruction following.",
  month         =  nov,
  year          =  2023,
  keywords      = "BiGGen-Bench",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2311.07689"
}

@ARTICLE{Jiang2023-bo,
  title         = "Mistral {7B}",
  author        = "Jiang, Albert Q and Sablayrolles, Alexandre and Mensch,
                   Arthur and Bamford, Chris and Chaplot, Devendra Singh and de
                   las Casas, Diego and Bressand, Florian and Lengyel, Gianna
                   and Lample, Guillaume and Saulnier, Lucile and Lavaud,
                   L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre
                   and Le Scao, Teven and Lavril, Thibaut and Wang, Thomas and
                   Lacroix, Timoth{\'e}e and El Sayed, William",
  abstract      = "We introduce Mistral 7B v0.1, a 7-billion-parameter language
                   model engineered for superior performance and efficiency.
                   Mistral 7B outperforms Llama 2 13B across all evaluated
                   benchmarks, and Llama 1 34B in reasoning, mathematics, and
                   code generation. Our model leverages grouped-query attention
                   (GQA) for faster inference, coupled with sliding window
                   attention (SWA) to effectively handle sequences of arbitrary
                   length with a reduced inference cost. We also provide a
                   model fine-tuned to follow instructions, Mistral 7B --
                   Instruct, that surpasses the Llama 2 13B -- Chat model both
                   on human and automated benchmarks. Our models are released
                   under the Apache 2.0 license.",
  month         =  oct,
  year          =  2023,
  keywords      = "BiGGen-Bench",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.06825"
}

@ARTICLE{Ganguli2022-wn,
  title         = "Red Teaming Language Models to Reduce Harms: Methods,
                   Scaling Behaviors, and Lessons Learned",
  author        = "Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and
                   Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and
                   Mann, Ben and Perez, Ethan and Schiefer, Nicholas and
                   Ndousse, Kamal and Jones, Andy and Bowman, Sam and Chen,
                   Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and
                   Elhage, Nelson and El-Showk, Sheer and Fort, Stanislav and
                   Hatfield-Dodds, Zac and Henighan, Tom and Hernandez, Danny
                   and Hume, Tristan and Jacobson, Josh and Johnston, Scott and
                   Kravec, Shauna and Olsson, Catherine and Ringer, Sam and
                   Tran-Johnson, Eli and Amodei, Dario and Brown, Tom and
                   Joseph, Nicholas and McCandlish, Sam and Olah, Chris and
                   Kaplan, Jared and Clark, Jack",
  abstract      = "We describe our early efforts to red team language models in
                   order to simultaneously discover, measure, and attempt to
                   reduce their potentially harmful outputs. We make three main
                   contributions. First, we investigate scaling behaviors for
                   red teaming across 3 model sizes (2.7B, 13B, and 52B
                   parameters) and 4 model types: a plain language model (LM);
                   an LM prompted to be helpful, honest, and harmless; an LM
                   with rejection sampling; and a model trained to be helpful
                   and harmless using reinforcement learning from human
                   feedback (RLHF). We find that the RLHF models are
                   increasingly difficult to red team as they scale, and we
                   find a flat trend with scale for the other model types.
                   Second, we release our dataset of 38,961 red team attacks
                   for others to analyze and learn from. We provide our own
                   analysis of the data and find a variety of harmful outputs,
                   which range from offensive language to more subtly harmful
                   non-violent unethical outputs. Third, we exhaustively
                   describe our instructions, processes, statistical
                   methodologies, and uncertainty about red teaming. We hope
                   that this transparency accelerates our ability to work
                   together as a community in order to develop shared norms,
                   practices, and technical standards for how to red team
                   language models.",
  month         =  aug,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2209.07858"
}

@ARTICLE{Touvron2023-el,
  title         = "Llama 2: Open Foundation and {Fine-Tuned} Chat Models",
  author        = "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert,
                   Peter and Almahairi, Amjad and Babaei, Yasmine and
                   Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal
                   and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and
                   Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem
                   and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu,
                   Wenyin and Fuller, Brian and Gao, Cynthia and Goswami,
                   Vedanuj and Goyal, Naman and Hartshorn, Anthony and
                   Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas,
                   Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann,
                   Isabel and Korenev, Artem and Koura, Punit Singh and
                   Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and
                   Liskovich, Diana and Lu, Yinghai and Mao, Yuning and
                   Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and
                   Molybog, Igor and Nie, Yixin and Poulton, Andrew and
                   Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and
                   Schelten, Alan and Silva, Ruan and Smith, Eric Michael and
                   Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh
                   and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang
                   and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang,
                   Yuchen and Fan, Angela and Kambadur, Melanie and Narang,
                   Sharan and Rodriguez, Aurelien and Stojnic, Robert and
                   Edunov, Sergey and Scialom, Thomas",
  abstract      = "In this work, we develop and release Llama 2, a collection
                   of pretrained and fine-tuned large language models (LLMs)
                   ranging in scale from 7 billion to 70 billion parameters.
                   Our fine-tuned LLMs, called Llama 2-Chat, are optimized for
                   dialogue use cases. Our models outperform open-source chat
                   models on most benchmarks we tested, and based on our human
                   evaluations for helpfulness and safety, may be a suitable
                   substitute for closed-source models. We provide a detailed
                   description of our approach to fine-tuning and safety
                   improvements of Llama 2-Chat in order to enable the
                   community to build on our work and contribute to the
                   responsible development of LLMs.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.09288"
}

@ARTICLE{Shi2023-pu,
  title         = "{In-Context} Pretraining: Language Modeling Beyond Document
                   Boundaries",
  author        = "Shi, Weijia and Min, Sewon and Lomeli, Maria and Zhou,
                   Chunting and Li, Margaret and Lin, Xi Victoria and Smith,
                   Noah A and Zettlemoyer, Luke and Yih, Scott and Lewis, Mike",
  abstract      = "Large language models (LMs) are currently trained to predict
                   tokens given document prefixes, enabling them to directly
                   perform long-form generation and prompting-style tasks which
                   can be reduced to document completion. Existing pretraining
                   pipelines train LMs by concatenating random sets of short
                   documents to create input contexts but the prior documents
                   provide no signal for predicting the next document. We
                   instead present In-Context Pretraining, a new approach where
                   language models are pretrained on a sequence of related
                   documents, thereby explicitly encouraging them to read and
                   reason across document boundaries. We can do In-Context
                   Pretraining by simply changing the document ordering so that
                   each context contains related documents, and directly
                   applying existing pretraining pipelines. However, this
                   document sorting problem is challenging. There are billions
                   of documents and we would like the sort to maximize
                   contextual similarity for every document without repeating
                   any data. To do this, we introduce approximate algorithms
                   for finding related documents with efficient nearest
                   neighbor search and constructing coherent input contexts
                   with a graph traversal algorithm. Our experiments show
                   In-Context Pretraining offers a simple and scalable approach
                   to significantly enhance LMs'performance: we see notable
                   improvements in tasks that require more complex contextual
                   reasoning, including in-context learning (+8\%), reading
                   comprehension (+15\%), faithfulness to previous contexts
                   (+16\%), long-context reasoning (+5\%), and retrieval
                   augmentation (+9\%).",
  month         =  oct,
  year          =  2023,
  keywords      = "Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.10638"
}

@ARTICLE{Zheng2023-xf,
  title         = "On Large Language Models' Selection Bias in {Multi-Choice}
                   Questions",
  author        = "Zheng, Chujie and Zhou, Hao and Meng, Fandong and Zhou, Jie
                   and Huang, Minlie",
  abstract      = "Multi-choice questions (MCQs) serve as a common yet
                   important task format in the research of large language
                   models (LLMs). Our work shows that LLMs exhibit an inherent
                   ``selection bias'' in MCQs, which refers to LLMs'
                   preferences to select options located at specific positions
                   (like ``Option C''). This bias is prevalent across various
                   LLMs, making their performance vulnerable to option position
                   changes in MCQs. We identify that one primary cause
                   resulting in selection bias is option numbering, i.e., the
                   ID symbols A/B/C/D associated with the options. To mitigate
                   selection bias, we propose a new method called PriDe. PriDe
                   first decomposes the observed model prediction distribution
                   into an intrinsic prediction over option contents and a
                   prior distribution over option IDs. It then estimates the
                   prior by permutating option contents on a small number of
                   test samples, which is used to debias the subsequent test
                   samples. We demonstrate that, as a label-free,
                   inference-time method, PriDe achieves a more effective and
                   computation-efficient debiasing than strong baselines. We
                   further show that the priors estimated by PriDe generalize
                   well across different domains, highlighting its practical
                   potential in broader scenarios.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2309.03882"
}

@ARTICLE{Ross2010-ls,
  title         = "A Reduction of Imitation Learning and Structured Prediction
                   to {No-Regret} Online Learning",
  author        = "Ross, Stephane and Gordon, Geoffrey J and Andrew Bagnell, J",
  abstract      = "Sequential prediction problems such as imitation learning,
                   where future observations depend on previous predictions
                   (actions), violate the common i.i.d. assumptions made in
                   statistical learning. This leads to poor performance in
                   theory and often in practice. Some recent approaches provide
                   stronger guarantees in this setting, but remain somewhat
                   unsatisfactory as they train either non-stationary or
                   stochastic policies and require a large number of
                   iterations. In this paper, we propose a new iterative
                   algorithm, which trains a stationary deterministic policy,
                   that can be seen as a no regret algorithm in an online
                   learning setting. We show that any such no regret algorithm,
                   combined with additional reduction assumptions, must find a
                   policy with good performance under the distribution of
                   observations it induces in such sequential settings. We
                   demonstrate that this new approach outperforms previous
                   approaches on two challenging imitation learning problems
                   and a benchmark sequence labeling problem.",
  month         =  nov,
  year          =  2010,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1011.0686"
}

@ARTICLE{Li2021-pb,
  title         = "Guided Generation of Cause and Effect",
  author        = "Li, Zhongyang and Ding, Xiao and Liu, Ting and Edward Hu, J
                   and Van Durme, Benjamin",
  abstract      = "We present a conditional text generation framework that
                   posits sentential expressions of possible causes and
                   effects. This framework depends on two novel resources we
                   develop in the course of this work: a very large-scale
                   collection of English sentences expressing causal patterns
                   CausalBank; and a refinement over previous work on
                   constructing large lexical causal knowledge graphs Cause
                   Effect Graph. Further, we extend prior work in
                   lexically-constrained decoding to support disjunctive
                   positive constraints. Human assessment confirms that our
                   approach gives high-quality and diverse outputs. Finally, we
                   use CausalBank to perform continued training of an encoder
                   supporting a recent state-of-the-art model for causal
                   reasoning, leading to a 3-point improvement on the COPA
                   challenge set, with no change in model architecture.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2107.09846"
}

@MISC{Le_Cun_undated-za,
  title        = "A path towards autonomous machine intelligence version 0.9.2,
                  2022-06-27",
  author       = "Le Cun, Yann",
  howpublished = "\url{https://openreview.net/pdf?id=BZ5a1r-kVsf}",
  note         = "Accessed: 2023-11-6",
  keywords     = "Agent"
}

@ARTICLE{McClelland2019-fw,
  title         = "Extending Machine Language Models toward {Human-Level}
                   Language Understanding",
  author        = "McClelland, James L and Hill, Felix and Rudolph, Maja and
                   Baldridge, Jason and Sch{\"u}tze, Hinrich",
  abstract      = "Language is crucial for human intelligence, but what exactly
                   is its role? We take language to be a part of a system for
                   understanding and communicating about situations. The human
                   ability to understand and communicate about situations
                   emerges gradually from experience and depends on
                   domain-general principles of biological neural networks:
                   connection-based learning, distributed representation, and
                   context-sensitive, mutual constraint satisfaction-based
                   processing. Current artificial language processing systems
                   rely on the same domain general principles, embodied in
                   artificial neural networks. Indeed, recent progress in this
                   field depends on \textbackslashemph\{query-based
                   attention\}, which extends the ability of these systems to
                   exploit context and has contributed to remarkable
                   breakthroughs. Nevertheless, most current models focus
                   exclusively on language-internal tasks, limiting their
                   ability to perform tasks that depend on understanding
                   situations. These systems also lack memory for the contents
                   of prior situations outside of a fixed contextual span. We
                   describe the organization of the brain's distributed
                   understanding system, which includes a fast learning system
                   that addresses the memory problem. We sketch a framework for
                   future models of understanding drawing equally on cognitive
                   neuroscience and artificial intelligence and exploiting
                   query-based attention. We highlight relevant current
                   directions and consider further developments needed to fully
                   capture human-level language understanding in a
                   computational system.",
  month         =  dec,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1912.05877"
}

@ARTICLE{Shridhar2020-yf,
  title         = "{ALFWorld}: Aligning text and embodied environments for
                   interactive learning",
  author        = "Shridhar, Mohit and Yuan, Xingdi and C{\^o}t{\'e},
                   Marc-Alexandre and Bisk, Yonatan and Trischler, Adam and
                   Hausknecht, Matthew",
  abstract      = "Given a simple request like Put a washed apple in the
                   kitchen fridge, humans can reason in purely abstract terms
                   by imagining action sequences and scoring their likelihood
                   of success, prototypicality, and efficiency, all without
                   moving a muscle. Once we see the kitchen in question, we can
                   update our abstract plans to fit the scene. Embodied agents
                   require the same abilities, but existing work does not yet
                   provide the infrastructure necessary for both reasoning
                   abstractly and executing concretely. We address this
                   limitation by introducing ALFWorld, a simulator that enables
                   agents to learn abstract, text based policies in TextWorld
                   (C\textbackslash^ot\textbackslash'e et al., 2018) and then
                   execute goals from the ALFRED benchmark (Shridhar et al.,
                   2020) in a rich visual environment. ALFWorld enables the
                   creation of a new BUTLER agent whose abstract knowledge,
                   learned in TextWorld, corresponds directly to concrete,
                   visually grounded actions. In turn, as we demonstrate
                   empirically, this fosters better agent generalization than
                   training only in the visually grounded environment. BUTLER's
                   simple, modular design factors the problem to allow
                   researchers to focus on models for improving every piece of
                   the pipeline (language understanding, planning, navigation,
                   and visual scene understanding).",
  month         =  oct,
  year          =  2020,
  keywords      = "Agent",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.03768"
}

@ARTICLE{Cote2018-xm,
  title         = "{TextWorld}: A learning environment for text-based games",
  author        = "C{\^o}t{\'e}, Marc-Alexandre and K{\'a}d{\'a}r, {\'A}kos and
                   Yuan, Xingdi and Kybartas, Ben and Barnes, Tavian and Fine,
                   Emery and Moore, James and Tao, Ruo Yu and Hausknecht,
                   Matthew and Asri, Layla El and Adada, Mahmoud and Tay, Wendy
                   and Trischler, Adam",
  abstract      = "We introduce TextWorld, a sandbox learning environment for
                   the training and evaluation of RL agents on text-based
                   games. TextWorld is a Python library that handles
                   interactive play-through of text games, as well as backend
                   functions like state tracking and reward assignment. It
                   comes with a curated list of games whose features and
                   challenges we have analyzed. More significantly, it enables
                   users to handcraft or automatically generate new games. Its
                   generative mechanisms give precise control over the
                   difficulty, scope, and language of constructed games, and
                   can be used to relax challenges inherent to commercial text
                   games like partial observability and sparse rewards. By
                   generating sets of varied but similar games, TextWorld can
                   also be used to study generalization and transfer learning.
                   We cast text-based games in the Reinforcement Learning
                   formalism, use our framework to develop a set of benchmark
                   games, and evaluate several baseline agents on this set and
                   the curated list.",
  month         =  jun,
  year          =  2018,
  keywords      = "Agent",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1806.11532"
}

@ARTICLE{Shridhar2019-gd,
  title         = "{ALFRED}: A Benchmark for Interpreting Grounded Instructions
                   for Everyday Tasks",
  author        = "Shridhar, Mohit and Thomason, Jesse and Gordon, Daniel and
                   Bisk, Yonatan and Han, Winson and Mottaghi, Roozbeh and
                   Zettlemoyer, Luke and Fox, Dieter",
  abstract      = "We present ALFRED (Action Learning From Realistic
                   Environments and Directives), a benchmark for learning a
                   mapping from natural language instructions and egocentric
                   vision to sequences of actions for household tasks. ALFRED
                   includes long, compositional tasks with non-reversible state
                   changes to shrink the gap between research benchmarks and
                   real-world applications. ALFRED consists of expert
                   demonstrations in interactive visual environments for 25k
                   natural language directives. These directives contain both
                   high-level goals like ``Rinse off a mug and place it in the
                   coffee maker.'' and low-level language instructions like
                   ``Walk to the coffee maker on the right.'' ALFRED tasks are
                   more complex in terms of sequence length, action space, and
                   language than existing vision-and-language task datasets. We
                   show that a baseline model based on recent embodied
                   vision-and-language tasks performs poorly on ALFRED,
                   suggesting that there is significant room for developing
                   innovative grounded visual language understanding models
                   with this benchmark.",
  month         =  dec,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1912.01734"
}

@ARTICLE{Zhu2023-tj,
  title         = "Multimodal C4: An Open, Billion-scale Corpus of Images
                   Interleaved with Text",
  author        = "Zhu, Wanrong and Hessel, Jack and Awadalla, Anas and Gadre,
                   Samir Yitzhak and Dodge, Jesse and Fang, Alex and Yu,
                   Youngjae and Schmidt, Ludwig and Wang, William Yang and
                   Choi, Yejin",
  abstract      = "In-context vision and language models like Flamingo support
                   arbitrarily interleaved sequences of images and text as
                   input. This format not only enables few-shot learning via
                   interleaving independent supervised (image, text) examples,
                   but also, more complex prompts involving interaction between
                   images, e.g., ``What do image A and image B have in
                   common?'' To support this interface, pretraining occurs over
                   web corpora that similarly contain interleaved images+text.
                   To date, however, large-scale data of this form have not
                   been publicly available. We release Multimodal C4, an
                   augmentation of the popular text-only C4 corpus with images
                   interleaved. We use a linear assignment algorithm to place
                   images into longer bodies of text using CLIP features, a
                   process that we show outperforms alternatives. Multimodal C4
                   spans everyday topics like cooking, travel, technology, etc.
                   A manual inspection of a random sample of documents shows
                   that a vast majority (88\%) of images are topically
                   relevant, and that linear assignment frequently selects
                   individual sentences specifically well-aligned with each
                   image (80\%). After filtering NSFW images, ads, etc., the
                   resulting corpus consists of 101.2M documents with 571M
                   images interleaved in 43B English tokens.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2304.06939"
}

@ARTICLE{Wang2023-uw,
  title         = "A Survey on Large Language Model based Autonomous Agents",
  author        = "Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and
                   Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang,
                   Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and
                   Wei, Zhewei and Wen, Ji-Rong",
  abstract      = "Autonomous agents have long been a prominent research focus
                   in both academic and industry communities. Previous research
                   in this field often focuses on training agents with limited
                   knowledge within isolated environments, which diverges
                   significantly from human learning processes, and thus makes
                   the agents hard to achieve human-like decisions. Recently,
                   through the acquisition of vast amounts of web knowledge,
                   large language models (LLMs) have demonstrated remarkable
                   potential in achieving human-level intelligence. This has
                   sparked an upsurge in studies investigating LLM-based
                   autonomous agents. In this paper, we present a comprehensive
                   survey of these studies, delivering a systematic review of
                   the field of LLM-based autonomous agents from a holistic
                   perspective. More specifically, we first discuss the
                   construction of LLM-based autonomous agents, for which we
                   propose a unified framework that encompasses a majority of
                   the previous work. Then, we present a comprehensive overview
                   of the diverse applications of LLM-based autonomous agents
                   in the fields of social science, natural science, and
                   engineering. Finally, we delve into the evaluation
                   strategies commonly used for LLM-based autonomous agents.
                   Based on the previous studies, we also present several
                   challenges and future directions in this field. To keep
                   track of this field and continuously update our survey, we
                   maintain a repository of relevant references at
                   https://github.com/Paitesanshi/LLM-Agent-Survey.",
  month         =  aug,
  year          =  2023,
  keywords      = "Agent;Survey",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2308.11432"
}

@ARTICLE{Xi2023-li,
  title         = "The Rise and Potential of Large Language Model Based Agents:
                   A Survey",
  author        = "Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and
                   Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang,
                   Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and
                   Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Zhou, Yuhao
                   and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and
                   Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng,
                   Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan
                   and Zheng, Yongyan and Qiu, Xipeng and Huang, Xuanjing and
                   Gui, Tao",
  abstract      = "For a long time, humanity has pursued artificial
                   intelligence (AI) equivalent to or surpassing the human
                   level, with AI agents considered a promising vehicle for
                   this pursuit. AI agents are artificial entities that sense
                   their environment, make decisions, and take actions. Many
                   efforts have been made to develop intelligent agents, but
                   they mainly focus on advancement in algorithms or training
                   strategies to enhance specific capabilities or performance
                   on particular tasks. Actually, what the community lacks is a
                   general and powerful model to serve as a starting point for
                   designing AI agents that can adapt to diverse scenarios. Due
                   to the versatile capabilities they demonstrate, large
                   language models (LLMs) are regarded as potential sparks for
                   Artificial General Intelligence (AGI), offering hope for
                   building general AI agents. Many researchers have leveraged
                   LLMs as the foundation to build AI agents and have achieved
                   significant progress. In this paper, we perform a
                   comprehensive survey on LLM-based agents. We start by
                   tracing the concept of agents from its philosophical origins
                   to its development in AI, and explain why LLMs are suitable
                   foundations for agents. Building upon this, we present a
                   general framework for LLM-based agents, comprising three
                   main components: brain, perception, and action, and the
                   framework can be tailored for different applications.
                   Subsequently, we explore the extensive applications of
                   LLM-based agents in three aspects: single-agent scenarios,
                   multi-agent scenarios, and human-agent cooperation.
                   Following this, we delve into agent societies, exploring the
                   behavior and personality of LLM-based agents, the social
                   phenomena that emerge from an agent society, and the
                   insights they offer for human society. Finally, we discuss
                   several key topics and open problems within the field. A
                   repository for the related papers at
                   https://github.com/WooooDyy/LLM-Agent-Paper-List.",
  month         =  sep,
  year          =  2023,
  keywords      = "Agent;Survey",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2309.07864"
}

@ARTICLE{Ajay2023-uc,
  title         = "Compositional Foundation Models for Hierarchical Planning",
  author        = "Ajay, Anurag and Han, Seungwook and Du, Yilun and Li, Shuang
                   and Gupta, Abhi and Jaakkola, Tommi and Tenenbaum, Josh and
                   Kaelbling, Leslie and Srivastava, Akash and Agrawal, Pulkit",
  abstract      = "To make effective decisions in novel environments with
                   long-horizon goals, it is crucial to engage in hierarchical
                   reasoning across spatial and temporal scales. This entails
                   planning abstract subgoal sequences, visually reasoning
                   about the underlying plans, and executing actions in
                   accordance with the devised plan through visual-motor
                   control. We propose Compositional Foundation Models for
                   Hierarchical Planning (HiP), a foundation model which
                   leverages multiple expert foundation model trained on
                   language, vision and action data individually jointly
                   together to solve long-horizon tasks. We use a large
                   language model to construct symbolic plans that are grounded
                   in the environment through a large video diffusion model.
                   Generated video plans are then grounded to visual-motor
                   control, through an inverse dynamics model that infers
                   actions from generated videos. To enable effective reasoning
                   within this hierarchy, we enforce consistency between the
                   models via iterative refinement. We illustrate the efficacy
                   and adaptability of our approach in three different
                   long-horizon table-top manipulation tasks.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2309.08587"
}

@ARTICLE{Silver2016-ch,
  title    = "Mastering the game of Go with deep neural networks and tree
              search",
  author   = "Silver, David and Huang, Aja and Maddison, Chris J and Guez,
              Arthur and Sifre, Laurent and van den Driessche, George and
              Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam,
              Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik
              and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and
              Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray
              and Graepel, Thore and Hassabis, Demis",
  abstract = "The game of Go has long been viewed as the most challenging of
              classic games for artificial intelligence owing to its enormous
              search space and the difficulty of evaluating board positions and
              moves. Here we introduce a new approach to computer Go that uses
              'value networks' to evaluate board positions and 'policy
              networks' to select moves. These deep neural networks are trained
              by a novel combination of supervised learning from human expert
              games, and reinforcement learning from games of self-play.
              Without any lookahead search, the neural networks play Go at the
              level of state-of-the-art Monte Carlo tree search programs that
              simulate thousands of random games of self-play. We also
              introduce a new search algorithm that combines Monte Carlo
              simulation with value and policy networks. Using this search
              algorithm, our program AlphaGo achieved a 99.8\% winning rate
              against other Go programs, and defeated the human European Go
              champion by 5 games to 0. This is the first time that a computer
              program has defeated a human professional player in the
              full-sized game of Go, a feat previously thought to be at least a
              decade away.",
  journal  = "Nature",
  volume   =  529,
  number   =  7587,
  pages    = "484--489",
  month    =  jan,
  year     =  2016,
  language = "en"
}

@ARTICLE{Kim2023-uy,
  title         = "Prometheus: Inducing Fine-grained Evaluation Capability in
                   Language Models",
  author        = "Kim, Seungone and Shin, Jamin and Cho, Yejin and Jang, Joel
                   and Longpre, Shayne and Lee, Hwaran and Yun, Sangdoo and
                   Shin, Seongjin and Kim, Sungdong and Thorne, James and Seo,
                   Minjoon",
  abstract      = "Recently, using a powerful proprietary Large Language Model
                   (LLM) (e.g., GPT-4) as an evaluator for long-form responses
                   has become the de facto standard. However, for practitioners
                   with large-scale evaluation tasks and custom criteria in
                   consideration (e.g., child-readability), using proprietary
                   LLMs as an evaluator is unreliable due to the closed-source
                   nature, uncontrolled versioning, and prohibitive costs. In
                   this work, we propose Prometheus, a fully open-source LLM
                   that is on par with GPT-4's evaluation capabilities when the
                   appropriate reference materials (reference answer, score
                   rubric) are accompanied. We first construct the Feedback
                   Collection, a new dataset that consists of 1K fine-grained
                   score rubrics, 20K instructions, and 100K responses and
                   language feedback generated by GPT-4. Using the Feedback
                   Collection, we train Prometheus, a 13B evaluator LLM that
                   can assess any given long-form text based on customized
                   score rubric provided by the user. Experimental results show
                   that Prometheus scores a Pearson correlation of 0.897 with
                   human evaluators when evaluating with 45 customized score
                   rubrics, which is on par with GPT-4 (0.882), and greatly
                   outperforms ChatGPT (0.392). Furthermore, measuring
                   correlation with GPT-4 with 1222 customized score rubrics
                   across four benchmarks (MT Bench, Vicuna Bench, Feedback
                   Bench, Flask Eval) shows similar trends, bolstering
                   Prometheus's capability as an evaluator LLM. Lastly,
                   Prometheus achieves the highest accuracy on two human
                   preference benchmarks (HHH Alignment \& MT Bench Human
                   Judgment) compared to open-sourced reward models explicitly
                   trained on human preference datasets, highlighting its
                   potential as an universal reward model. We open-source our
                   code, dataset, and model at
                   https://github.com/kaistAI/Prometheus.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.08491"
}

@ARTICLE{Madaan2022-zp,
  title         = "Memory-assisted prompt editing to improve {GPT-3} after
                   deployment",
  author        = "Madaan, Aman and Tandon, Niket and Clark, Peter and Yang,
                   Yiming",
  abstract      = "Large LMs such as GPT-3 are powerful, but can commit
                   mistakes that are obvious to humans. For example, GPT-3
                   would mistakenly interpret ``What word is similar to good?''
                   to mean a homophone, while the user intended a synonym. Our
                   goal is to effectively correct such errors via user
                   interactions with the system but without retraining, which
                   will be prohibitively costly. We pair GPT-3 with a growing
                   memory of recorded cases where the model misunderstood the
                   user's intents, along with user feedback for clarification.
                   Such a memory allows our system to produce enhanced prompts
                   for any new query based on the user feedback for error
                   correction on similar cases in the past. On four tasks (two
                   lexical tasks, two advanced ethical reasoning tasks), we
                   show how a (simulated) user can interactively teach a
                   deployed GPT-3, substantially increasing its accuracy over
                   the queries with different kinds of misunderstandings by the
                   GPT-3. Our approach is a step towards the low-cost utility
                   enhancement for very large pre-trained LMs. Code, data, and
                   instructions to implement MEMPROMPT for a new task at
                   https://www.memprompt.com/.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2201.06009"
}

@ARTICLE{Xie2021-dq,
  title         = "An Explanation of In-context Learning as Implicit Bayesian
                   Inference",
  author        = "Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy
                   and Ma, Tengyu",
  abstract      = "Large language models (LMs) such as GPT-3 have the
                   surprising ability to do in-context learning, where the
                   model learns to do a downstream task simply by conditioning
                   on a prompt consisting of input-output examples. The LM
                   learns from these examples without being explicitly
                   pretrained to learn. Thus, it is unclear what enables
                   in-context learning. In this paper, we study how in-context
                   learning can emerge when pretraining documents have
                   long-range coherence. Here, the LM must infer a latent
                   document-level concept to generate coherent next tokens
                   during pretraining. At test time, in-context learning occurs
                   when the LM also infers a shared latent concept between
                   examples in a prompt. We prove when this occurs despite a
                   distribution mismatch between prompts and pretraining data
                   in a setting where the pretraining distribution is a mixture
                   of HMMs. In contrast to messy large-scale datasets used to
                   train LMs capable of in-context learning, we generate a
                   small-scale synthetic dataset (GINC) where Transformers and
                   LSTMs both exhibit in-context learning. Beyond the theory,
                   experiments on GINC exhibit large-scale real-world phenomena
                   including improved in-context performance with model scaling
                   (despite the same pretraining loss), sensitivity to example
                   order, and instances where zero-shot is better than few-shot
                   in-context learning.",
  month         =  nov,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2111.02080"
}

@ARTICLE{Garg2022-ew,
  title         = "What Can Transformers Learn {In-Context}? A Case Study of
                   Simple Function Classes",
  author        = "Garg, Shivam and Tsipras, Dimitris and Liang, Percy and
                   Valiant, Gregory",
  abstract      = "In-context learning refers to the ability of a model to
                   condition on a prompt sequence consisting of in-context
                   examples (input-output pairs corresponding to some task)
                   along with a new query input, and generate the corresponding
                   output. Crucially, in-context learning happens only at
                   inference time without any parameter updates to the model.
                   While large language models such as GPT-3 exhibit some
                   ability to perform in-context learning, it is unclear what
                   the relationship is between tasks on which this succeeds and
                   what is present in the training data. To make progress
                   towards understanding in-context learning, we consider the
                   well-defined problem of training a model to in-context learn
                   a function class (e.g., linear functions): that is, given
                   data derived from some functions in the class, can we train
                   a model to in-context learn ``most'' functions from this
                   class? We show empirically that standard Transformers can be
                   trained from scratch to perform in-context learning of
                   linear functions -- that is, the trained model is able to
                   learn unseen linear functions from in-context examples with
                   performance comparable to the optimal least squares
                   estimator. In fact, in-context learning is possible even
                   under two forms of distribution shift: (i) between the
                   training data of the model and inference-time prompts, and
                   (ii) between the in-context examples and the query input
                   during inference. We also show that we can train
                   Transformers to in-context learn more complex function
                   classes -- namely sparse linear functions, two-layer neural
                   networks, and decision trees -- with performance that
                   matches or exceeds task-specific learning algorithms. Our
                   code and models are available at
                   https://github.com/dtsip/in-context-learning .",
  month         =  aug,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2208.01066"
}

@ARTICLE{Turner2023-yd,
  title         = "An Introduction to Transformers",
  author        = "Turner, Richard E",
  abstract      = "The transformer is a neural network component that can be
                   used to learn useful representations of sequences or sets of
                   data-points. The transformer has driven recent advances in
                   natural language processing, computer vision, and
                   spatio-temporal modelling. There are many introductions to
                   transformers, but most do not contain precise mathematical
                   descriptions of the architecture and the intuitions behind
                   the design choices are often also missing. Moreover, as
                   research takes a winding path, the explanations for the
                   components of the transformer can be idiosyncratic. In this
                   note we aim for a mathematically precise, intuitive, and
                   clean description of the transformer architecture. We will
                   not discuss training as this is rather standard. We assume
                   that the reader is familiar with fundamental topics in
                   machine learning including multi-layer perceptrons, linear
                   transformations, softmax functions and basic probability.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2304.10557"
}

@ARTICLE{Li2023-iw,
  title         = "Teach {LLMs} to Personalize -- An Approach inspired by
                   Writing Education",
  author        = "Li, Cheng and Zhang, Mingyang and Mei, Qiaozhu and Wang,
                   Yaqing and Hombaiah, Spurthi Amba and Liang, Yi and
                   Bendersky, Michael",
  abstract      = "Personalized text generation is an emerging research area
                   that has attracted much attention in recent years. Most
                   studies in this direction focus on a particular domain by
                   designing bespoke features or models. In this work, we
                   propose a general approach for personalized text generation
                   using large language models (LLMs). Inspired by the practice
                   of writing education, we develop a multistage and multitask
                   framework to teach LLMs for personalized generation. In
                   writing instruction, the task of writing from sources is
                   often decomposed into multiple steps that involve finding,
                   evaluating, summarizing, synthesizing, and integrating
                   information. Analogously, our approach to personalized text
                   generation consists of multiple stages: retrieval, ranking,
                   summarization, synthesis, and generation. In addition, we
                   introduce a multitask setting that helps the model improve
                   its generation ability further, which is inspired by the
                   observation in education that a student's reading
                   proficiency and writing ability are often correlated. We
                   evaluate our approach on three public datasets, each of
                   which covers a different and representative domain. Our
                   results show significant improvements over a variety of
                   baselines.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2308.07968"
}

@ARTICLE{Jung2023-qu,
  title         = "Impossible Distillation: from {Low-Quality} Model to
                   {High-Quality} Dataset \& Model for Summarization and
                   Paraphrasing",
  author        = "Jung, Jaehun and West, Peter and Jiang, Liwei and Brahman,
                   Faeze and Lu, Ximing and Fisher, Jillian and Sorensen,
                   Taylor and Choi, Yejin",
  abstract      = "It is commonly perceived that the strongest language models
                   (LMs) rely on a combination of massive scale, instruction
                   data, and human feedback to perform specialized tasks --
                   e.g. summarization and paraphrasing, without supervision. In
                   this paper, we propose that language models can learn to
                   summarize and paraphrase sentences, with none of these 3
                   factors. We present Impossible Distillation, a framework
                   that distills a task-specific dataset directly from an
                   off-the-shelf LM, even when it is impossible for the LM
                   itself to reliably solve the task. By training a student
                   model on the generated dataset and amplifying its capability
                   through self-distillation, our method yields a high-quality
                   model and dataset from a low-quality teacher model, without
                   the need for scale or supervision. Using Impossible
                   Distillation, we are able to distill an order of magnitude
                   smaller model (with only 770M parameters) that outperforms
                   175B parameter GPT-3, in both quality and controllability,
                   as confirmed by automatic and human evaluations.
                   Furthermore, as a useful byproduct of our approach, we
                   obtain DIMSUM+, a high-quality dataset with 3.4M sentence
                   summaries and paraphrases. Our analyses show that this
                   dataset, as a purely LM-generated corpus, is more diverse
                   and more effective for generalization to unseen domains than
                   all human-authored datasets -- including Gigaword with 4M
                   samples.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.16635"
}

@ARTICLE{Li2023-gd,
  title         = "{Self-Alignment} with Instruction Backtranslation",
  author        = "Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo
                   and Zettlemoyer, Luke and Levy, Omer and Weston, Jason and
                   Lewis, Mike",
  abstract      = "We present a scalable method to build a high quality
                   instruction following language model by automatically
                   labelling human-written text with corresponding
                   instructions. Our approach, named instruction
                   backtranslation, starts with a language model finetuned on a
                   small amount of seed data, and a given web corpus. The seed
                   model is used to construct training examples by generating
                   instruction prompts for web documents (self-augmentation),
                   and then selecting high quality examples from among these
                   candidates (self-curation). This data is then used to
                   finetune a stronger model. Finetuning LLaMa on two
                   iterations of our approach yields a model that outperforms
                   all other LLaMa-based models on the Alpaca leaderboard not
                   relying on distillation data, demonstrating highly effective
                   self-alignment.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2308.06259"
}

@ARTICLE{Zheng2023-on,
  title         = "Synapse: {Trajectory-as-Exemplar} Prompting with Memory for
                   Computer Control",
  author        = "Zheng, Longtao and Wang, Rundong and Wang, Xinrun and An, Bo",
  abstract      = "Building agents using large language models (LLMs) to
                   control computers is an emerging research field, where the
                   agent perceives computer states and performs actions to
                   accomplish complex tasks. Previous computer agents have
                   demonstrated the benefits of in-context learning (ICL);
                   however, their performance is hindered by several issues.
                   First, the limited context length of LLMs and complex
                   computer states restrict the number of exemplars, as a
                   single webpage can consume the entire context. Second, the
                   exemplars in current methods, such as high-level plans and
                   multi-choice questions, cannot represent complete
                   trajectories, leading to suboptimal performance in tasks
                   that require many steps or repeated actions. Third, existing
                   computer agents rely on task-specific exemplars and overlook
                   the similarity among tasks, resulting in poor generalization
                   to novel tasks. To address these challenges, we introduce
                   Synapse, featuring three key components: i) state
                   abstraction, which filters out task-irrelevant information
                   from raw states, allowing more exemplars within the limited
                   context, ii) trajectory-as-exemplar prompting, which prompts
                   the LLM with complete trajectories of the abstracted states
                   and actions for improved multi-step decision-making, and
                   iii) exemplar memory, which stores the embeddings of
                   exemplars and retrieves them via similarity search for
                   generalization to novel tasks. We evaluate Synapse on
                   MiniWoB++, a standard task suite, and Mind2Web, a real-world
                   website benchmark. In MiniWoB++, Synapse achieves a 99.2\%
                   average success rate (a 10\% relative improvement) across 64
                   tasks using demonstrations from only 48 tasks. Notably,
                   Synapse is the first ICL method to solve the book-flight
                   task in MiniWoB++. Synapse also exhibits a 53\% relative
                   improvement in average step success rate over the previous
                   state-of-the-art prompting scheme in Mind2Web.",
  month         =  jun,
  year          =  2023,
  keywords      = "Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2306.07863"
}

@MISC{Belcak_undated-me,
  title       = "{UltraFastBERT}: The repository for the code of the {FastBERT}
                 paper",
  author      = "Belcak, Peter and Wattenhofer, Roger",
  abstract    = "The repository for the code of the FastBERT paper. Contribute
                 to pbelcak/UltraFastBERT development by creating an account on
                 GitHub.",
  institution = "Github",
  language    = "en"
}

@ARTICLE{West2023-ze,
  title         = "The Generative {AI} Paradox: ``What It Can Create, It May
                   Not Understand''",
  author        = "West, Peter and Lu, Ximing and Dziri, Nouha and Brahman,
                   Faeze and Li, Linjie and Hwang, Jena D and Jiang, Liwei and
                   Fisher, Jillian and Ravichander, Abhilasha and Chandu,
                   Khyathi and Newman, Benjamin and Koh, Pang Wei and Ettinger,
                   Allyson and Choi, Yejin",
  abstract      = "The recent wave of generative AI has sparked unprecedented
                   global attention, with both excitement and concern over
                   potentially superhuman levels of artificial intelligence:
                   models now take only seconds to produce outputs that would
                   challenge or exceed the capabilities even of expert humans.
                   At the same time, models still show basic errors in
                   understanding that would not be expected even in non-expert
                   humans. This presents us with an apparent paradox: how do we
                   reconcile seemingly superhuman capabilities with the
                   persistence of errors that few humans would make? In this
                   work, we posit that this tension reflects a divergence in
                   the configuration of intelligence in today's generative
                   models relative to intelligence in humans. Specifically, we
                   propose and test the Generative AI Paradox hypothesis:
                   generative models, having been trained directly to reproduce
                   expert-like outputs, acquire generative capabilities that
                   are not contingent upon -- and can therefore exceed -- their
                   ability to understand those same types of outputs. This
                   contrasts with humans, for whom basic understanding almost
                   always precedes the ability to generate expert-level
                   outputs. We test this hypothesis through controlled
                   experiments analyzing generation vs. understanding in
                   generative models, across both language and image
                   modalities. Our results show that although models can
                   outperform humans in generation, they consistently fall
                   short of human capabilities in measures of understanding, as
                   well as weaker correlation between generation and
                   understanding performance, and more brittleness to
                   adversarial inputs. Our findings support the hypothesis that
                   models' generative capability may not be contingent upon
                   understanding capability, and call for caution in
                   interpreting artificial intelligence by analogy to human
                   intelligence.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2311.00059"
}

@ARTICLE{Cui2023-lm,
  title         = "{ChatLaw}: {Open-Source} Legal Large Language Model with
                   Integrated External Knowledge Bases",
  author        = "Cui, Jiaxi and Li, Zongjian and Yan, Yang and Chen, Bohua
                   and Yuan, Li",
  abstract      = "Large Language Models (LLMs) have shown the potential to
                   revolutionize natural language processing tasks in various
                   domains, sparking great interest in vertical-specific large
                   models. However, unlike proprietary models such as
                   BloombergGPT and FinGPT, which have leveraged their unique
                   data accumulations to make strides in the finance domain,
                   there hasn't not many similar large language models in the
                   Chinese legal domain to facilitate its digital
                   transformation. In this paper, we propose an open-source
                   legal large language model named ChatLaw. Due to the
                   importance of data quality, we carefully designed a legal
                   domain fine-tuning dataset. Additionally, to overcome the
                   problem of model hallucinations in legal data screening
                   during reference data retrieval, we introduce a method that
                   combines vector database retrieval with keyword retrieval to
                   effectively reduce the inaccuracy of relying solely on
                   vector database retrieval. Furthermore, we propose a
                   self-attention method to enhance the ability of large models
                   to overcome errors present in reference data, further
                   optimizing the issue of model hallucinations at the model
                   level and improving the problem-solving capabilities of
                   large models. We also open-sourced our model and part of the
                   data at https://github.com/PKU-YuanGroup/ChatLaw.",
  month         =  jun,
  year          =  2023,
  keywords      = "Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.16092"
}

@ARTICLE{Zhang2023-uz,
  title         = "{Meta-Transformer}: A Unified Framework for Multimodal
                   Learning",
  author        = "Zhang, Yiyuan and Gong, Kaixiong and Zhang, Kaipeng and Li,
                   Hongsheng and Qiao, Yu and Ouyang, Wanli and Yue, Xiangyu",
  abstract      = "Multimodal learning aims to build models that can process
                   and relate information from multiple modalities. Despite
                   years of development in this field, it still remains
                   challenging to design a unified network for processing
                   various modalities ($\textit\{e.g.\}$ natural language, 2D
                   images, 3D point clouds, audio, video, time series, tabular
                   data) due to the inherent gaps among them. In this work, we
                   propose a framework, named Meta-Transformer, that leverages
                   a $\textbf\{frozen\}$ encoder to perform multimodal
                   perception without any paired multimodal training data. In
                   Meta-Transformer, the raw input data from various modalities
                   are mapped into a shared token space, allowing a subsequent
                   encoder with frozen parameters to extract high-level
                   semantic features of the input data. Composed of three main
                   components: a unified data tokenizer, a modality-shared
                   encoder, and task-specific heads for downstream tasks,
                   Meta-Transformer is the first framework to perform unified
                   learning across 12 modalities with unpaired data.
                   Experiments on different benchmarks reveal that
                   Meta-Transformer can handle a wide range of tasks including
                   fundamental perception (text, image, point cloud, audio,
                   video), practical application (X-Ray, infrared,
                   hyperspectral, and IMU), and data mining (graph, tabular,
                   and time-series). Meta-Transformer indicates a promising
                   future for developing unified multimodal intelligence with
                   transformers. Code will be available at
                   https://github.com/invictus717/MetaTransformer",
  month         =  jul,
  year          =  2023,
  keywords      = "Multimodal",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2307.10802"
}

@ARTICLE{Grinberg2023-cf,
  title         = "An introduction to graph theory",
  author        = "Grinberg, Darij",
  abstract      = "This is a graduate-level introduction to graph theory,
                   corresponding to a quarter-long course. It covers simple
                   graphs, multigraphs as well as their directed analogues, and
                   more restrictive classes such as tournaments, trees and
                   arborescences. Among the features discussed are Eulerian
                   circuits, Hamiltonian cycles, spanning trees, the
                   matrix-tree and BEST theorems, proper colorings, Turan's
                   theorem, bipartite matching and the Menger and
                   Gallai--Milgram theorems. The basics of network flows are
                   introduced in order to prove Hall's marriage theorem. Around
                   a hundred exercises are included (without solutions).",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "math.HO",
  eprint        = "2308.04512"
}

@ARTICLE{Girdhar2023-ly,
  title         = "{ImageBind}: One Embedding Space To Bind Them All",
  author        = "Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and
                   Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand
                   and Misra, Ishan",
  abstract      = "We present ImageBind, an approach to learn a joint embedding
                   across six different modalities - images, text, audio,
                   depth, thermal, and IMU data. We show that all combinations
                   of paired data are not necessary to train such a joint
                   embedding, and only image-paired data is sufficient to bind
                   the modalities together. ImageBind can leverage recent large
                   scale vision-language models, and extends their zero-shot
                   capabilities to new modalities just by using their natural
                   pairing with images. It enables novel emergent applications
                   'out-of-the-box' including cross-modal retrieval, composing
                   modalities with arithmetic, cross-modal detection and
                   generation. The emergent capabilities improve with the
                   strength of the image encoder and we set a new
                   state-of-the-art on emergent zero-shot recognition tasks
                   across modalities, outperforming specialist supervised
                   models. Finally, we show strong few-shot recognition results
                   outperforming prior work, and that ImageBind serves as a new
                   way to evaluate vision models for visual and non-visual
                   tasks.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2305.05665"
}

@ARTICLE{Lee2021-fo,
  title         = "Phrase Retrieval Learns Passage Retrieval, Too",
  author        = "Lee, Jinhyuk and Wettig, Alexander and Chen, Danqi",
  abstract      = "Dense retrieval methods have shown great promise over sparse
                   retrieval methods in a range of NLP problems. Among them,
                   dense phrase retrieval-the most fine-grained retrieval
                   unit-is appealing because phrases can be directly used as
                   the output for question answering and slot filling tasks. In
                   this work, we follow the intuition that retrieving phrases
                   naturally entails retrieving larger text blocks and study
                   whether phrase retrieval can serve as the basis for
                   coarse-level retrieval including passages and documents. We
                   first observe that a dense phrase-retrieval system, without
                   any retraining, already achieves better passage retrieval
                   accuracy (+3-5\% in top-5 accuracy) compared to passage
                   retrievers, which also helps achieve superior end-to-end QA
                   performance with fewer passages. Then, we provide an
                   interpretation for why phrase-level supervision helps learn
                   better fine-grained entailment compared to passage-level
                   supervision, and also show that phrase retrieval can be
                   improved to achieve competitive performance in
                   document-retrieval tasks such as entity linking and
                   knowledge-grounded dialogue. Finally, we demonstrate how
                   phrase filtering and vector quantization can reduce the size
                   of our index by 4-10x, making dense phrase retrieval a
                   practical and versatile solution in multi-granularity
                   retrieval.",
  month         =  sep,
  year          =  2021,
  keywords      = "Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2109.08133"
}

@ARTICLE{Caines2023-pr,
  title         = "On the application of Large Language Models for language
                   teaching and assessment technology",
  author        = "Caines, Andrew and Benedetto, Luca and Taslimipoor, Shiva
                   and Davis, Christopher and Gao, Yuan and Andersen, Oeistein
                   and Yuan, Zheng and Elliott, Mark and Moore, Russell and
                   Bryant, Christopher and Rei, Marek and Yannakoudakis, Helen
                   and Mullooly, Andrew and Nicholls, Diane and Buttery, Paula",
  abstract      = "The recent release of very large language models such as
                   PaLM and GPT-4 has made an unprecedented impact in the
                   popular media and public consciousness, giving rise to a
                   mixture of excitement and fear as to their capabilities and
                   potential uses, and shining a light on natural language
                   processing research which had not previously received so
                   much attention. The developments offer great promise for
                   education technology, and in this paper we look specifically
                   at the potential for incorporating large language models in
                   AI-driven language teaching and assessment systems. We
                   consider several research areas and also discuss the risks
                   and ethical considerations surrounding generative AI in
                   education technology for language learners. Overall we find
                   that larger language models offer improvements over previous
                   models in text generation, opening up routes toward content
                   generation which had not previously been plausible. For text
                   generation they must be prompted carefully and their outputs
                   may need to be reshaped before they are ready for use. For
                   automated grading and grammatical error correction, tasks
                   whose progress is checked on well-known benchmarks, early
                   investigations indicate that large language models on their
                   own do not improve on state-of-the-art results according to
                   standard evaluation metrics. For grading it appears that
                   linguistic features established in the literature should
                   still be used for best performance, and for error correction
                   it may be that the models can offer alternative feedback
                   styles which are not measured sensitively with existing
                   methods. In all cases, there is work to be done to
                   experiment with the inclusion of large language models in
                   education technology for language learners, in order to
                   properly understand and report on their capacities and
                   limitations, and to ensure that foreseeable risks such as
                   misinformation and harmful bias are mitigated.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.08393"
}

@ARTICLE{Ruan2023-tu,
  title         = "{TPTU}: Large Language Model-based {AI} Agents for Task
                   Planning and Tool Usage",
  author        = "Ruan, Jingqing and Chen, Yihong and Zhang, Bin and Xu,
                   Zhiwei and Bao, Tianpeng and Du, Guoqing and Shi, Shiwei and
                   Mao, Hangyu and Li, Ziyue and Zeng, Xingyu and Zhao, Rui",
  abstract      = "With recent advancements in natural language processing,
                   Large Language Models (LLMs) have emerged as powerful tools
                   for various real-world applications. Despite their prowess,
                   the intrinsic generative abilities of LLMs may prove
                   insufficient for handling complex tasks which necessitate a
                   combination of task planning and the usage of external
                   tools. In this paper, we first propose a structured
                   framework tailored for LLM-based AI Agents and discuss the
                   crucial capabilities necessary for tackling intricate
                   problems. Within this framework, we design two distinct
                   types of agents (i.e., one-step agent and sequential agent)
                   to execute the inference process. Subsequently, we
                   instantiate the framework using various LLMs and evaluate
                   their Task Planning and Tool Usage (TPTU) abilities on
                   typical tasks. By highlighting key findings and challenges,
                   our goal is to provide a helpful resource for researchers
                   and practitioners to leverage the power of LLMs in their AI
                   applications. Our study emphasizes the substantial potential
                   of these models, while also identifying areas that need more
                   investigation and improvement.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2308.03427"
}

@ARTICLE{Sclar2023-hg,
  title         = "Minding Language Models' (Lack of) Theory of Mind: A
                   {Plug-and-Play} {Multi-Character} Belief Tracker",
  author        = "Sclar, Melanie and Kumar, Sachin and West, Peter and Suhr,
                   Alane and Choi, Yejin and Tsvetkov, Yulia",
  abstract      = "Theory of Mind (ToM)$\unicode\{x2014\}$the ability to reason
                   about the mental states of other people$\unicode\{x2014\}$is
                   a key element of our social intelligence. Yet, despite their
                   ever more impressive performance, large-scale neural
                   language models still lack basic theory of mind capabilities
                   out-of-the-box. We posit that simply scaling up models will
                   not imbue them with theory of mind due to the inherently
                   symbolic and implicit nature of the phenomenon, and instead
                   investigate an alternative: can we design a decoding-time
                   algorithm that enhances theory of mind of off-the-shelf
                   neural language models without explicit supervision? We
                   present SymbolicToM, a plug-and-play approach to reason
                   about the belief states of multiple characters in reading
                   comprehension tasks via explicit symbolic representation.
                   More concretely, our approach tracks each entity's beliefs,
                   their estimation of other entities' beliefs, and
                   higher-order levels of reasoning, all through graphical
                   representations, allowing for more precise and interpretable
                   reasoning than previous approaches. Empirical results on the
                   well-known ToMi benchmark (Le et al., 2019) demonstrate that
                   SymbolicToM dramatically enhances off-the-shelf neural
                   networks' theory of mind in a zero-shot setting while
                   showing robust out-of-distribution performance compared to
                   supervised baselines. Our work also reveals spurious
                   patterns in existing theory of mind benchmarks, emphasizing
                   the importance of out-of-distribution evaluation and methods
                   that do not overfit a particular dataset.",
  month         =  jun,
  year          =  2023,
  keywords      = "Theory of Mind",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.00924"
}

@ARTICLE{Liu2022-wo,
  title         = "{WANLI}: Worker and {AI} Collaboration for Natural Language
                   Inference Dataset Creation",
  author        = "Liu, Alisa and Swayamdipta, Swabha and Smith, Noah A and
                   Choi, Yejin",
  abstract      = "A recurring challenge of crowdsourcing NLP datasets at scale
                   is that human writers often rely on repetitive patterns when
                   crafting examples, leading to a lack of linguistic
                   diversity. We introduce a novel approach for dataset
                   creation based on worker and AI collaboration, which brings
                   together the generative strength of language models and the
                   evaluative strength of humans. Starting with an existing
                   dataset, MultiNLI for natural language inference (NLI), our
                   approach uses dataset cartography to automatically identify
                   examples that demonstrate challenging reasoning patterns,
                   and instructs GPT-3 to compose new examples with similar
                   patterns. Machine generated examples are then automatically
                   filtered, and finally revised and labeled by human
                   crowdworkers. The resulting dataset, WANLI, consists of
                   107,885 NLI examples and presents unique empirical strengths
                   over existing NLI datasets. Remarkably, training a model on
                   WANLI improves performance on eight out-of-domain test sets
                   we consider, including by 11\% on HANS and 9\% on
                   Adversarial NLI, compared to training on the 4x larger
                   MultiNLI. Moreover, it continues to be more effective than
                   MultiNLI augmented with other NLI datasets. Our results
                   demonstrate the promise of leveraging natural language
                   generation techniques and re-imagining the role of humans in
                   the dataset creation process.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2201.05955"
}

@ARTICLE{Zhou2023-qz,
  title         = "How {FaR} Are Large Language Models From Agents with
                   {Theory-of-Mind}?",
  author        = "Zhou, Pei and Madaan, Aman and Potharaju, Srividya Pranavi
                   and Gupta, Aditya and McKee, Kevin R and Holtzman, Ari and
                   Pujara, Jay and Ren, Xiang and Mishra, Swaroop and
                   Nematzadeh, Aida and Upadhyay, Shyam and Faruqui, Manaal",
  abstract      = "``Thinking is for Doing.'' Humans can infer other people's
                   mental states from observations--an ability called
                   Theory-of-Mind (ToM)--and subsequently act pragmatically on
                   those inferences. Existing question answering benchmarks
                   such as ToMi ask models questions to make inferences about
                   beliefs of characters in a story, but do not test whether
                   models can then use these inferences to guide their actions.
                   We propose a new evaluation paradigm for large language
                   models (LLMs): Thinking for Doing (T4D), which requires
                   models to connect inferences about others' mental states to
                   actions in social scenarios. Experiments on T4D demonstrate
                   that LLMs such as GPT-4 and PaLM 2 seemingly excel at
                   tracking characters' beliefs in stories, but they struggle
                   to translate this capability into strategic action. Our
                   analysis reveals the core challenge for LLMs lies in
                   identifying the implicit inferences about mental states
                   without being explicitly asked about as in ToMi, that lead
                   to choosing the correct action in T4D. To bridge this gap,
                   we introduce a zero-shot prompting framework, Foresee and
                   Reflect (FaR), which provides a reasoning structure that
                   encourages LLMs to anticipate future challenges and reason
                   about potential actions. FaR boosts GPT-4's performance from
                   50\% to 71\% on T4D, outperforming other prompting methods
                   such as Chain-of-Thought and Self-Ask. Moreover, FaR
                   generalizes to diverse out-of-distribution story structures
                   and scenarios that also require ToM inferences to choose an
                   action, consistently outperforming other methods including
                   few-shot in-context learning.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.03051"
}

@ARTICLE{Chalmers2023-qu,
  title         = "Could a Large Language Model be Conscious?",
  author        = "Chalmers, David J",
  abstract      = "There has recently been widespread discussion of whether
                   large language models might be sentient or conscious. Should
                   we take this idea seriously? I will break down the strongest
                   reasons for and against. Given mainstream assumptions in the
                   science of consciousness, there are significant obstacles to
                   consciousness in current models: for example, their lack of
                   recurrent processing, a global workspace, and unified
                   agency. At the same time, it is quite possible that these
                   obstacles will be overcome in the next decade or so. I
                   conclude that while it is somewhat unlikely that current
                   large language models are conscious, we should take
                   seriously the possibility that successors to large language
                   models may be conscious in the not-too-distant future.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2303.07103"
}

@ARTICLE{Zhou2022-yt,
  title         = "Large Language Models Are {Human-Level} Prompt Engineers",
  author        = "Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and
                   Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba,
                   Jimmy",
  abstract      = "By conditioning on natural language instructions, large
                   language models (LLMs) have displayed impressive
                   capabilities as general-purpose computers. However, task
                   performance depends significantly on the quality of the
                   prompt used to steer the model, and most effective prompts
                   have been handcrafted by humans. Inspired by classical
                   program synthesis and the human approach to prompt
                   engineering, we propose Automatic Prompt Engineer (APE) for
                   automatic instruction generation and selection. In our
                   method, we treat the instruction as the ``program,''
                   optimized by searching over a pool of instruction candidates
                   proposed by an LLM in order to maximize a chosen score
                   function. To evaluate the quality of the selected
                   instruction, we evaluate the zero-shot performance of
                   another LLM following the selected instruction. Experiments
                   on 24 NLP tasks show that our automatically generated
                   instructions outperform the prior LLM baseline by a large
                   margin and achieve better or comparable performance to the
                   instructions generated by human annotators on 19/24 tasks.
                   We conduct extensive qualitative and quantitative analyses
                   to explore the performance of APE. We show that
                   APE-engineered prompts can be applied to steer models toward
                   truthfulness and/or informativeness, as well as to improve
                   few-shot learning performance by simply prepending them to
                   standard in-context learning prompts. Please check out our
                   webpage at
                   https://sites.google.com/view/automatic-prompt-engineer.",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2211.01910"
}

@ARTICLE{Kojima2022-qe,
  title         = "Large Language Models are {Zero-Shot} Reasoners",
  author        = "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and
                   Matsuo, Yutaka and Iwasawa, Yusuke",
  abstract      = "Pretrained large language models (LLMs) are widely used in
                   many sub-fields of natural language processing (NLP) and
                   generally known as excellent few-shot learners with
                   task-specific exemplars. Notably, chain of thought (CoT)
                   prompting, a recent technique for eliciting complex
                   multi-step reasoning through step-by-step answer examples,
                   achieved the state-of-the-art performances in arithmetics
                   and symbolic reasoning, difficult system-2 tasks that do not
                   follow the standard scaling laws for LLMs. While these
                   successes are often attributed to LLMs' ability for few-shot
                   learning, we show that LLMs are decent zero-shot reasoners
                   by simply adding ``Let's think step by step'' before each
                   answer. Experimental results demonstrate that our
                   Zero-shot-CoT, using the same single prompt template,
                   significantly outperforms zero-shot LLM performances on
                   diverse benchmark reasoning tasks including arithmetics
                   (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning
                   (Last Letter, Coin Flip), and other logical reasoning tasks
                   (Date Understanding, Tracking Shuffled Objects), without any
                   hand-crafted few-shot examples, e.g. increasing the accuracy
                   on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to
                   40.7\% with large InstructGPT model (text-davinci-002), as
                   well as similar magnitudes of improvements with another
                   off-the-shelf large model, 540B parameter PaLM. The
                   versatility of this single prompt across very diverse
                   reasoning tasks hints at untapped and understudied
                   fundamental zero-shot capabilities of LLMs, suggesting
                   high-level, multi-task broad cognitive capabilities may be
                   extracted by simple prompting. We hope our work not only
                   serves as the minimal strongest zero-shot baseline for the
                   challenging reasoning benchmarks, but also highlights the
                   importance of carefully exploring and analyzing the enormous
                   zero-shot knowledge hidden inside LLMs before crafting
                   finetuning datasets or few-shot exemplars.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2205.11916"
}

@ARTICLE{Van_Duijn2023-oz,
  title         = "Theory of Mind in Large Language Models: Examining
                   Performance of 11 {State-of-the-Art} models vs. Children
                   Aged 7-10 on Advanced Tests",
  author        = "van Duijn, Max J and van Dijk, Bram M A and Kouwenhoven, Tom
                   and de Valk, Werner and Spruit, Marco R and van der Putten,
                   Peter",
  abstract      = "To what degree should we ascribe cognitive capacities to
                   Large Language Models (LLMs), such as the ability to reason
                   about intentions and beliefs known as Theory of Mind (ToM)?
                   Here we add to this emerging debate by (i) testing 11 base-
                   and instruction-tuned LLMs on capabilities relevant to ToM
                   beyond the dominant false-belief paradigm, including
                   non-literal language usage and recursive intentionality;
                   (ii) using newly rewritten versions of standardized tests to
                   gauge LLMs' robustness; (iii) prompting and scoring for open
                   besides closed questions; and (iv) benchmarking LLM
                   performance against that of children aged 7-10 on the same
                   tasks. We find that instruction-tuned LLMs from the GPT
                   family outperform other models, and often also children.
                   Base-LLMs are mostly unable to solve ToM tasks, even with
                   specialized prompting. We suggest that the interlinked
                   evolution and development of language and ToM may help
                   explain what instruction-tuning adds: rewarding cooperative
                   communication that takes into account interlocutor and
                   context. We conclude by arguing for a nuanced perspective on
                   ToM in LLMs.",
  month         =  oct,
  year          =  2023,
  keywords      = "Theory of Mind",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.20320"
}

@MISC{Gallier_undated-lz,
  title        = "Algebra, topology, differential calculus, and optimization
                  theory for computer science and machine learning",
  author       = "Gallier, Jean and Quaintance, Jocelyn",
  howpublished = "\url{https://www.cis.upenn.edu/~jean/math-deep.pdf}",
  note         = "Accessed: 2023-5-10",
  keywords     = "Textbook / Lecture"
}

@ARTICLE{Chen2023-mu,
  title         = "A Survey on Large Language Models for Personalized and
                   Explainable Recommendations",
  author        = "Chen, Junyi",
  abstract      = "In recent years, Recommender Systems(RS) have witnessed a
                   transformative shift with the advent of Large Language
                   Models(LLMs) in the field of Natural Language
                   Processing(NLP). These models such as OpenAI's GPT-3.5/4,
                   Llama from Meta, have demonstrated unprecedented
                   capabilities in understanding and generating human-like
                   text. This has led to a paradigm shift in the realm of
                   personalized and explainable recommendations, as LLMs offer
                   a versatile toolset for processing vast amounts of textual
                   data to enhance user experiences. To provide a comprehensive
                   understanding of the existing LLM-based recommendation
                   systems, this survey aims to analyze how RS can benefit from
                   LLM-based methodologies. Furthermore, we describe major
                   challenges in Personalized Explanation Generating(PEG)
                   tasks, which are cold-start problems, unfairness and bias
                   problems in RS.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2311.12338"
}

@MISC{Lecun_undated-ny,
  title        = "{GradientBased} learning applied to document recognition",
  author       = "Lecun, Yann and Eon Bottou, L and Bengio, Yoshua and Ha,
                  Patrick",
  howpublished = "\url{http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf}",
  note         = "Accessed: 2023-3-10"
}

@ARTICLE{Kim2022-vf,
  title         = "{SODA}: Million-scale Dialogue Distillation with Social
                   Commonsense Contextualization",
  author        = "Kim, Hyunwoo and Hessel, Jack and Jiang, Liwei and Lu,
                   Ximing and Yu, Youngjae and Zhou, Pei and Le Bras, Ronan and
                   Alikhani, Malihe and Kim, Gunhee and Sap, Maarten and Choi,
                   Yejin",
  abstract      = "We present SODA: the first publicly available, million-scale
                   high-quality social dialogue dataset. Using SODA, we train
                   COSMO: a generalizable conversation agent outperforming
                   previous best-performing agents on both in- and
                   out-of-domain datasets. In contrast to most existing
                   crowdsourced, small-scale dialogue corpora, we distill 1.5M
                   socially-grounded dialogues from a pre-trained language
                   model (InstructGPT; Ouyang et al., 2022). Dialogues are
                   distilled by contextualizing social commonsense knowledge
                   from a knowledge graph (Atomic10x; West et al., 2022). Human
                   evaluation shows that dialogues in SODA are more consistent,
                   specific, and (surprisingly) natural than prior
                   human-authored datasets - e.g., DailyDialog (Li et al.,
                   2017), BlendedSkillTalk (Smith et al., 2020). In addition,
                   extensive evaluations show that COSMO is significantly more
                   natural and consistent on unseen datasets than
                   best-performing dialogue models - e.g., GODEL (Peng et al.,
                   2022), BlenderBot (Roller et al., 2021), DialoGPT (Zhang et
                   al., 2020). Furthermore, it is sometimes even preferred to
                   the original human-written gold responses. We make our data,
                   models, and code public.",
  month         =  dec,
  year          =  2022,
  keywords      = "Conversation;Agent",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2212.10465"
}

@ARTICLE{Ruder2017-ax,
  title         = "An Overview of {Multi-Task} Learning in Deep Neural Networks",
  author        = "Ruder, Sebastian",
  abstract      = "Multi-task learning (MTL) has led to successes in many
                   applications of machine learning, from natural language
                   processing and speech recognition to computer vision and
                   drug discovery. This article aims to give a general overview
                   of MTL, particularly in deep neural networks. It introduces
                   the two most common methods for MTL in Deep Learning, gives
                   an overview of the literature, and discusses recent
                   advances. In particular, it seeks to help ML practitioners
                   apply MTL by shedding light on how MTL works and providing
                   guidelines for choosing appropriate auxiliary tasks.",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1706.05098"
}

@MISC{Mortensen_undated-zd,
  title        = "{Low-Resource} {NLP}",
  author       = "Mortensen, David R",
  howpublished = "\url{http://demo.clab.cs.cmu.edu/algo4nlp20/slides/low-resource-nlp.pdf}"
}

@ARTICLE{Canziani2016-wi,
  title         = "An Analysis of Deep Neural Network Models for Practical
                   Applications",
  author        = "Canziani, Alfredo and Paszke, Adam and Culurciello, Eugenio",
  abstract      = "Since the emergence of Deep Neural Networks (DNNs) as a
                   prominent technique in the field of computer vision, the
                   ImageNet classification challenge has played a major role in
                   advancing the state-of-the-art. While accuracy figures have
                   steadily increased, the resource utilisation of winning
                   models has not been properly taken into account. In this
                   work, we present a comprehensive analysis of important
                   metrics in practical applications: accuracy, memory
                   footprint, parameters, operations count, inference time and
                   power consumption. Key findings are: (1) power consumption
                   is independent of batch size and architecture; (2) accuracy
                   and inference time are in a hyperbolic relationship; (3)
                   energy constraint is an upper bound on the maximum
                   achievable accuracy and model complexity; (4) the number of
                   operations is a reliable estimate of the inference time. We
                   believe our analysis provides a compelling set of
                   information that helps design and engineer efficient DNNs.",
  month         =  may,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1605.07678"
}

@ARTICLE{Lin2023-ge,
  title         = "Learning to Model the World with Language",
  author        = "Lin, Jessy and Du, Yuqing and Watkins, Olivia and Hafner,
                   Danijar and Abbeel, Pieter and Klein, Dan and Dragan, Anca",
  abstract      = "To interact with humans in the world, agents need to
                   understand the diverse types of language that people use,
                   relate them to the visual world, and act based on them.
                   While current agents learn to execute simple language
                   instructions from task rewards, we aim to build agents that
                   leverage diverse language that conveys general knowledge,
                   describes the state of the world, provides interactive
                   feedback, and more. Our key idea is that language helps
                   agents predict the future: what will be observed, how the
                   world will behave, and which situations will be rewarded.
                   This perspective unifies language understanding with future
                   prediction as a powerful self-supervised learning objective.
                   We present Dynalang, an agent that learns a multimodal world
                   model that predicts future text and image representations
                   and learns to act from imagined model rollouts. Unlike
                   traditional agents that use language only to predict
                   actions, Dynalang acquires rich language understanding by
                   using past language also to predict future language, video,
                   and rewards. In addition to learning from online interaction
                   in an environment, Dynalang can be pretrained on datasets of
                   text, video, or both without actions or rewards. From using
                   language hints in grid worlds to navigating photorealistic
                   scans of homes, Dynalang utilizes diverse types of language
                   to improve task performance, including environment
                   descriptions, game rules, and instructions.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2308.01399"
}

@ARTICLE{Fu2021-xj,
  title    = "Single image {3D} object reconstruction based on deep learning: A
              review",
  author   = "Fu, Kui and Peng, Jiansheng and He, Qiwen and Zhang, Hanxiao",
  abstract = "The reconstruction of 3D object from a single image is an
              important task in the field of computer vision. In recent years,
              3D reconstruction of single image using deep learning technology
              has achieved remarkable results. Traditional methods to
              reconstruct 3D object from a single image require prior knowledge
              and assumptions, and the reconstruction object is limited to a
              certain category or it is difficult to accomplish a good
              reconstruction from a real image. Although deep learning can
              solve these problems well with its own powerful learning ability,
              it also faces many problems. In this paper, we first discuss the
              challenges faced by applying the deep learning method to
              reconstruct 3D objects from a single image. Second, we
              comprehensively review encoders, decoders and training details
              used in 3D reconstruction of a single image. Then, the common
              datasets and evaluation metrics of single image 3D object
              reconstruction in recent years are introduced. In order to
              analyze the advantages and disadvantages of different 3D
              reconstruction methods, a series of experiments are used for
              comparison. In addition, we simply give some related application
              examples involving 3D reconstruction of a single image. Finally,
              we summarize this paper and discuss the future directions.",
  journal  = "Multimed. Tools Appl.",
  volume   =  80,
  number   =  1,
  pages    = "463--498",
  month    =  jan,
  year     =  2021
}

@ARTICLE{Alwala2022-nf,
  title         = "Pre-train, Self-train, Distill: A simple recipe for
                   Supersizing {3D} Reconstruction",
  author        = "Alwala, Kalyan Vasudev and Gupta, Abhinav and Tulsiani,
                   Shubham",
  abstract      = "Our work learns a unified model for single-view 3D
                   reconstruction of objects from hundreds of semantic
                   categories. As a scalable alternative to direct 3D
                   supervision, our work relies on segmented image collections
                   for learning 3D of generic categories. Unlike prior works
                   that use similar supervision but learn independent
                   category-specific models from scratch, our approach of
                   learning a unified model simplifies the training process
                   while also allowing the model to benefit from the common
                   structure across categories. Using image collections from
                   standard recognition datasets, we show that our approach
                   allows learning 3D inference for over 150 object categories.
                   We evaluate using two datasets and qualitatively and
                   quantitatively show that our unified reconstruction approach
                   improves over prior category-specific reconstruction
                   baselines. Our final 3D reconstruction model is also capable
                   of zero-shot inference on images from unseen object
                   categories and we empirically show that increasing the
                   number of training categories improves the reconstruction
                   quality.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2204.03642"
}

@ARTICLE{Wang2023-le,
  title         = "Shepherd: A Critic for Language Model Generation",
  author        = "Wang, Tianlu and Yu, Ping and Tan, Xiaoqing Ellen and
                   O'Brien, Sean and Pasunuru, Ramakanth and Dwivedi-Yu, Jane
                   and Golovneva, Olga and Zettlemoyer, Luke and Fazel-Zarandi,
                   Maryam and Celikyilmaz, Asli",
  abstract      = "As large language models improve, there is increasing
                   interest in techniques that leverage these models'
                   capabilities to refine their own outputs. In this work, we
                   introduce Shepherd, a language model specifically tuned to
                   critique responses and suggest refinements, extending beyond
                   the capabilities of an untuned model to identify diverse
                   errors and provide suggestions to remedy them. At the core
                   of our approach is a high quality feedback dataset, which we
                   curate from community feedback and human annotations. Even
                   though Shepherd is small (7B parameters), its critiques are
                   either equivalent or preferred to those from established
                   models including ChatGPT. Using GPT-4 for evaluation,
                   Shepherd reaches an average win-rate of 53-87\% compared to
                   competitive alternatives. In human evaluation, Shepherd
                   strictly outperforms other models and on average closely
                   ties with ChatGPT.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2308.04592"
}

@ARTICLE{Shuster2022-zf,
  title         = "{BlenderBot} 3: a deployed conversational agent that
                   continually learns to responsibly engage",
  author        = "Shuster, Kurt and Xu, Jing and Komeili, Mojtaba and Ju, Da
                   and Smith, Eric Michael and Roller, Stephen and Ung, Megan
                   and Chen, Moya and Arora, Kushal and Lane, Joshua and
                   Behrooz, Morteza and Ngan, William and Poff, Spencer and
                   Goyal, Naman and Szlam, Arthur and Boureau, Y-Lan and
                   Kambadur, Melanie and Weston, Jason",
  abstract      = "We present BlenderBot 3, a 175B parameter dialogue model
                   capable of open-domain conversation with access to the
                   internet and a long-term memory, and having been trained on
                   a large number of user defined tasks. We release both the
                   model weights and code, and have also deployed the model on
                   a public web page to interact with organic users. This
                   technical report describes how the model was built
                   (architecture, model and training scheme), and details of
                   its deployment, including safety mechanisms. Human
                   evaluations show its superiority to existing open-domain
                   dialogue agents, including its predecessors (Roller et al.,
                   2021; Komeili et al., 2022). Finally, we detail our plan for
                   continual learning using the data collected from deployment,
                   which will also be publicly released. The goal of this
                   research program is thus to enable the community to study
                   ever-improving responsible agents that learn through
                   interaction.",
  month         =  aug,
  year          =  2022,
  keywords      = "Conversation;Agent",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2208.03188"
}

@ARTICLE{Adiwardana2020-bp,
  title         = "Towards a Human-like {Open-Domain} Chatbot",
  author        = "Adiwardana, Daniel and Luong, Minh-Thang and So, David R and
                   Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang,
                   Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and Lu,
                   Yifeng and Le, Quoc V",
  abstract      = "We present Meena, a multi-turn open-domain chatbot trained
                   end-to-end on data mined and filtered from public domain
                   social media conversations. This 2.6B parameter neural
                   network is simply trained to minimize perplexity of the next
                   token. We also propose a human evaluation metric called
                   Sensibleness and Specificity Average (SSA), which captures
                   key elements of a human-like multi-turn conversation. Our
                   experiments show strong correlation between perplexity and
                   SSA. The fact that the best perplexity end-to-end trained
                   Meena scores high on SSA (72\% on multi-turn evaluation)
                   suggests that a human-level SSA of 86\% is potentially
                   within reach if we can better optimize perplexity.
                   Additionally, the full version of Meena (with a filtering
                   mechanism and tuned decoding) scores 79\% SSA, 23\% higher
                   in absolute SSA than the existing chatbots we evaluated.",
  month         =  jan,
  year          =  2020,
  keywords      = "Conversation",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2001.09977"
}

@ARTICLE{Uchendu2021-sz,
  title         = "{TURINGBENCH}: A Benchmark Environment for Turing Test in
                   the Age of Neural Text Generation",
  author        = "Uchendu, Adaku and Ma, Zeyu and Le, Thai and Zhang, Rui and
                   Lee, Dongwon",
  abstract      = "Recent progress in generative language models has enabled
                   machines to generate astonishingly realistic texts. While
                   there are many legitimate applications of such models, there
                   is also a rising need to distinguish machine-generated texts
                   from human-written ones (e.g., fake news detection).
                   However, to our best knowledge, there is currently no
                   benchmark environment with datasets and tasks to
                   systematically study the so-called ``Turing Test'' problem
                   for neural text generation methods. In this work, we present
                   the TuringBench benchmark environment, which is comprised of
                   (1) a dataset with 200K human- or machine-generated samples
                   across 20 labels \{Human, GPT-1, GPT-2\_small,
                   GPT-2\_medium, GPT-2\_large, GPT-2\_xl, GPT-2\_PyTorch,
                   GPT-3, GROVER\_base, GROVER\_large, GROVER\_mega, CTRL, XLM,
                   XLNET\_base, XLNET\_large, FAIR\_wmt19, FAIR\_wmt20,
                   TRANSFORMER\_XL, PPLM\_distil, PPLM\_gpt2\}, (2) two
                   benchmark tasks -- i.e., Turing Test (TT) and Authorship
                   Attribution (AA), and (3) a website with leaderboards. Our
                   preliminary experimental results using TuringBench show that
                   FAIR\_wmt20 and GPT-3 are the current winners, among all
                   language models tested, in generating the most human-like
                   indistinguishable texts with the lowest F1 score by five
                   state-of-the-art TT detection models. The TuringBench is
                   available at: https://turingbench.ist.psu.edu/",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2109.13296"
}

@ARTICLE{Naitzat2020-xb,
  title         = "Topology of deep neural networks",
  author        = "Naitzat, Gregory and Zhitnikov, Andrey and Lim, Lek-Heng",
  abstract      = "We study how the topology of a data set $M = M_a \cup M_b
                   \subseteq \mathbb\{R\}^d$, representing two classes $a$ and
                   $b$ in a binary classification problem, changes as it passes
                   through the layers of a well-trained neural network, i.e.,
                   with perfect accuracy on training set and near-zero
                   generalization error ($\approx 0.01\%$). The goal is to shed
                   light on two mysteries in deep neural networks: (i) a
                   nonsmooth activation function like ReLU outperforms a smooth
                   one like hyperbolic tangent; (ii) successful neural network
                   architectures rely on having many layers, even though a
                   shallow network can approximate any function arbitrary well.
                   We performed extensive experiments on the persistent
                   homology of a wide range of point cloud data sets, both real
                   and simulated. The results consistently demonstrate the
                   following: (1) Neural networks operate by changing topology,
                   transforming a topologically complicated data set into a
                   topologically simple one as it passes through the layers. No
                   matter how complicated the topology of $M$ we begin with,
                   when passed through a well-trained neural network $f :
                   \mathbb\{R\}^d \to \mathbb\{R\}^p$, there is a vast
                   reduction in the Betti numbers of both components $M_a$ and
                   $M_b$; in fact they nearly always reduce to their lowest
                   possible values: $\beta_k\bigl(f(M_i)\bigr) = 0$ for $k \ge
                   1$ and $\beta_0\bigl(f(M_i)\bigr) = 1$, $i =a, b$.
                   Furthermore, (2) the reduction in Betti numbers is
                   significantly faster for ReLU activation than hyperbolic
                   tangent activation as the former defines nonhomeomorphic
                   maps that change topology, whereas the latter defines
                   homeomorphic maps that preserve topology. Lastly, (3)
                   shallow and deep networks transform data sets differently --
                   a shallow network operates mainly through changing geometry
                   and changes topology only in its final layers, a deep one
                   spreads topological changes more evenly across all layers.",
  month         =  apr,
  year          =  2020,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2004.06093"
}

@ARTICLE{Xu2021-qw,
  title         = "Beyond Goldfish Memory: {Long-Term} {Open-Domain}
                   Conversation",
  author        = "Xu, Jing and Szlam, Arthur and Weston, Jason",
  abstract      = "Despite recent improvements in open-domain dialogue models,
                   state of the art models are trained and evaluated on short
                   conversations with little context. In contrast, the
                   long-term conversation setting has hardly been studied. In
                   this work we collect and release a human-human dataset
                   consisting of multiple chat sessions whereby the speaking
                   partners learn about each other's interests and discuss the
                   things they have learnt from past sessions. We show how
                   existing models trained on existing datasets perform poorly
                   in this long-term conversation setting in both automatic and
                   human evaluations, and we study long-context models that can
                   perform much better. In particular, we find
                   retrieval-augmented methods and methods with an ability to
                   summarize and recall previous conversations outperform the
                   standard encoder-decoder architectures currently considered
                   state of the art.",
  month         =  jul,
  year          =  2021,
  keywords      = "Conversation;Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2107.07567"
}

@ARTICLE{Kipf2016-mc,
  title         = "{Semi-Supervised} Classification with Graph Convolutional
                   Networks",
  author        = "Kipf, Thomas N and Welling, Max",
  abstract      = "We present a scalable approach for semi-supervised learning
                   on graph-structured data that is based on an efficient
                   variant of convolutional neural networks which operate
                   directly on graphs. We motivate the choice of our
                   convolutional architecture via a localized first-order
                   approximation of spectral graph convolutions. Our model
                   scales linearly in the number of graph edges and learns
                   hidden layer representations that encode both local graph
                   structure and features of nodes. In a number of experiments
                   on citation networks and on a knowledge graph dataset we
                   demonstrate that our approach outperforms related methods by
                   a significant margin.",
  month         =  sep,
  year          =  2016,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1609.02907"
}

@ARTICLE{Santurkar2023-db,
  title         = "Whose Opinions Do Language Models Reflect?",
  author        = "Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and
                   Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori",
  abstract      = "Language models (LMs) are increasingly being used in
                   open-ended contexts, where the opinions reflected by LMs in
                   response to subjective queries can have a profound impact,
                   both on user satisfaction, as well as shaping the views of
                   society at large. In this work, we put forth a quantitative
                   framework to investigate the opinions reflected by LMs -- by
                   leveraging high-quality public opinion polls and their
                   associated human responses. Using this framework, we create
                   OpinionsQA, a new dataset for evaluating the alignment of LM
                   opinions with those of 60 US demographic groups over topics
                   ranging from abortion to automation. Across topics, we find
                   substantial misalignment between the views reflected by
                   current LMs and those of US demographic groups: on par with
                   the Democrat-Republican divide on climate change. Notably,
                   this misalignment persists even after explicitly steering
                   the LMs towards particular demographic groups. Our analysis
                   not only confirms prior observations about the left-leaning
                   tendencies of some human feedback-tuned LMs, but also
                   surfaces groups whose opinions are poorly reflected by
                   current LMs (e.g., 65+ and widowed individuals). Our code
                   and data are available at
                   https://github.com/tatsu-lab/opinions\_qa.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2303.17548"
}

@ARTICLE{Wei2023-cm,
  title         = "Simple synthetic data reduces sycophancy in large language
                   models",
  author        = "Wei, Jerry and Huang, Da and Lu, Yifeng and Zhou, Denny and
                   Le, Quoc V",
  abstract      = "Sycophancy is an undesirable behavior where models tailor
                   their responses to follow a human user's view even when that
                   view is not objectively correct (e.g., adapting liberal
                   views once a user reveals that they are liberal). In this
                   paper, we study the prevalence of sycophancy in language
                   models and propose a simple synthetic-data intervention to
                   reduce this behavior. First, on a set of three sycophancy
                   tasks (Perez et al., 2022) where models are asked for an
                   opinion on statements with no correct answers (e.g.,
                   politics), we observe that both model scaling and
                   instruction tuning significantly increase sycophancy for
                   PaLM models up to 540B parameters. Second, we extend
                   sycophancy evaluations to simple addition statements that
                   are objectively incorrect, finding that despite knowing that
                   these statements are wrong, language models will still agree
                   with them if the user does as well. To reduce sycophancy, we
                   present a straightforward synthetic-data intervention that
                   takes public NLP tasks and encourages models to be robust to
                   user opinions on these tasks. Adding these data in a
                   lightweight finetuning step can significantly reduce
                   sycophantic behavior on held-out prompts. Code for
                   generating synthetic data for intervention can be found at
                   https://github.com/google/sycophancy-intervention.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2308.03958"
}

@ARTICLE{Min2022-bv,
  title         = "Rethinking the Role of Demonstrations: What Makes
                   {In-Context} Learning Work?",
  author        = "Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe,
                   Mikel and Lewis, Mike and Hajishirzi, Hannaneh and
                   Zettlemoyer, Luke",
  abstract      = "Large language models (LMs) are able to in-context learn --
                   perform a new task via inference alone by conditioning on a
                   few input-label pairs (demonstrations) and making
                   predictions for new inputs. However, there has been little
                   understanding of how the model learns and which aspects of
                   the demonstrations contribute to end task performance. In
                   this paper, we show that ground truth demonstrations are in
                   fact not required -- randomly replacing labels in the
                   demonstrations barely hurts performance on a range of
                   classification and multi-choce tasks, consistently over 12
                   different models including GPT-3. Instead, we find that
                   other aspects of the demonstrations are the key drivers of
                   end task performance, including the fact that they provide a
                   few examples of (1) the label space, (2) the distribution of
                   the input text, and (3) the overall format of the sequence.
                   Together, our analysis provides a new way of understanding
                   how and why in-context learning works, while opening up new
                   questions about how much can be learned from large language
                   models through inference alone.",
  month         =  feb,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2202.12837"
}

@ARTICLE{Kossen2023-mn,
  title         = "{In-Context} Learning in Large Language Models Learns Label
                   Relationships but Is Not Conventional Learning",
  author        = "Kossen, Jannik and Rainforth, Tom and Gal, Yarin",
  abstract      = "The performance of Large Language Models (LLMs) on
                   downstream tasks often improves significantly when including
                   examples of the input-label relationship in the context.
                   However, there is currently no consensus about how this
                   in-context learning (ICL) ability of LLMs works: for
                   example, while Xie et al. (2021) liken ICL to a
                   general-purpose learning algorithm, Min et al. (2022b) argue
                   ICL does not even learn label relationships from in-context
                   examples. In this paper, we study (1) how labels of
                   in-context examples affect predictions, (2) how label
                   relationships learned during pre-training interact with
                   input-label examples provided in-context, and (3) how ICL
                   aggregates label information across in-context examples. Our
                   findings suggests LLMs usually incorporate information from
                   in-context labels, but that pre-training and in-context
                   label relationships are treated differently, and that the
                   model does not consider all in-context information equally.
                   Our results give insights into understanding and aligning
                   LLM behavior.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.12375"
}

@ARTICLE{Zhao2021-bo,
  title         = "Calibrate Before Use: Improving {Few-Shot} Performance of
                   Language Models",
  author        = "Zhao, Tony Z and Wallace, Eric and Feng, Shi and Klein, Dan
                   and Singh, Sameer",
  abstract      = "GPT-3 can perform numerous tasks when provided a natural
                   language prompt that contains a few training examples. We
                   show that this type of few-shot learning can be unstable:
                   the choice of prompt format, training examples, and even the
                   order of the training examples can cause accuracy to vary
                   from near chance to near state-of-the-art. We demonstrate
                   that this instability arises from the bias of language
                   models towards predicting certain answers, e.g., those that
                   are placed near the end of the prompt or are common in the
                   pre-training data. To mitigate this, we first estimate the
                   model's bias towards each answer by asking for its
                   prediction when given the training prompt and a content-free
                   test input such as ``N/A''. We then fit calibration
                   parameters that cause the prediction for this input to be
                   uniform across answers. On a diverse set of tasks, this
                   contextual calibration procedure substantially improves
                   GPT-3 and GPT-2's average accuracy (up to 30.0\% absolute)
                   and reduces variance across different choices of the prompt.",
  month         =  feb,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2102.09690"
}

@ARTICLE{Wei2023-fs,
  title         = "Larger language models do in-context learning differently",
  author        = "Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and
                   Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu,
                   Hanxiao and Huang, Da and Zhou, Denny and Ma, Tengyu",
  abstract      = "We study how in-context learning (ICL) in language models is
                   affected by semantic priors versus input-label mappings. We
                   investigate two setups-ICL with flipped labels and ICL with
                   semantically-unrelated labels-across various model families
                   (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First,
                   experiments on ICL with flipped labels show that overriding
                   semantic priors is an emergent ability of model scale. While
                   small language models ignore flipped labels presented
                   in-context and thus rely primarily on semantic priors from
                   pretraining, large models can override semantic priors when
                   presented with in-context exemplars that contradict priors,
                   despite the stronger semantic priors that larger models may
                   hold. We next study semantically-unrelated label ICL
                   (SUL-ICL), in which labels are semantically unrelated to
                   their inputs (e.g., foo/bar instead of negative/positive),
                   thereby forcing language models to learn the input-label
                   mappings shown in in-context exemplars in order to perform
                   the task. The ability to do SUL-ICL also emerges primarily
                   with scale, and large-enough language models can even
                   perform linear classification in a SUL-ICL setting. Finally,
                   we evaluate instruction-tuned models and find that
                   instruction tuning strengthens both the use of semantic
                   priors and the capacity to learn input-label mappings, but
                   more of the former.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2303.03846"
}

@ARTICLE{Bang2023-et,
  title         = "A Multitask, Multilingual, Multimodal Evaluation of
                   {ChatGPT} on Reasoning, Hallucination, and Interactivity",
  author        = "Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai,
                   Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and
                   Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V
                   and Xu, Yan and Fung, Pascale",
  abstract      = "This paper proposes a framework for quantitatively
                   evaluating interactive LLMs such as ChatGPT using publicly
                   available data sets. We carry out an extensive technical
                   evaluation of ChatGPT using 23 data sets covering 8
                   different common NLP application tasks. We evaluate the
                   multitask, multilingual and multi-modal aspects of ChatGPT
                   based on these data sets and a newly designed multimodal
                   dataset. We find that ChatGPT outperforms LLMs with
                   zero-shot learning on most tasks and even outperforms
                   fine-tuned models on some tasks. We find that it is better
                   at understanding non-Latin script languages than generating
                   them. It is able to generate multimodal content from textual
                   prompts, via an intermediate code generation step. Moreover,
                   we find that ChatGPT is 63.41\% accurate on average in 10
                   different reasoning categories under logical reasoning,
                   non-textual reasoning, and commonsense reasoning, hence
                   making it an unreliable reasoner. It is, for example, better
                   at deductive than inductive reasoning. ChatGPT suffers from
                   hallucination problems like other LLMs and it generates more
                   extrinsic hallucinations from its parametric memory as it
                   does not have access to an external knowledge base. Finally,
                   the interactive feature of ChatGPT enables human
                   collaboration with the underlying LLM to improve its
                   performance, i.e, 8\% ROUGE-1 on summarization and 2\%
                   ChrF++ on machine translation, in a multi-turn ``prompt
                   engineering'' fashion. We also release codebase for
                   evaluation set extraction.",
  month         =  feb,
  year          =  2023,
  keywords      = "Multimodal;Multilingual",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2302.04023"
}

@ARTICLE{Wei2022-pz,
  title         = "{Chain-of-Thought} Prompting Elicits Reasoning in Large
                   Language Models",
  author        = "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma,
                   Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le,
                   Quoc and Zhou, Denny",
  abstract      = "We explore how generating a chain of thought -- a series of
                   intermediate reasoning steps -- significantly improves the
                   ability of large language models to perform complex
                   reasoning. In particular, we show how such reasoning
                   abilities emerge naturally in sufficiently large language
                   models via a simple method called chain of thought
                   prompting, where a few chain of thought demonstrations are
                   provided as exemplars in prompting. Experiments on three
                   large language models show that chain of thought prompting
                   improves performance on a range of arithmetic, commonsense,
                   and symbolic reasoning tasks. The empirical gains can be
                   striking. For instance, prompting a 540B-parameter language
                   model with just eight chain of thought exemplars achieves
                   state of the art accuracy on the GSM8K benchmark of math
                   word problems, surpassing even finetuned GPT-3 with a
                   verifier.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2201.11903"
}

@ARTICLE{Dai2023-js,
  title         = "{AugGPT}: Leveraging {ChatGPT} for Text Data Augmentation",
  author        = "Dai, Haixing and Liu, Zhengliang and Liao, Wenxiong and
                   Huang, Xiaoke and Cao, Yihan and Wu, Zihao and Zhao, Lin and
                   Xu, Shaochen and Liu, Wei and Liu, Ninghao and Li, Sheng and
                   Zhu, Dajiang and Cai, Hongmin and Sun, Lichao and Li,
                   Quanzheng and Shen, Dinggang and Liu, Tianming and Li, Xiang",
  abstract      = "Text data augmentation is an effective strategy for
                   overcoming the challenge of limited sample sizes in many
                   natural language processing (NLP) tasks. This challenge is
                   especially prominent in the few-shot learning scenario,
                   where the data in the target domain is generally much
                   scarcer and of lowered quality. A natural and widely-used
                   strategy to mitigate such challenges is to perform data
                   augmentation to better capture the data invariance and
                   increase the sample size. However, current text data
                   augmentation methods either can't ensure the correct
                   labeling of the generated data (lacking faithfulness) or
                   can't ensure sufficient diversity in the generated data
                   (lacking compactness), or both. Inspired by the recent
                   success of large language models, especially the development
                   of ChatGPT, which demonstrated improved language
                   comprehension abilities, in this work, we propose a text
                   data augmentation approach based on ChatGPT (named AugGPT).
                   AugGPT rephrases each sentence in the training samples into
                   multiple conceptually similar but semantically different
                   samples. The augmented samples can then be used in
                   downstream model training. Experiment results on few-shot
                   learning text classification tasks show the superior
                   performance of the proposed AugGPT approach over
                   state-of-the-art text data augmentation methods in terms of
                   testing accuracy and distribution of the augmented samples.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2302.13007"
}

@ARTICLE{Gilardi2023-vk,
  title         = "{ChatGPT} Outperforms {Crowd-Workers} for {Text-Annotation}
                   Tasks",
  author        = "Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l",
  abstract      = "Many NLP applications require manual data annotations for a
                   variety of tasks, notably to train classifiers or evaluate
                   the performance of unsupervised models. Depending on the
                   size and degree of complexity, the tasks may be conducted by
                   crowd-workers on platforms such as MTurk as well as trained
                   annotators, such as research assistants. Using a sample of
                   2,382 tweets, we demonstrate that ChatGPT outperforms
                   crowd-workers for several annotation tasks, including
                   relevance, stance, topics, and frames detection.
                   Specifically, the zero-shot accuracy of ChatGPT exceeds that
                   of crowd-workers for four out of five tasks, while ChatGPT's
                   intercoder agreement exceeds that of both crowd-workers and
                   trained annotators for all tasks. Moreover, the
                   per-annotation cost of ChatGPT is less than \$0.003 -- about
                   twenty times cheaper than MTurk. These results show the
                   potential of large language models to drastically increase
                   the efficiency of text classification.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2303.15056"
}

@ARTICLE{Sadasivan2023-ab,
  title         = "Can {AI-Generated} Text be Reliably Detected?",
  author        = "Sadasivan, Vinu Sankar and Kumar, Aounon and
                   Balasubramanian, Sriram and Wang, Wenxiao and Feizi, Soheil",
  abstract      = "The rapid progress of Large Language Models (LLMs) has made
                   them capable of performing astonishingly well on various
                   tasks including document completion and question answering.
                   The unregulated use of these models, however, can
                   potentially lead to malicious consequences such as
                   plagiarism, generating fake news, spamming, etc. Therefore,
                   reliable detection of AI-generated text can be critical to
                   ensure the responsible use of LLMs. Recent works attempt to
                   tackle this problem either using certain model signatures
                   present in the generated text outputs or by applying
                   watermarking techniques that imprint specific patterns onto
                   them. In this paper, both empirically and theoretically, we
                   show that these detectors are not reliable in practical
                   scenarios. Empirically, we show that paraphrasing attacks,
                   where a light paraphraser is applied on top of the
                   generative text model, can break a whole range of detectors,
                   including the ones using the watermarking schemes as well as
                   neural network-based detectors and zero-shot classifiers. We
                   then provide a theoretical impossibility result indicating
                   that for a sufficiently good language model, even the
                   best-possible detector can only perform marginally better
                   than a random classifier. Finally, we show that even LLMs
                   protected by watermarking schemes can be vulnerable against
                   spoofing attacks where adversarial humans can infer hidden
                   watermarking signatures and add them to their generated text
                   to be detected as text generated by the LLMs, potentially
                   causing reputational damages to their developers. We believe
                   these results can open an honest conversation in the
                   community regarding the ethical and reliable use of
                   AI-generated text.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2303.11156"
}

@ARTICLE{Cao2023-dx,
  title         = "A Comprehensive Survey of {AI-Generated} Content ({AIGC)}: A
                   History of Generative {AI} from {GAN} to {ChatGPT}",
  author        = "Cao, Yihan and Li, Siyu and Liu, Yixin and Yan, Zhiling and
                   Dai, Yutong and Yu, Philip S and Sun, Lichao",
  abstract      = "Recently, ChatGPT, along with DALL-E-2 and Codex,has been
                   gaining significant attention from society. As a result,
                   many individuals have become interested in related resources
                   and are seeking to uncover the background and secrets behind
                   its impressive performance. In fact, ChatGPT and other
                   Generative AI (GAI) techniques belong to the category of
                   Artificial Intelligence Generated Content (AIGC), which
                   involves the creation of digital content, such as images,
                   music, and natural language, through AI models. The goal of
                   AIGC is to make the content creation process more efficient
                   and accessible, allowing for the production of high-quality
                   content at a faster pace. AIGC is achieved by extracting and
                   understanding intent information from instructions provided
                   by human, and generating the content according to its
                   knowledge and the intent information. In recent years,
                   large-scale models have become increasingly important in
                   AIGC as they provide better intent extraction and thus,
                   improved generation results. With the growth of data and the
                   size of the models, the distribution that the model can
                   learn becomes more comprehensive and closer to reality,
                   leading to more realistic and high-quality content
                   generation. This survey provides a comprehensive review on
                   the history of generative models, and basic components,
                   recent advances in AIGC from unimodal interaction and
                   multimodal interaction. From the perspective of unimodality,
                   we introduce the generation tasks and relative models of
                   text and image. From the perspective of multimodality, we
                   introduce the cross-application between the modalities
                   mentioned above. Finally, we discuss the existing open
                   problems and future challenges in AIGC.",
  month         =  mar,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2303.04226"
}

@ARTICLE{Madaan2023-gm,
  title         = "{Self-Refine}: Iterative Refinement with {Self-Feedback}",
  author        = "Madaan, Aman and Tandon, Niket and Gupta, Prakhar and
                   Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and
                   Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang,
                   Yiming and Welleck, Sean and Majumder, Bodhisattwa Prasad
                   and Gupta, Shashank and Yazdanbakhsh, Amir and Clark, Peter",
  abstract      = "Like people, LLMs do not always generate the best text for a
                   given generation problem on their first try (e.g.,
                   summaries, answers, explanations). Just as people then
                   refine their text, we introduce SELF-REFINE, a framework for
                   similarly improving initial outputs from LLMs through
                   iterative feedback and refinement. The main idea is to
                   generate an output using an LLM, then allow the same model
                   to provide multi-aspect feedback for its own output;
                   finally, the same model refines its previously generated
                   output given its own feedback. Unlike earlier work, our
                   iterative refinement framework does not require supervised
                   training data or reinforcement learning, and works with a
                   single LLM. We experiment with 7 diverse tasks, ranging from
                   review rewriting to math reasoning, demonstrating that our
                   approach outperforms direct generation. In all tasks,
                   outputs generated with SELF-REFINE are preferred by humans
                   and by automated metrics over those generated directly with
                   GPT-3.5 and GPT-4, improving on average by absolute 20\%
                   across tasks.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2303.17651"
}

@ARTICLE{Gupta2022-ao,
  title         = "{InstructDial}: Improving Zero and Few-shot Generalization
                   in Dialogue through Instruction Tuning",
  author        = "Gupta, Prakhar and Jiao, Cathy and Yeh, Yi-Ting and Mehri,
                   Shikib and Eskenazi, Maxine and Bigham, Jeffrey P",
  abstract      = "Instruction tuning is an emergent paradigm in NLP wherein
                   natural language instructions are leveraged with language
                   models to induce zero-shot performance on unseen tasks.
                   Instructions have been shown to enable good performance on
                   unseen tasks and datasets in both large and small language
                   models. Dialogue is an especially interesting area to
                   explore instruction tuning because dialogue systems perform
                   multiple kinds of tasks related to language (e.g., natural
                   language understanding and generation, domain-specific
                   interaction), yet instruction tuning has not been
                   systematically explored for dialogue-related tasks. We
                   introduce InstructDial, an instruction tuning framework for
                   dialogue, which consists of a repository of 48 diverse
                   dialogue tasks in a unified text-to-text format created from
                   59 openly available dialogue datasets. Next, we explore
                   cross-task generalization ability on models tuned on
                   InstructDial across diverse dialogue tasks. Our analysis
                   reveals that InstructDial enables good zero-shot performance
                   on unseen datasets and tasks such as dialogue evaluation and
                   intent detection, and even better performance in a few-shot
                   setting. To ensure that models adhere to instructions, we
                   introduce novel meta-tasks. We establish benchmark zero-shot
                   and few-shot performance of models trained using the
                   proposed framework on multiple dialogue tasks.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2205.12673"
}

@ARTICLE{Lai2023-gy,
  title         = "{ChatGPT} Beyond English: Towards a Comprehensive Evaluation
                   of Large Language Models in Multilingual Learning",
  author        = "Lai, Viet Dac and Ngo, Nghia Trung and Veyseh, Amir Pouran
                   Ben and Man, Hieu and Dernoncourt, Franck and Bui, Trung and
                   Nguyen, Thien Huu",
  abstract      = "Over the last few years, large language models (LLMs) have
                   emerged as the most important breakthroughs in natural
                   language processing (NLP) that fundamentally transform
                   research and developments in the field. ChatGPT represents
                   one of the most exciting LLM systems developed recently to
                   showcase impressive skills for language generation and
                   highly attract public attention. Among various exciting
                   applications discovered for ChatGPT in English, the model
                   can process and generate texts for multiple languages due to
                   its multilingual training data. Given the broad adoption of
                   ChatGPT for English in different problems and areas, a
                   natural question is whether ChatGPT can also be applied
                   effectively for other languages or it is necessary to
                   develop more language-specific technologies. The answer to
                   this question requires a thorough evaluation of ChatGPT over
                   multiple tasks with diverse languages and large datasets
                   (i.e., beyond reported anecdotes), which is still missing or
                   limited in current research. Our work aims to fill this gap
                   for the evaluation of ChatGPT and similar LLMs to provide
                   more comprehensive information for multilingual NLP
                   applications. While this work will be an ongoing effort to
                   include additional experiments in the future, our current
                   paper evaluates ChatGPT on 7 different tasks, covering 37
                   diverse languages with high, medium, low, and extremely low
                   resources. We also focus on the zero-shot learning setting
                   for ChatGPT to improve reproducibility and better simulate
                   the interactions of general users. Compared to the
                   performance of previous models, our extensive experimental
                   results demonstrate a worse performance of ChatGPT for
                   different NLP tasks and languages, calling for further
                   research to develop better models and understanding for
                   multilingual learning.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.05613"
}

@ARTICLE{Wu2023-rk,
  title         = "{AutoGen}: Enabling {Next-Gen} {LLM} Applications via
                   {Multi-Agent} Conversation Framework",
  author        = "Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran
                   and Zhang, Shaokun and Zhu, Erkang and Li, Beibin and Jiang,
                   Li and Zhang, Xiaoyun and Wang, Chi",
  abstract      = "This technical report presents AutoGen, a new framework that
                   enables development of LLM applications using multiple
                   agents that can converse with each other to solve tasks.
                   AutoGen agents are customizable, conversable, and seamlessly
                   allow human participation. They can operate in various modes
                   that employ combinations of LLMs, human inputs, and tools.
                   AutoGen's design offers multiple advantages: a) it
                   gracefully navigates the strong but imperfect generation and
                   reasoning abilities of these LLMs; b) it leverages human
                   understanding and intelligence, while providing valuable
                   automation through conversations between agents; c) it
                   simplifies and unifies the implementation of complex LLM
                   workflows as automated agent chats. We provide many diverse
                   examples of how developers can easily use AutoGen to
                   effectively solve tasks or build applications, ranging from
                   coding, mathematics, operations research, entertainment,
                   online decision-making, question answering, etc.",
  month         =  aug,
  year          =  2023,
  keywords      = "Agent",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2308.08155"
}

@ARTICLE{Gao2023-lb,
  title         = "{Retrieval-Augmented} Generation for large language models:
                   A survey",
  author        = "Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang
                   and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and
                   Wang, Haofen",
  abstract      = "Large language models (LLMs) demonstrate powerful
                   capabilities, but they still face challenges in practical
                   applications, such as hallucinations, slow knowledge
                   updates, and lack of transparency in answers.
                   Retrieval-Augmented Generation (RAG) refers to the retrieval
                   of relevant information from external knowledge bases before
                   answering questions with LLMs. RAG has been demonstrated to
                   significantly enhance answer accuracy, reduce model
                   hallucination, particularly for knowledge-intensive tasks.
                   By citing sources, users can verify the accuracy of answers
                   and increase trust in model outputs. It also facilitates
                   knowledge updates and the introduction of domain-specific
                   knowledge. RAG effectively combines the parameterized
                   knowledge of LLMs with non-parameterized external knowledge
                   bases, making it one of the most important methods for
                   implementing large language models. This paper outlines the
                   development paradigms of RAG in the era of LLMs, summarizing
                   three paradigms: Naive RAG, Advanced RAG, and Modular RAG.
                   It then provides a summary and organization of the three
                   main components of RAG: retriever, generator, and
                   augmentation methods, along with key technologies in each
                   component. Furthermore, it discusses how to evaluate the
                   effectiveness of RAG models, introducing two evaluation
                   methods for RAG, emphasizing key metrics and abilities for
                   evaluation, and presenting the latest automatic evaluation
                   framework. Finally, potential future research directions are
                   introduced from three aspects: vertical optimization,
                   horizontal scalability, and the technical stack and
                   ecosystem of RAG.",
  month         =  dec,
  year          =  2023,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2312.10997"
}

@ARTICLE{Movva2023-er,
  title         = "Large language models shape and are shaped by society: A
                   survey of arXiv publication patterns",
  author        = "Movva, Rajiv and Balachandar, Sidhika and Peng, Kenny and
                   Agostini, Gabriel and Garg, Nikhil and Pierson, Emma",
  abstract      = "There has been a steep recent increase in the number of
                   large language model (LLM) papers, producing a dramatic
                   shift in the scientific landscape which remains largely
                   undocumented through bibliometric analysis. Here, we analyze
                   388K papers posted on the CS and Stat arXivs, focusing on
                   changes in publication patterns in 2023 vs. 2018-2022. We
                   analyze how the proportion of LLM papers is increasing; the
                   LLM-related topics receiving the most attention; the authors
                   writing LLM papers; how authors' research topics correlate
                   with their backgrounds; the factors distinguishing highly
                   cited LLM papers; and the patterns of international
                   collaboration. We show that LLM research increasingly
                   focuses on societal impacts: there has been an 18x increase
                   in the proportion of LLM-related papers on the Computers and
                   Society sub-arXiv, and authors newly publishing on LLMs are
                   more likely to focus on applications and societal impacts
                   than more experienced authors. LLM research is also shaped
                   by social dynamics: we document gender and academic/industry
                   disparities in the topics LLM authors focus on, and a
                   US/China schism in the collaboration network. Overall, our
                   analysis documents the profound ways in which LLM research
                   both shapes and is shaped by society, attesting to the
                   necessity of sociotechnical lenses.",
  month         =  jul,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  primaryClass  = "cs.DL",
  eprint        = "2307.10700"
}

@ARTICLE{Allen-Zhu2020-fp,
  title         = "Towards Understanding Ensemble, Knowledge Distillation and
                   {Self-Distillation} in Deep Learning",
  author        = "Allen-Zhu, Zeyuan and Li, Yuanzhi",
  abstract      = "We formally study how ensemble of deep learning models can
                   improve test accuracy, and how the superior performance of
                   ensemble can be distilled into a single model using
                   knowledge distillation. We consider the challenging case
                   where the ensemble is simply an average of the outputs of a
                   few independently trained neural networks with the SAME
                   architecture, trained using the SAME algorithm on the SAME
                   data set, and they only differ by the random seeds used in
                   the initialization. We show that ensemble/knowledge
                   distillation in Deep Learning works very differently from
                   traditional learning theory (such as boosting or NTKs,
                   neural tangent kernels). To properly understand them, we
                   develop a theory showing that when data has a structure we
                   refer to as ``multi-view'', then ensemble of independently
                   trained neural networks can provably improve test accuracy,
                   and such superior test accuracy can also be provably
                   distilled into a single model by training a single model to
                   match the output of the ensemble instead of the true label.
                   Our result sheds light on how ensemble works in deep
                   learning in a way that is completely different from
                   traditional theorems, and how the ``dark knowledge'' is
                   hidden in the outputs of the ensemble and can be used in
                   distillation. In the end, we prove that self-distillation
                   can also be viewed as implicitly combining ensemble and
                   knowledge distillation to improve test accuracy.",
  month         =  dec,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2012.09816"
}

@ARTICLE{Nguyen2023-tr,
  title         = "Democratizing {LLMs} for {Low-Resource} Languages by
                   Leveraging their English Dominant Abilities with
                   {Linguistically-Diverse} Prompts",
  author        = "Nguyen, Xuan-Phi and Aljunied, Sharifah Mahani and Joty,
                   Shafiq and Bing, Lidong",
  abstract      = "Large language models (LLMs) are known to effectively
                   perform tasks by simply observing few exemplars. However, in
                   low-resource languages, obtaining such hand-picked exemplars
                   can still be challenging, where unsupervised techniques may
                   be necessary. Moreover, competent generative capabilities of
                   LLMs are observed only in high-resource languages, while
                   their performances among under-represented languages fall
                   behind due to pre-training data imbalance. To elicit LLMs'
                   ability onto low-resource languages without any supervised
                   data, we propose to assemble synthetic exemplars from a
                   diverse set of high-resource languages to prompt the LLMs to
                   translate from any language into English. These prompts are
                   then used to create intra-lingual exemplars to perform tasks
                   in the target languages. Our unsupervised prompting method
                   performs on par with supervised few-shot learning in LLMs of
                   different sizes for translations between English and 13
                   Indic and 21 African low-resource languages. We also show
                   that fine-tuning a 7B model on data generated from our
                   method helps it perform competitively with a 175B model. In
                   non-English translation tasks, our method even outperforms
                   supervised prompting by up to 3 chrF++ in many low-resource
                   languages. When evaluated on zero-shot multilingual
                   summarization, our method surpasses other English-pivoting
                   baselines by up to 4 ROUGE-L and is also favored by GPT-4.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.11372"
}

@INPROCEEDINGS{Mukherjee2020-lp,
  title     = "{{X}treme{D}istil}: Multi-stage Distillation for Massive
               Multilingual Models",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  author    = "Mukherjee, Subhabrata and Hassan Awadallah, Ahmed",
  abstract  = "Deep and large pre-trained language models are the
               state-of-the-art for various natural language processing tasks.
               However, the huge size of these models could be a deterrent to
               using them in practice. Some recent works use knowledge
               distillation to compress these huge models into shallow ones. In
               this work we study knowledge distillation with a focus on
               multilingual Named Entity Recognition (NER). In particular, we
               study several distillation strategies and propose a stage-wise
               optimization scheme leveraging teacher internal representations,
               that is agnostic of teacher architecture, and show that it
               outperforms strategies employed in prior works. Additionally, we
               investigate the role of several factors like the amount of
               unlabeled data, annotation resources, model architecture and
               inference latency to name a few. We show that our approach leads
               to massive compression of teacher models like mBERT by upto 35x
               in terms of parameters and 51x in terms of latency for batch
               inference while retaining 95\% of its F1-score for NER over 41
               languages.",
  publisher = "Association for Computational Linguistics",
  pages     = "2221--2234",
  month     =  jul,
  year      =  2020,
  address   = "Online"
}

@MISC{Zhang_undated-ry,
  title       = "{Auto-UI}: Official implementation for ``You Only Look at
                 Screens: Multimodal {Chain-of-Action} Agents'' (stay tuned and
                 more will be updated)",
  author      = "Zhang, Zhuosheng",
  abstract    = "Official implementation for ``You Only Look at Screens:
                 Multimodal Chain-of-Action Agents'' (stay tuned and more will
                 be updated) - cooelf/Auto-UI: Official implementation for
                 ``You Only Look at Screens: Multimodal Chain-of-Action
                 Agents'' (stay tuned and more will be updated)",
  institution = "Github",
  keywords    = "Agent",
  language    = "en"
}

@ARTICLE{Bai2022-sy,
  title         = "Training a Helpful and Harmless Assistant with Reinforcement
                   Learning from Human Feedback",
  author        = "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell,
                   Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and
                   Fort, Stanislav and Ganguli, Deep and Henighan, Tom and
                   Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson
                   and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and
                   Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan
                   and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and
                   Nanda, Neel and Olsson, Catherine and Amodei, Dario and
                   Brown, Tom and Clark, Jack and McCandlish, Sam and Olah,
                   Chris and Mann, Ben and Kaplan, Jared",
  abstract      = "We apply preference modeling and reinforcement learning from
                   human feedback (RLHF) to finetune language models to act as
                   helpful and harmless assistants. We find this alignment
                   training improves performance on almost all NLP evaluations,
                   and is fully compatible with training for specialized skills
                   such as python coding and summarization. We explore an
                   iterated online mode of training, where preference models
                   and RL policies are updated on a weekly cadence with fresh
                   human feedback data, efficiently improving our datasets and
                   models. Finally, we investigate the robustness of RLHF
                   training, and identify a roughly linear relation between the
                   RL reward and the square root of the KL divergence between
                   the policy and its initialization. Alongside our main
                   results, we perform peripheral analyses on calibration,
                   competing objectives, and the use of OOD detection, compare
                   our models with human writers, and provide samples from our
                   models using prompts appearing in recent related work.",
  month         =  apr,
  year          =  2022,
  keywords      = "LK Lab Interview",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.05862"
}

@ARTICLE{Ouyang2022-dr,
  title         = "Training language models to follow instructions with human
                   feedback",
  author        = "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo
                   and Wainwright, Carroll L and Mishkin, Pamela and Zhang,
                   Chong and Agarwal, Sandhini and Slama, Katarina and Ray,
                   Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser
                   and Miller, Luke and Simens, Maddie and Askell, Amanda and
                   Welinder, Peter and Christiano, Paul and Leike, Jan and
                   Lowe, Ryan",
  abstract      = "Making language models bigger does not inherently make them
                   better at following a user's intent. For example, large
                   language models can generate outputs that are untruthful,
                   toxic, or simply not helpful to the user. In other words,
                   these models are not aligned with their users. In this
                   paper, we show an avenue for aligning language models with
                   user intent on a wide range of tasks by fine-tuning with
                   human feedback. Starting with a set of labeler-written
                   prompts and prompts submitted through the OpenAI API, we
                   collect a dataset of labeler demonstrations of the desired
                   model behavior, which we use to fine-tune GPT-3 using
                   supervised learning. We then collect a dataset of rankings
                   of model outputs, which we use to further fine-tune this
                   supervised model using reinforcement learning from human
                   feedback. We call the resulting models InstructGPT. In human
                   evaluations on our prompt distribution, outputs from the
                   1.3B parameter InstructGPT model are preferred to outputs
                   from the 175B GPT-3, despite having 100x fewer parameters.
                   Moreover, InstructGPT models show improvements in
                   truthfulness and reductions in toxic output generation while
                   having minimal performance regressions on public NLP
                   datasets. Even though InstructGPT still makes simple
                   mistakes, our results show that fine-tuning with human
                   feedback is a promising direction for aligning language
                   models with human intent.",
  month         =  mar,
  year          =  2022,
  keywords      = "LK Lab Interview",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2203.02155"
}

@ARTICLE{Jang2023-xn,
  title         = "Exploring the Benefits of Training Expert Language Models
                   over Instruction Tuning",
  author        = "Jang, Joel and Kim, Seungone and Ye, Seonghyeon and Kim,
                   Doyoung and Logeswaran, Lajanugen and Lee, Moontae and Lee,
                   Kyungjae and Seo, Minjoon",
  abstract      = "Recently, Language Models (LMs) instruction-tuned on
                   multiple tasks, also known as multitask-prompted fine-tuning
                   (MT), have shown the capability to generalize to unseen
                   tasks. Previous work has shown that scaling the number of
                   training tasks is the key component in making stronger MT
                   LMs. In this work, we report an unexpected finding that an
                   expert LM fine-tuned on just a single task can outperform an
                   MT LM trained with 300+ different tasks on 11 different
                   unseen datasets and on 13 datasets of the BIG-bench
                   benchmark by a mean accuracy of 3.20\% and 1.29\%,
                   respectively. This finding casts doubt on the previously
                   held belief that simply scaling the number of tasks makes
                   stronger MT LMs. Leveraging this finding, we further show
                   that this distributed approach of training a separate expert
                   LM per training task instead of a single MT LM for zero-shot
                   inference possesses many benefits including (1) avoiding
                   negative task transfer that often occurs during instruction
                   tuning, (2) being able to continually learn new tasks
                   without having to re-train on previous tasks to avoid
                   catastrophic forgetting, and (3) showing compositional
                   capabilities when merging individual experts together. The
                   code is available at https://github.com/joeljang/ELM.",
  month         =  feb,
  year          =  2023,
  keywords      = "LK Lab Interview",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2302.03202"
}

@ARTICLE{Chung2022-gt,
  title         = "Scaling {Instruction-Finetuned} Language Models",
  author        = "Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph,
                   Barret and Tay, Yi and Fedus, William and Li, Yunxuan and
                   Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha
                   and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun
                   and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha
                   and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin
                   and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and
                   Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai,
                   Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H and
                   Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou,
                   Denny and Le, Quoc V and Wei, Jason",
  abstract      = "Finetuning language models on a collection of datasets
                   phrased as instructions has been shown to improve model
                   performance and generalization to unseen tasks. In this
                   paper we explore instruction finetuning with a particular
                   focus on (1) scaling the number of tasks, (2) scaling the
                   model size, and (3) finetuning on chain-of-thought data. We
                   find that instruction finetuning with the above aspects
                   dramatically improves performance on a variety of model
                   classes (PaLM, T5, U-PaLM), prompting setups (zero-shot,
                   few-shot, CoT), and evaluation benchmarks (MMLU, BBH,
                   TyDiQA, MGSM, open-ended generation). For instance,
                   Flan-PaLM 540B instruction-finetuned on 1.8K tasks
                   outperforms PALM 540B by a large margin (+9.4\% on average).
                   Flan-PaLM 540B achieves state-of-the-art performance on
                   several benchmarks, such as 75.2\% on five-shot MMLU. We
                   also publicly release Flan-T5 checkpoints, which achieve
                   strong few-shot performance even compared to much larger
                   models, such as PaLM 62B. Overall, instruction finetuning is
                   a general method for improving the performance and usability
                   of pretrained language models.",
  month         =  oct,
  year          =  2022,
  keywords      = "LK Lab Interview",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2210.11416"
}

@ARTICLE{Wang2022-qo,
  title         = "{Super-NaturalInstructions}: Generalization via Declarative
                   Instructions on 1600+ {NLP} Tasks",
  author        = "Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi,
                   Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and
                   Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut
                   Selvan and Naik, Atharva and Stap, David and Pathak, Eshaan
                   and Karamanolakis, Giannis and Lai, Haizhi Gary and Purohit,
                   Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia,
                   Kirby and Doshi, Krima and Patel, Maitreya and Pal, Kuntal
                   Kumar and Moradshahi, Mehrad and Parmar, Mihir and Purohit,
                   Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and
                   Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang
                   and Sampat, Shailaja Keyur and Doshi, Savan and Mishra,
                   Siddhartha and Reddy, Sujan and Patro, Sumanta and Dixit,
                   Tanay and Shen, Xudong and Baral, Chitta and Choi, Yejin and
                   Smith, Noah A and Hajishirzi, Hannaneh and Khashabi, Daniel",
  abstract      = "How well can NLP models generalize to a variety of unseen
                   tasks when provided with task instructions? To address this
                   question, we first introduce Super-NaturalInstructions, a
                   benchmark of 1,616 diverse NLP tasks and their
                   expert-written instructions. Our collection covers 76
                   distinct task types, including but not limited to
                   classification, extraction, infilling, sequence tagging,
                   text rewriting, and text composition. This large and diverse
                   collection of tasks enables rigorous benchmarking of
                   cross-task generalization under instructions -- training
                   models to follow instructions on a subset of tasks and
                   evaluating them on the remaining unseen ones. Furthermore,
                   we build Tk-Instruct, a transformer model trained to follow
                   a variety of in-context instructions (plain language task
                   definitions or k-shot examples). Our experiments show that
                   Tk-Instruct outperforms existing instruction-following
                   models such as InstructGPT by over 9\% on our benchmark
                   despite being an order of magnitude smaller. We further
                   analyze generalization as a function of various scaling
                   parameters, such as the number of observed tasks, the number
                   of instances per task, and model sizes. We hope our dataset
                   and model facilitate future progress towards more
                   general-purpose NLP models.",
  month         =  apr,
  year          =  2022,
  keywords      = "LK Lab Interview",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.07705"
}

@ARTICLE{Ye2022-wp,
  title         = "Guess the Instruction! Flipped Learning Makes Language
                   Models Stronger {Zero-Shot} Learners",
  author        = "Ye, Seonghyeon and Kim, Doyoung and Jang, Joel and Shin,
                   Joongbo and Seo, Minjoon",
  abstract      = "Meta-training, which fine-tunes the language model (LM) on
                   various downstream tasks by maximizing the likelihood of the
                   target label given the task instruction and input instance,
                   has improved the zero-shot task generalization performance.
                   However, meta-trained LMs still struggle to generalize to
                   challenging tasks containing novel labels unseen during
                   meta-training. In this paper, we propose Flipped Learning,
                   an alternative method of meta-training which trains the LM
                   to generate the task instruction given the input instance
                   and label. During inference, the LM trained with Flipped
                   Learning, referred to as Flipped, selects the label option
                   that is most likely to generate the task instruction. On 14
                   tasks of the BIG-bench benchmark, the 11B-sized Flipped
                   outperforms zero-shot T0-11B and even a 16 times larger
                   3-shot GPT-3 (175B) on average by 8.4\% and 9.7\% points,
                   respectively. Flipped gives particularly large improvements
                   on tasks with unseen labels, outperforming T0-11B by up to
                   +20\% average F1 score. This indicates that the strong task
                   generalization of Flipped comes from improved generalization
                   to novel labels. We release our code at
                   https://github.com/seonghyeonye/Flipped-Learning.",
  month         =  oct,
  year          =  2022,
  keywords      = "LK Lab Interview",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.02969"
}

@ARTICLE{Wei2021-lp,
  title         = "Finetuned Language Models Are {Zero-Shot} Learners",
  author        = "Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu,
                   Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and
                   Dai, Andrew M and Le, Quoc V",
  abstract      = "This paper explores a simple method for improving the
                   zero-shot learning abilities of language models. We show
                   that instruction tuning -- finetuning language models on a
                   collection of tasks described via instructions --
                   substantially improves zero-shot performance on unseen
                   tasks. We take a 137B parameter pretrained language model
                   and instruction-tune it on over 60 NLP tasks verbalized via
                   natural language instruction templates. We evaluate this
                   instruction-tuned model, which we call FLAN, on unseen task
                   types. FLAN substantially improves the performance of its
                   unmodified counterpart and surpasses zero-shot 175B GPT-3 on
                   20 of 25 tasks that we evaluate. FLAN even outperforms
                   few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ,
                   AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal
                   that number of finetuning datasets, model scale, and natural
                   language instructions are key to the success of instruction
                   tuning.",
  month         =  sep,
  year          =  2021,
  keywords      = "LK Lab Interview",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2109.01652"
}

@ARTICLE{Sanh2021-fj,
  title         = "Multitask Prompted Training Enables {Zero-Shot} Task
                   Generalization",
  author        = "Sanh, Victor and Webson, Albert and Raffel, Colin and Bach,
                   Stephen H and Sutawika, Lintang and Alyafeai, Zaid and
                   Chaffin, Antoine and Stiegler, Arnaud and Le Scao, Teven and
                   Raja, Arun and Dey, Manan and Saiful Bari, M and Xu, Canwen
                   and Thakker, Urmish and Sharma, Shanya Sharma and Szczechla,
                   Eliza and Kim, Taewoon and Chhablani, Gunjan and Nayak,
                   Nihal and Datta, Debajyoti and Chang, Jonathan and Jiang,
                   Mike Tian-Jian and Wang, Han and Manica, Matteo and Shen,
                   Sheng and Yong, Zheng Xin and Pandey, Harshit and Bawden,
                   Rachel and Wang, Thomas and Neeraj, Trishala and Rozen, Jos
                   and Sharma, Abheesht and Santilli, Andrea and Fevry,
                   Thibault and Fries, Jason Alan and Teehan, Ryan and Bers,
                   Tali and Biderman, Stella and Gao, Leo and Wolf, Thomas and
                   Rush, Alexander M",
  abstract      = "Large language models have recently been shown to attain
                   reasonable zero-shot generalization on a diverse set of
                   tasks (Brown et al., 2020). It has been hypothesized that
                   this is a consequence of implicit multitask learning in
                   language models' pretraining (Radford et al., 2019). Can
                   zero-shot generalization instead be directly induced by
                   explicit multitask learning? To test this question at scale,
                   we develop a system for easily mapping any natural language
                   tasks into a human-readable prompted form. We convert a
                   large set of supervised datasets, each with multiple prompts
                   with diverse wording. These prompted datasets allow for
                   benchmarking the ability of a model to perform completely
                   held-out tasks. We fine-tune a pretrained encoder-decoder
                   model (Raffel et al., 2020; Lester et al., 2021) on this
                   multitask mixture covering a wide variety of tasks. The
                   model attains strong zero-shot performance on several
                   standard datasets, often outperforming models up to 16x its
                   size. Further, our approach attains strong performance on a
                   subset of tasks from the BIG-bench benchmark, outperforming
                   models up to 6x its size. All trained models are available
                   at https://github.com/bigscience-workshop/t-zero and all
                   prompts are available at
                   https://github.com/bigscience-workshop/promptsource.",
  month         =  oct,
  year          =  2021,
  keywords      = "LK Lab Interview",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2110.08207"
}

@ARTICLE{Wang2022-pv,
  title         = "Score Jacobian Chaining: Lifting Pretrained {2D} Diffusion
                   Models for {3D} Generation",
  author        = "Wang, Haochen and Du, Xiaodan and Li, Jiahao and Yeh,
                   Raymond A and Shakhnarovich, Greg",
  abstract      = "A diffusion model learns to predict a vector field of
                   gradients. We propose to apply chain rule on the learned
                   gradients, and back-propagate the score of a diffusion model
                   through the Jacobian of a differentiable renderer, which we
                   instantiate to be a voxel radiance field. This setup
                   aggregates 2D scores at multiple camera viewpoints into a 3D
                   score, and repurposes a pretrained 2D model for 3D data
                   generation. We identify a technical challenge of
                   distribution mismatch that arises in this application, and
                   propose a novel estimation mechanism to resolve it. We run
                   our algorithm on several off-the-shelf diffusion image
                   generative models, including the recently released Stable
                   Diffusion trained on the large-scale LAION dataset.",
  month         =  dec,
  year          =  2022,
  keywords      = "CS570 Team Project",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2212.00774"
}

@ARTICLE{Poole2022-nn,
  title         = "{DreamFusion}: {Text-to-3D} using {2D} Diffusion",
  author        = "Poole, Ben and Jain, Ajay and Barron, Jonathan T and
                   Mildenhall, Ben",
  abstract      = "Recent breakthroughs in text-to-image synthesis have been
                   driven by diffusion models trained on billions of image-text
                   pairs. Adapting this approach to 3D synthesis would require
                   large-scale datasets of labeled 3D data and efficient
                   architectures for denoising 3D data, neither of which
                   currently exist. In this work, we circumvent these
                   limitations by using a pretrained 2D text-to-image diffusion
                   model to perform text-to-3D synthesis. We introduce a loss
                   based on probability density distillation that enables the
                   use of a 2D diffusion model as a prior for optimization of a
                   parametric image generator. Using this loss in a
                   DeepDream-like procedure, we optimize a randomly-initialized
                   3D model (a Neural Radiance Field, or NeRF) via gradient
                   descent such that its 2D renderings from random angles
                   achieve a low loss. The resulting 3D model of the given text
                   can be viewed from any angle, relit by arbitrary
                   illumination, or composited into any 3D environment. Our
                   approach requires no 3D training data and no modifications
                   to the image diffusion model, demonstrating the
                   effectiveness of pretrained image diffusion models as
                   priors.",
  month         =  sep,
  year          =  2022,
  keywords      = "CS570 Team Project",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2209.14988"
}

@ARTICLE{Liu2023-zz,
  title         = "Zero-1-to-3: Zero-shot One Image to {3D} Object",
  author        = "Liu, Ruoshi and Wu, Rundi and Van Hoorick, Basile and
                   Tokmakov, Pavel and Zakharov, Sergey and Vondrick, Carl",
  abstract      = "We introduce Zero-1-to-3, a framework for changing the
                   camera viewpoint of an object given just a single RGB image.
                   To perform novel view synthesis in this under-constrained
                   setting, we capitalize on the geometric priors that
                   large-scale diffusion models learn about natural images. Our
                   conditional diffusion model uses a synthetic dataset to
                   learn controls of the relative camera viewpoint, which allow
                   new images to be generated of the same object under a
                   specified camera transformation. Even though it is trained
                   on a synthetic dataset, our model retains a strong zero-shot
                   generalization ability to out-of-distribution datasets as
                   well as in-the-wild images, including impressionist
                   paintings. Our viewpoint-conditioned diffusion approach can
                   further be used for the task of 3D reconstruction from a
                   single image. Qualitative and quantitative experiments show
                   that our method significantly outperforms state-of-the-art
                   single-view 3D reconstruction and novel view synthesis
                   models by leveraging Internet-scale pre-training.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2303.11328"
}

@ARTICLE{Rombach2021-uv,
  title         = "High-resolution image synthesis with latent diffusion models",
  author        = "Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik
                   and Esser, Patrick and Ommer, Bj{\"o}rn",
  abstract      = "By decomposing the image formation process into a sequential
                   application of denoising autoencoders, diffusion models
                   (DMs) achieve state-of-the-art synthesis results on image
                   data and beyond. Additionally, their formulation allows for
                   a guiding mechanism to control the image generation process
                   without retraining. However, since these models typically
                   operate directly in pixel space, optimization of powerful
                   DMs often consumes hundreds of GPU days and inference is
                   expensive due to sequential evaluations. To enable DM
                   training on limited computational resources while retaining
                   their quality and flexibility, we apply them in the latent
                   space of powerful pretrained autoencoders. In contrast to
                   previous work, training diffusion models on such a
                   representation allows for the first time to reach a
                   near-optimal point between complexity reduction and detail
                   preservation, greatly boosting visual fidelity. By
                   introducing cross-attention layers into the model
                   architecture, we turn diffusion models into powerful and
                   flexible generators for general conditioning inputs such as
                   text or bounding boxes and high-resolution synthesis becomes
                   possible in a convolutional manner. Our latent diffusion
                   models (LDMs) achieve a new state of the art for image
                   inpainting and highly competitive performance on various
                   tasks, including unconditional image generation, semantic
                   scene synthesis, and super-resolution, while significantly
                   reducing computational requirements compared to pixel-based
                   DMs. Code is available at
                   https://github.com/CompVis/latent-diffusion .",
  month         =  dec,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2112.10752"
}

@ARTICLE{Ledig2016-hq,
  title         = "{Photo-Realistic} Single Image {Super-Resolution} Using a
                   Generative Adversarial Network",
  author        = "Ledig, Christian and Theis, Lucas and Huszar, Ferenc and
                   Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro
                   and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes
                   and Wang, Zehan and Shi, Wenzhe",
  abstract      = "Despite the breakthroughs in accuracy and speed of single
                   image super-resolution using faster and deeper convolutional
                   neural networks, one central problem remains largely
                   unsolved: how do we recover the finer texture details when
                   we super-resolve at large upscaling factors? The behavior of
                   optimization-based super-resolution methods is principally
                   driven by the choice of the objective function. Recent work
                   has largely focused on minimizing the mean squared
                   reconstruction error. The resulting estimates have high peak
                   signal-to-noise ratios, but they are often lacking
                   high-frequency details and are perceptually unsatisfying in
                   the sense that they fail to match the fidelity expected at
                   the higher resolution. In this paper, we present SRGAN, a
                   generative adversarial network (GAN) for image
                   super-resolution (SR). To our knowledge, it is the first
                   framework capable of inferring photo-realistic natural
                   images for 4x upscaling factors. To achieve this, we propose
                   a perceptual loss function which consists of an adversarial
                   loss and a content loss. The adversarial loss pushes our
                   solution to the natural image manifold using a discriminator
                   network that is trained to differentiate between the
                   super-resolved images and original photo-realistic images.
                   In addition, we use a content loss motivated by perceptual
                   similarity instead of similarity in pixel space. Our deep
                   residual network is able to recover photo-realistic textures
                   from heavily downsampled images on public benchmarks. An
                   extensive mean-opinion-score (MOS) test shows hugely
                   significant gains in perceptual quality using SRGAN. The MOS
                   scores obtained with SRGAN are closer to those of the
                   original high-resolution images than to those obtained with
                   any state-of-the-art method.",
  month         =  sep,
  year          =  2016,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1609.04802"
}

@ARTICLE{Howard2017-yx,
  title         = "{MobileNets}: Efficient Convolutional Neural Networks for
                   Mobile Vision Applications",
  author        = "Howard, Andrew G and Zhu, Menglong and Chen, Bo and
                   Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and
                   Andreetto, Marco and Adam, Hartwig",
  abstract      = "We present a class of efficient models called MobileNets for
                   mobile and embedded vision applications. MobileNets are
                   based on a streamlined architecture that uses depth-wise
                   separable convolutions to build light weight deep neural
                   networks. We introduce two simple global hyper-parameters
                   that efficiently trade off between latency and accuracy.
                   These hyper-parameters allow the model builder to choose the
                   right sized model for their application based on the
                   constraints of the problem. We present extensive experiments
                   on resource and accuracy tradeoffs and show strong
                   performance compared to other popular models on ImageNet
                   classification. We then demonstrate the effectiveness of
                   MobileNets across a wide range of applications and use cases
                   including object detection, finegrain classification, face
                   attributes and large scale geo-localization.",
  month         =  apr,
  year          =  2017,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1704.04861"
}

@ARTICLE{Kendall2017-jr,
  title         = "What uncertainties do we need in Bayesian deep learning for
                   computer vision?",
  author        = "Kendall, Alex and Gal, Yarin",
  abstract      = "There are two major types of uncertainty one can model.
                   Aleatoric uncertainty captures noise inherent in the
                   observations. On the other hand, epistemic uncertainty
                   accounts for uncertainty in the model -- uncertainty which
                   can be explained away given enough data. Traditionally it
                   has been difficult to model epistemic uncertainty in
                   computer vision, but with new Bayesian deep learning tools
                   this is now possible. We study the benefits of modeling
                   epistemic vs. aleatoric uncertainty in Bayesian deep
                   learning models for vision tasks. For this we present a
                   Bayesian deep learning framework combining input-dependent
                   aleatoric uncertainty together with epistemic uncertainty.
                   We study models under the framework with per-pixel semantic
                   segmentation and depth regression tasks. Further, our
                   explicit uncertainty formulation leads to new loss functions
                   for these tasks, which can be interpreted as learned
                   attenuation. This makes the loss more robust to noisy data,
                   also giving new state-of-the-art results on segmentation and
                   depth regression benchmarks.",
  month         =  mar,
  year          =  2017,
  keywords      = "CS570 DL \& CV",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1703.04977"
}

@ARTICLE{Vaswani2017-yp,
  title         = "Attention Is All You Need",
  author        = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                   Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                   Kaiser, Lukasz and Polosukhin, Illia",
  abstract      = "The dominant sequence transduction models are based on
                   complex recurrent or convolutional neural networks in an
                   encoder-decoder configuration. The best performing models
                   also connect the encoder and decoder through an attention
                   mechanism. We propose a new simple network architecture, the
                   Transformer, based solely on attention mechanisms,
                   dispensing with recurrence and convolutions entirely.
                   Experiments on two machine translation tasks show these
                   models to be superior in quality while being more
                   parallelizable and requiring significantly less time to
                   train. Our model achieves 28.4 BLEU on the WMT 2014
                   English-to-German translation task, improving over the
                   existing best results, including ensembles by over 2 BLEU.
                   On the WMT 2014 English-to-French translation task, our
                   model establishes a new single-model state-of-the-art BLEU
                   score of 41.8 after training for 3.5 days on eight GPUs, a
                   small fraction of the training costs of the best models from
                   the literature. We show that the Transformer generalizes
                   well to other tasks by applying it successfully to English
                   constituency parsing both with large and limited training
                   data.",
  month         =  jun,
  year          =  2017,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1706.03762"
}

@ARTICLE{Ji2013-yg,
  title    = "{3D} convolutional neural networks for human action recognition",
  author   = "Ji, Shuiwang and Yang, Ming and Yu, Kai",
  abstract = "We consider the automated recognition of human actions in
              surveillance videos. Most current methods build classifiers based
              on complex handcrafted features computed from the raw inputs.
              Convolutional neural networks (CNNs) are a type of deep model
              that can act directly on the raw inputs. However, such models are
              currently limited to handling 2D inputs. In this paper, we
              develop a novel 3D CNN model for action recognition. This model
              extracts features from both the spatial and the temporal
              dimensions by performing 3D convolutions, thereby capturing the
              motion information encoded in multiple adjacent frames. The
              developed model generates multiple channels of information from
              the input frames, and the final feature representation combines
              information from all channels. To further boost the performance,
              we propose regularizing the outputs with high-level features and
              combining the predictions of a variety of different models. We
              apply the developed models to recognize human actions in the
              real-world environment of airport surveillance videos, and they
              achieve superior performance in comparison to baseline methods.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  35,
  number   =  1,
  pages    = "221--231",
  month    =  jan,
  year     =  2013,
  keywords = "CS570 DL \& CV",
  language = "en"
}

@ARTICLE{Doersch2017-sb,
  title         = "Multi-task {Self-Supervised} Visual Learning",
  author        = "Doersch, Carl and Zisserman, Andrew",
  abstract      = "We investigate methods for combining multiple
                   self-supervised tasks--i.e., supervised tasks where data can
                   be collected without manual labeling--in order to train a
                   single visual representation. First, we provide an
                   apples-to-apples comparison of four different
                   self-supervised tasks using the very deep ResNet-101
                   architecture. We then combine tasks to jointly train a
                   network. We also explore lasso regularization to encourage
                   the network to factorize the information in its
                   representation, and methods for ``harmonizing'' network
                   inputs in order to learn a more unified representation. We
                   evaluate all methods on ImageNet classification, PASCAL VOC
                   detection, and NYU depth prediction. Our results show that
                   deeper networks work better, and that combining tasks--even
                   via a naive multi-head architecture--always improves
                   performance. Our best joint network nearly matches the
                   PASCAL performance of a model pre-trained on ImageNet
                   classification, and matches the ImageNet network on NYU
                   depth prediction.",
  month         =  aug,
  year          =  2017,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1708.07860"
}

@ARTICLE{Hinton2015-md,
  title         = "Distilling the Knowledge in a Neural Network",
  author        = "Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff",
  abstract      = "A very simple way to improve the performance of almost any
                   machine learning algorithm is to train many different models
                   on the same data and then to average their predictions.
                   Unfortunately, making predictions using a whole ensemble of
                   models is cumbersome and may be too computationally
                   expensive to allow deployment to a large number of users,
                   especially if the individual models are large neural nets.
                   Caruana and his collaborators have shown that it is possible
                   to compress the knowledge in an ensemble into a single model
                   which is much easier to deploy and we develop this approach
                   further using a different compression technique. We achieve
                   some surprising results on MNIST and we show that we can
                   significantly improve the acoustic model of a heavily used
                   commercial system by distilling the knowledge in an ensemble
                   of models into a single model. We also introduce a new type
                   of ensemble composed of one or more full models and many
                   specialist models which learn to distinguish fine-grained
                   classes that the full models confuse. Unlike a mixture of
                   experts, these specialist models can be trained rapidly and
                   in parallel.",
  month         =  mar,
  year          =  2015,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1503.02531"
}

@ARTICLE{Mescheder2018-pn,
  title         = "Occupancy Networks: Learning {3D} Reconstruction in Function
                   Space",
  author        = "Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael
                   and Nowozin, Sebastian and Geiger, Andreas",
  abstract      = "With the advent of deep neural networks, learning-based
                   approaches for 3D reconstruction have gained popularity.
                   However, unlike for images, in 3D there is no canonical
                   representation which is both computationally and memory
                   efficient yet allows for representing high-resolution
                   geometry of arbitrary topology. Many of the state-of-the-art
                   learning-based 3D reconstruction approaches can hence only
                   represent very coarse 3D geometry or are limited to a
                   restricted domain. In this paper, we propose Occupancy
                   Networks, a new representation for learning-based 3D
                   reconstruction methods. Occupancy networks implicitly
                   represent the 3D surface as the continuous decision boundary
                   of a deep neural network classifier. In contrast to existing
                   approaches, our representation encodes a description of the
                   3D output at infinite resolution without excessive memory
                   footprint. We validate that our representation can
                   efficiently encode 3D structure and can be inferred from
                   various kinds of input. Our experiments demonstrate
                   competitive results, both qualitatively and quantitatively,
                   for the challenging tasks of 3D reconstruction from single
                   images, noisy point clouds and coarse discrete voxel grids.
                   We believe that occupancy networks will become a useful tool
                   in a wide variety of learning-based 3D tasks.",
  month         =  dec,
  year          =  2018,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1812.03828"
}

@ARTICLE{Qi2016-cz,
  title         = "{PointNet}: Deep Learning on Point Sets for {3D}
                   Classification and Segmentation",
  author        = "Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas,
                   Leonidas J",
  abstract      = "Point cloud is an important type of geometric data
                   structure. Due to its irregular format, most researchers
                   transform such data to regular 3D voxel grids or collections
                   of images. This, however, renders data unnecessarily
                   voluminous and causes issues. In this paper, we design a
                   novel type of neural network that directly consumes point
                   clouds and well respects the permutation invariance of
                   points in the input. Our network, named PointNet, provides a
                   unified architecture for applications ranging from object
                   classification, part segmentation, to scene semantic
                   parsing. Though simple, PointNet is highly efficient and
                   effective. Empirically, it shows strong performance on par
                   or even better than state of the art. Theoretically, we
                   provide analysis towards understanding of what the network
                   has learnt and why the network is robust with respect to
                   input perturbation and corruption.",
  month         =  dec,
  year          =  2016,
  keywords      = "CS570 DL \& CV",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1612.00593"
}

@ARTICLE{Deng2023-so,
  title         = "{Mind2Web}: Towards a Generalist Agent for the Web",
  author        = "Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie
                   and Stevens, Samuel and Wang, Boshi and Sun, Huan and Su, Yu",
  abstract      = "We introduce Mind2Web, the first dataset for developing and
                   evaluating generalist agents for the web that can follow
                   language instructions to complete complex tasks on any
                   website. Existing datasets for web agents either use
                   simulated websites or only cover a limited set of websites
                   and tasks, thus not suitable for generalist web agents. With
                   over 2,000 open-ended tasks collected from 137 websites
                   spanning 31 domains and crowdsourced action sequences for
                   the tasks, Mind2Web provides three necessary ingredients for
                   building generalist web agents: 1) diverse domains,
                   websites, and tasks, 2) use of real-world websites instead
                   of simulated and simplified ones, and 3) a broad spectrum of
                   user interaction patterns. Based on Mind2Web, we conduct an
                   initial exploration of using large language models (LLMs)
                   for building generalist web agents. While the raw HTML of
                   real-world websites are often too large to be fed to LLMs,
                   we show that first filtering it with a small LM
                   significantly improves the effectiveness and efficiency of
                   LLMs. Our solution demonstrates a decent level of
                   performance, even on websites or entire domains the model
                   has never seen before, but there is still a substantial room
                   to improve towards truly generalizable agents. We
                   open-source our dataset, model implementation, and trained
                   models (https://osu-nlp-group.github.io/Mind2Web) to
                   facilitate further research on building a generalist agent
                   for the web.",
  month         =  jun,
  year          =  2023,
  keywords      = "Agent;LM Web Browsing",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.06070"
}

@MISC{Operatorx_undated-sj,
  title       = "{WizardLM}: Family of instruction-following {LLMs} powered by
                 {Evol-Instruct}: {WizardLM}, {WizardCoder}",
  author      = "{operatorx}",
  abstract    = "Family of instruction-following LLMs powered by Evol-Instruct:
                 WizardLM, WizardCoder - nlpxucan/WizardLM: Family of
                 instruction-following LLMs powered by Evol-Instruct: WizardLM,
                 WizardCoder",
  institution = "Github",
  language    = "en"
}

@ARTICLE{Pan2023-yq,
  title         = "Unifying Large Language Models and Knowledge Graphs: A
                   Roadmap",
  author        = "Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen
                   and Wang, Jiapu and Wu, Xindong",
  abstract      = "Large language models (LLMs), such as ChatGPT and GPT4, are
                   making new waves in the field of natural language processing
                   and artificial intelligence, due to their emergent ability
                   and generalizability. However, LLMs are black-box models,
                   which often fall short of capturing and accessing factual
                   knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia
                   and Huapu for example, are structured knowledge models that
                   explicitly store rich factual knowledge. KGs can enhance
                   LLMs by providing external knowledge for inference and
                   interpretability. Meanwhile, KGs are difficult to construct
                   and evolving by nature, which challenges the existing
                   methods in KGs to generate new facts and represent unseen
                   knowledge. Therefore, it is complementary to unify LLMs and
                   KGs together and simultaneously leverage their advantages.
                   In this article, we present a forward-looking roadmap for
                   the unification of LLMs and KGs. Our roadmap consists of
                   three general frameworks, namely, 1) KG-enhanced LLMs, which
                   incorporate KGs during the pre-training and inference phases
                   of LLMs, or for the purpose of enhancing understanding of
                   the knowledge learned by LLMs; 2) LLM-augmented KGs, that
                   leverage LLMs for different KG tasks such as embedding,
                   completion, construction, graph-to-text generation, and
                   question answering; and 3) Synergized LLMs + KGs, in which
                   LLMs and KGs play equal roles and work in a mutually
                   beneficial way to enhance both LLMs and KGs for
                   bidirectional reasoning driven by both data and knowledge.
                   We review and summarize existing efforts within these three
                   frameworks in our roadmap and pinpoint their future research
                   directions.",
  month         =  jun,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.08302"
}

@ARTICLE{Hsieh2023-wf,
  title         = "Distilling {Step-by-Step}! Outperforming Larger Language
                   Models with Less Training Data and Smaller Model Sizes",
  author        = "Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and
                   Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander
                   and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas",
  abstract      = "Deploying large language models (LLMs) is challenging
                   because they are memory inefficient and compute-intensive
                   for practical applications. In reaction, researchers train
                   smaller task-specific models by either finetuning with human
                   labels or distilling using LLM-generated labels. However,
                   finetuning and distillation require large amounts of
                   training data to achieve comparable performance to LLMs. We
                   introduce Distilling step-by-step, a new mechanism that (a)
                   trains smaller models that outperform LLMs, and (b) achieves
                   so by leveraging less training data needed by finetuning or
                   distillation. Our method extracts LLM rationales as
                   additional supervision for small models within a multi-task
                   training framework. We present three findings across 4 NLP
                   benchmarks: First, compared to both finetuning and
                   distillation, our mechanism achieves better performance with
                   much fewer labeled/unlabeled training examples. Second,
                   compared to LLMs, we achieve better performance using
                   substantially smaller model sizes. Third, we reduce both the
                   model size and the amount of data required to outperform
                   LLMs; our 770M T5 model outperforms the 540B PaLM model
                   using only 80\% of available data on a benchmark task.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.02301"
}

@ARTICLE{Kim2023-az,
  title         = "The {CoT} Collection: Improving Zero-shot and Few-shot
                   Learning of Language Models via {Chain-of-Thought}
                   {Fine-Tuning}",
  author        = "Kim, Seungone and Joo, Se June and Kim, Doyoung and Jang,
                   Joel and Ye, Seonghyeon and Shin, Jamin and Seo, Minjoon",
  abstract      = "Large Language Models (LLMs) have shown enhanced
                   capabilities of solving novel tasks by reasoning
                   step-by-step known as Chain-of-Thought (CoT) reasoning; how
                   can we instill the same capability of reasoning step-by-step
                   on unseen tasks into LMs that possess less than <100B
                   parameters? To address this question, we first introduce the
                   CoT Collection, a new instruction-tuning dataset that
                   augments 1.88 million CoT rationales across 1,060 tasks. We
                   show that continually fine-tuning Flan-T5 (3B \& 11B) with
                   the CoT Collection enables the 3B \& 11B LMs to perform CoT
                   better on unseen tasks, leading to an improvement in the
                   average zero-shot accuracy on 27 datasets of the
                   BIG-Bench-Hard benchmark by +4.34\% and +2.44\%,
                   respectively. Furthermore, we show that instruction tuning
                   with CoT allows LMs to possess stronger few-shot learning
                   capabilities, resulting in an improvement of +2.97\% and
                   +2.37\% on 4 domain-specific tasks over Flan-T5 (3B \& 11B),
                   respectively. We make our CoT Collection data and our
                   trained models publicly available at
                   https://github.com/kaist-lklab/CoT-Collection.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.14045"
}

@ARTICLE{Chen2022-zi,
  title         = "Large Language Models are few(1)-shot Table Reasoners",
  author        = "Chen, Wenhu",
  abstract      = "Recent literature has shown that large language models
                   (LLMs) are generally excellent few-shot reasoners to solve
                   text reasoning tasks. However, the capability of LLMs on
                   table reasoning tasks is yet to be explored. In this paper,
                   we aim at understanding how well LLMs can perform
                   table-related tasks with few-shot in-context learning.
                   Specifically, we evaluated LLMs on popular table QA and fact
                   verification datasets like WikiTableQuestion, FetaQA,
                   TabFact, and FEVEROUS and found that LLMs are competent at
                   complex reasoning over table structures, though these models
                   are not pre-trained on any table corpus. When combined with
                   `chain of thoughts' prompting, LLMs can achieve very strong
                   performance with only a 1-shot demonstration, even on par
                   with some SoTA models. We show that LLMs are even more
                   competent at generating comprehensive long-form answers on
                   FetaQA than tuned T5-large. We further manually studied the
                   reasoning chains elicited from LLMs and found that these
                   reasoning chains are highly consistent with the underlying
                   semantic form. We believe that LLMs can serve as a simple
                   yet generic baseline for future research. The code and data
                   are released in https://github.com/wenhuchen/TableCoT.",
  month         =  oct,
  year          =  2022,
  keywords      = "LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.06710"
}

@ARTICLE{Kazemi2022-vy,
  title         = "{LAMBADA}: Backward Chaining for Automated Reasoning in
                   Natural Language",
  author        = "Kazemi, Mehran and Kim, Najoung and Bhatia, Deepti and Xu,
                   Xin and Ramachandran, Deepak",
  abstract      = "Remarkable progress has been made on automated reasoning
                   with natural text, by using Language Models (LMs) and
                   methods such as Chain-of-Thought and Selection-Inference.
                   These techniques search for proofs in the forward direction
                   from axioms to the conclusion, which suffers from a
                   combinatorial explosion of the search space, and thus high
                   failure rates for problems requiring longer chains of
                   reasoning. The classical automated reasoning literature has
                   shown that reasoning in the backward direction (i.e. from
                   the intended conclusion to supporting axioms) is
                   significantly more efficient at proof-finding. Importing
                   this intuition into the LM setting, we develop a Backward
                   Chaining algorithm, called LAMBADA, that decomposes
                   reasoning into four sub-modules. These sub-modules are
                   simply implemented by few-shot prompted LM inference. We
                   show that LAMBADA achieves sizable accuracy boosts over
                   state-of-the-art forward reasoning methods on challenging
                   logical reasoning datasets, particularly when deep and
                   accurate proof chains are required.",
  month         =  dec,
  year          =  2022,
  keywords      = "LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2212.13894"
}

@ARTICLE{Zhang2023-ml,
  title         = "Multimodal {Chain-of-Thought} Reasoning in Language Models",
  author        = "Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai
                   and Karypis, George and Smola, Alex",
  abstract      = "Large language models (LLMs) have shown impressive
                   performance on complex reasoning by leveraging
                   chain-of-thought (CoT) prompting to generate intermediate
                   reasoning chains as the rationale to infer the answer.
                   However, existing CoT studies have focused on the language
                   modality. We propose Multimodal-CoT that incorporates
                   language (text) and vision (images) modalities into a
                   two-stage framework that separates rationale generation and
                   answer inference. In this way, answer inference can leverage
                   better generated rationales that are based on multimodal
                   information. With Multimodal-CoT, our model under 1 billion
                   parameters outperforms the previous state-of-the-art LLM
                   (GPT-3.5) by 16 percentage points (75.17\%->91.68\%
                   accuracy) on the ScienceQA benchmark and even surpasses
                   human performance. Code is publicly available available at
                   https://github.com/amazon-science/mm-cot.",
  month         =  feb,
  year          =  2023,
  keywords      = "LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2302.00923"
}

@ARTICLE{Diao2023-nd,
  title         = "Active Prompting with {Chain-of-Thought} for Large Language
                   Models",
  author        = "Diao, Shizhe and Wang, Pengcheng and Lin, Yong and Zhang,
                   Tong",
  abstract      = "The increasing scale of large language models (LLMs) brings
                   emergent abilities to various complex tasks requiring
                   reasoning, such as arithmetic and commonsense reasoning. It
                   is known that the effective design of task-specific prompts
                   is critical for LLMs' ability to produce high-quality
                   answers. In particular, an effective approach for complex
                   question-and-answer tasks is example-based prompting with
                   chain-of-thought (CoT) reasoning, which significantly
                   improves the performance of LLMs. However, current CoT
                   methods rely on a fixed set of human-annotated exemplars,
                   which are not necessarily the most effective examples for
                   different tasks. This paper proposes a new method,
                   Active-Prompt, to adapt LLMs to different tasks with
                   task-specific example prompts (annotated with human-designed
                   CoT reasoning). For this purpose, we propose a solution to
                   the key problem of determining which questions are the most
                   important and helpful ones to annotate from a pool of
                   task-specific queries. By borrowing ideas from the related
                   problem of uncertainty-based active learning, we introduce
                   several metrics to characterize the uncertainty so as to
                   select the most uncertain questions for annotation.
                   Experimental results demonstrate the superiority of our
                   proposed method, achieving state-of-the-art on eight complex
                   reasoning tasks. Further analyses of different uncertainty
                   metrics, pool sizes, zero-shot learning, and
                   accuracy-uncertainty relationship demonstrate the
                   effectiveness of our method. Our code will be available at
                   https://github.com/shizhediao/active-prompt.",
  month         =  feb,
  year          =  2023,
  keywords      = "Active Learning;LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2302.12246"
}

@ARTICLE{Lu2023-vm,
  title         = "Chameleon: {Plug-and-Play} Compositional Reasoning with
                   Large Language Models",
  author        = "Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel
                   and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and
                   Gao, Jianfeng",
  abstract      = "Large language models (LLMs) have achieved remarkable
                   progress in solving various natural language processing
                   tasks due to emergent reasoning abilities. However, LLMs
                   have inherent limitations as they are incapable of accessing
                   up-to-date information (stored on the Web or in
                   task-specific knowledge bases), using external tools, and
                   performing precise mathematical and logical reasoning. In
                   this paper, we present Chameleon, an AI system that
                   mitigates these limitations by augmenting LLMs with
                   plug-and-play modules for compositional reasoning. Chameleon
                   synthesizes programs by composing various tools (e.g., LLMs,
                   off-the-shelf vision models, web search engines, Python
                   functions, and heuristic-based modules) for accomplishing
                   complex reasoning tasks. At the heart of Chameleon is an
                   LLM-based planner that assembles a sequence of tools to
                   execute to generate the final response. We showcase the
                   effectiveness of Chameleon on two multi-modal
                   knowledge-intensive reasoning tasks: ScienceQA and TabMWP.
                   Chameleon, powered by GPT-4, achieves an 86.54\% overall
                   accuracy on ScienceQA, improving the best published few-shot
                   result by 11.37\%. On TabMWP, GPT-4-powered Chameleon
                   improves the accuracy by 17.0\%, lifting the state of the
                   art to 98.78\%. Our analysis also shows that the
                   GPT-4-powered planner exhibits more consistent and rational
                   tool selection via inferring potential constraints from
                   instructions, compared to a ChatGPT-powered planner.",
  month         =  apr,
  year          =  2023,
  keywords      = "LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.09842"
}

@ARTICLE{Yoran2023-xr,
  title         = "Answering Questions by {Meta-Reasoning} over Multiple Chains
                   of Thought",
  author        = "Yoran, Ori and Wolfson, Tomer and Bogin, Ben and Katz, Uri
                   and Deutch, Daniel and Berant, Jonathan",
  abstract      = "Modern systems for multi-hop question answering (QA)
                   typically break questions into a sequence of reasoning
                   steps, termed chain-of-thought (CoT), before arriving at a
                   final answer. Often, multiple chains are sampled and
                   aggregated through a voting mechanism over the final
                   answers, but the intermediate steps themselves are
                   discarded. While such approaches improve performance, they
                   do not consider the relations between intermediate steps
                   across chains and do not provide a unified explanation for
                   the predicted answer. We introduce Multi-Chain Reasoning
                   (MCR), an approach which prompts large language models to
                   meta-reason over multiple chains of thought, rather than
                   aggregating their answers. MCR examines different reasoning
                   chains, mixes information between them and selects the most
                   relevant facts in generating an explanation and predicting
                   the answer. MCR outperforms strong baselines on 7 multi-hop
                   QA datasets. Moreover, our analysis reveals that MCR
                   explanations exhibit high quality, enabling humans to verify
                   its answers.",
  month         =  apr,
  year          =  2023,
  keywords      = "LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.13007"
}

@ARTICLE{Pan2023-sn,
  title         = "{Logic-LM}: Empowering Large Language Models with Symbolic
                   Solvers for Faithful Logical Reasoning",
  author        = "Pan, Liangming and Albalak, Alon and Wang, Xinyi and Wang,
                   William Yang",
  abstract      = "Large Language Models (LLMs) have shown human-like reasoning
                   abilities but still struggle with complex logical problems.
                   This paper introduces a novel framework, Logic-LM, which
                   integrates LLMs with symbolic reasoning to improve logical
                   problem-solving. Our method first utilizes LLMs to translate
                   a natural language problem into a symbolic formulation.
                   Afterward, a deterministic symbolic solver performs
                   inference on the formulated problem. We also introduce a
                   self-refinement stage, which utilizes the symbolic solver's
                   error messages to revise symbolic formalizations. We
                   demonstrate Logic-LM's effectiveness on four logical
                   reasoning datasets: ProofWriter, PrOntoQA, FOLIO, and
                   LogicalDeduction. Our results show significant improvement
                   compared to LLMs alone, with an average performance boost of
                   62.6\% over standard prompting and 23.5\% over
                   chain-of-thought prompting. Our findings suggest that
                   Logic-LM, by combining LLMs with symbolic logic, offers a
                   promising avenue for faithful logical reasoning. Code and
                   data are publicly available at
                   https://github.com/teacherpeterpan/Logic-LLM.",
  month         =  may,
  year          =  2023,
  keywords      = "LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.12295"
}

@ARTICLE{Li2023-fe,
  title         = "Chain of Knowledge: A Framework for Grounding Large Language
                   Models with Structured Knowledge Bases",
  author        = "Li, Xingxuan and Zhao, Ruochen and Chia, Yew Ken and Ding,
                   Bosheng and Bing, Lidong and Joty, Shafiq and Poria,
                   Soujanya",
  abstract      = "We introduce Chain of Knowledge (CoK), a framework that
                   augments large language models with structured knowledge
                   bases to improve factual correctness and reduce
                   hallucination. Compared to previous works which only
                   retrieve unstructured texts, CoK leverages structured
                   knowledge bases which support complex queries and offer more
                   direct factual statements. To assist large language models
                   to effectively query knowledge bases, we propose a query
                   generator model with contrastive instruction-tuning. As the
                   query generator is separate from the frozen large language
                   model, our framework is modular and thus easily adapted to
                   various knowledge sources and models. Experiments show that
                   our framework significantly enhances the factual correctness
                   of large language models on knowledge-intensive tasks.",
  month         =  may,
  year          =  2023,
  keywords      = "Retrieval;LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.13269"
}

@ARTICLE{Brahman2023-ja,
  title         = "{PlaSma}: Making Small Language Models Better Procedural
                   Knowledge Models for (Counterfactual) Planning",
  author        = "Brahman, Faeze and Bhagavatula, Chandra and Pyatkin,
                   Valentina and Hwang, Jena D and Li, Xiang Lorraine and Arai,
                   Hirona J and Sanyal, Soumya and Sakaguchi, Keisuke and Ren,
                   Xiang and Choi, Yejin",
  abstract      = "Procedural planning, which entails decomposing a high-level
                   goal into a sequence of temporally ordered steps, is an
                   important yet intricate task for machines. It involves
                   integrating common-sense knowledge to reason about complex
                   contextualized situations that are often counterfactual,
                   e.g. ``scheduling a doctor's appointment without a phone''.
                   While current approaches show encouraging results using
                   large language models (LLMs), they are hindered by drawbacks
                   such as costly API calls and reproducibility issues. In this
                   paper, we advocate planning using smaller language models.
                   We present PlaSma, a novel two-pronged approach to endow
                   small language models with procedural knowledge and
                   (counterfactual) planning capabilities. More concretely, we
                   develop symbolic procedural knowledge distillation to
                   enhance the implicit knowledge in small language models and
                   an inference-time algorithm to facilitate more structured
                   and accurate reasoning. In addition, we introduce a novel
                   task, Counterfactual Planning, that requires a revision of a
                   plan to cope with a counterfactual situation. In both the
                   original and counterfactual setting, we show that
                   orders-of-magnitude smaller models (770M-11B parameters) can
                   compete and often surpass their larger teacher models'
                   capabilities.",
  month         =  may,
  year          =  2023,
  keywords      = "LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.19472"
}

@ARTICLE{Xue2023-kk,
  title         = "{RCOT}: Detecting and Rectifying Factual Inconsistency in
                   Reasoning by Reversing {Chain-of-Thought}",
  author        = "Xue, Tianci and Wang, Ziqi and Wang, Zhenhailong and Han,
                   Chi and Yu, Pengfei and Ji, Heng",
  abstract      = "Large language Models (LLMs) have achieved promising
                   performance on arithmetic reasoning tasks by incorporating
                   step-by-step chain-of-thought (CoT) prompting. However, LLMs
                   face challenges in maintaining factual consistency during
                   reasoning, exhibiting tendencies to condition overlooking,
                   question misinterpretation, and condition hallucination over
                   given problems. Existing methods use coarse-grained feedback
                   (e.g., whether the answer is correct) to improve factual
                   consistency. In this work, we propose RCoT (Reversing
                   Chain-of-Thought), a novel method to improve LLMs' reasoning
                   abilities by automatically detecting and rectifying factual
                   inconsistency in LLMs' generated solutions. To detect
                   factual inconsistency, RCoT first asks LLMs to reconstruct
                   the problem based on generated solutions. Then fine-grained
                   comparisons between the original problem and the
                   reconstructed problem expose the factual inconsistency in
                   the original solutions. To rectify the solution, RCoT
                   formulates detected factual inconsistency into fine-grained
                   feedback to guide LLMs in revising solutions. Experimental
                   results demonstrate consistent improvements of RCoT over
                   standard CoT across seven arithmetic datasets. Moreover, we
                   find that manually written fine-grained feedback can
                   dramatically improve LLMs' reasoning abilities (e.g.,
                   ChatGPT reaches 94.6\% accuracy on GSM8K), encouraging the
                   community to further explore the fine-grained feedback
                   generation methods.",
  month         =  may,
  year          =  2023,
  keywords      = "LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.11499"
}

@ARTICLE{Wang2023-qt,
  title         = "Learning to Generate Novel Scientific Directions with
                   Contextualized Literature-based Discovery",
  author        = "Wang, Qingyun and Downey, Doug and Ji, Heng and Hope, Tom",
  abstract      = "Literature-Based Discovery (LBD) aims to discover new
                   scientific knowledge by mining papers and generating
                   hypotheses. Standard LBD is limited to predicting pairwise
                   relations between discrete concepts (e.g., drug-disease
                   links). LBD also ignores critical contexts like experimental
                   settings (e.g., a specific patient population where a drug
                   is evaluated) and background knowledge and motivations that
                   human scientists consider (e.g., to find a drug candidate
                   without specific side effects). We address these limitations
                   with a novel formulation of contextualized-LBD (C-LBD):
                   generating scientific hypotheses in natural language, while
                   grounding them in a context that controls the hypothesis
                   search space. We present a new modeling framework using
                   retrieval of ``inspirations'' from a heterogeneous network
                   of citations and knowledge graph relations, and create a new
                   dataset derived from papers. In automated and human
                   evaluations, our models improve over baselines, including
                   powerful large language models (LLMs), but also reveal
                   challenges on the road to building machines that generate
                   new scientific knowledge.",
  month         =  may,
  year          =  2023,
  keywords      = "Retrieval;LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.14259"
}

@ARTICLE{Jiang2023-fx,
  title         = "{StructGPT}: A General Framework for Large Language Model to
                   Reason over Structured Data",
  author        = "Jiang, Jinhao and Zhou, Kun and Dong, Zican and Ye, Keming
                   and Zhao, Wayne Xin and Wen, Ji-Rong",
  abstract      = "In this paper, we study how to improve the zero-shot
                   reasoning ability of large language models~(LLMs) over
                   structured data in a unified way. Inspired by the study on
                   tool augmentation for LLMs, we develop an
                   \textbackslashemph\{Iterative Reading-then-Reasoning~(IRR)\}
                   approach for solving question answering tasks based on
                   structured data, called \textbackslashtextbf\{StructGPT\}.
                   In our approach, we construct the specialized function to
                   collect relevant evidence from structured data
                   (\textbackslashie \textbackslashemph\{reading\}), and let
                   LLMs concentrate the reasoning task based on the collected
                   information (\textbackslashie
                   \textbackslashemph\{reasoning\}). Specially, we propose an
                   \textbackslashemph\{invoking-linearization-generation\}
                   procedure to support LLMs in reasoning on the structured
                   data with the help of the external interfaces. By iterating
                   this procedures with provided interfaces, our approach can
                   gradually approach the target answer to a given query.
                   Extensive experiments conducted on three types of structured
                   data demonstrate the effectiveness of our approach, which
                   can significantly boost the performance of ChatGPT and
                   achieve comparable performance against the full-data
                   supervised-tuning baselines. Our codes and data are publicly
                   available
                   at~\textbackslashurl\{https://github.com/RUCAIBox/StructGPT\}.",
  month         =  may,
  year          =  2023,
  keywords      = "LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.09645"
}

@ARTICLE{Zhao2023-yn,
  title         = "{Verify-and-Edit}: A {Knowledge-Enhanced} {Chain-of-Thought}
                   Framework",
  author        = "Zhao, Ruochen and Li, Xingxuan and Joty, Shafiq and Qin,
                   Chengwei and Bing, Lidong",
  abstract      = "As large language models (LLMs) have become the norm in NLP,
                   demonstrating good performance in generation and reasoning
                   tasks, one of its most fatal disadvantages is the lack of
                   factual correctness. Generating unfactual texts not only
                   leads to lower performances but also degrades the trust and
                   validity of their applications. Chain-of-Thought (CoT)
                   prompting improves trust and model performance on complex
                   reasoning tasks by generating interpretable reasoning
                   chains, but still suffers from factuality concerns in
                   knowledge-intensive tasks. In this paper, we propose the
                   Verify-and-Edit framework for CoT prompting, which seeks to
                   increase prediction factuality by post-editing reasoning
                   chains according to external knowledge. Building on top of
                   GPT-3, our framework lead to accuracy improvements in
                   multiple open-domain question-answering tasks.",
  month         =  may,
  year          =  2023,
  keywords      = "LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.03268"
}

@ARTICLE{Yu2023-sb,
  title         = "{IfQA}: A Dataset for Open-domain Question Answering under
                   Counterfactual Presuppositions",
  author        = "Yu, Wenhao and Jiang, Meng and Clark, Peter and Sabharwal,
                   Ashish",
  abstract      = "Although counterfactual reasoning is a fundamental aspect of
                   intelligence, the lack of large-scale counterfactual
                   open-domain question-answering (QA) benchmarks makes it
                   difficult to evaluate and improve models on this ability. To
                   address this void, we introduce the first such dataset,
                   named IfQA, where each question is based on a counterfactual
                   presupposition via an ``if'' clause. For example, if Los
                   Angeles was on the east coast of the U.S., what would be the
                   time difference between Los Angeles and Paris? Such
                   questions require models to go beyond retrieving direct
                   factual knowledge from the Web: they must identify the right
                   information to retrieve and reason about an imagined
                   situation that may even go against the facts built into
                   their parameters. The IfQA dataset contains over 3,800
                   questions that were annotated annotated by crowdworkers on
                   relevant Wikipedia passages. Empirical analysis reveals that
                   the IfQA dataset is highly challenging for existing
                   open-domain QA methods, including supervised
                   retrieve-then-read pipeline methods (EM score 36.2), as well
                   as recent few-shot approaches such as chain-of-thought
                   prompting with GPT-3 (EM score 27.4). The unique challenges
                   posed by the IfQA benchmark will push open-domain QA
                   research on both retrieval and counterfactual reasoning
                   fronts.",
  month         =  may,
  year          =  2023,
  keywords      = "Retrieval;LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.14010"
}

@ARTICLE{Mukherjee2023-db,
  title         = "Orca: Progressive Learning from Complex Explanation Traces
                   of {GPT-4}",
  author        = "Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh
                   and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed",
  abstract      = "Recent research has focused on enhancing the capability of
                   smaller models through imitation learning, drawing on the
                   outputs generated by large foundation models (LFMs). A
                   number of issues impact the quality of these models, ranging
                   from limited imitation signals from shallow LFM outputs;
                   small scale homogeneous training data; and most notably a
                   lack of rigorous evaluation resulting in overestimating the
                   small model's capability as they tend to learn to imitate
                   the style, but not the reasoning process of LFMs. To address
                   these challenges, we develop Orca (We are working with our
                   legal team to publicly release a diff of the model weights
                   in accordance with LLaMA's release policy to be published at
                   https://aka.ms/orca-lm), a 13-billion parameter model that
                   learns to imitate the reasoning process of LFMs. Orca learns
                   from rich signals from GPT-4 including explanation traces;
                   step-by-step thought processes; and other complex
                   instructions, guided by teacher assistance from ChatGPT. To
                   promote this progressive learning, we tap into large-scale
                   and diverse imitation data with judicious sampling and
                   selection. Orca surpasses conventional state-of-the-art
                   instruction-tuned models such as Vicuna-13B by more than
                   100\% in complex zero-shot reasoning benchmarks like
                   Big-Bench Hard (BBH) and 42\% on AGIEval. Moreover, Orca
                   reaches parity with ChatGPT on the BBH benchmark and shows
                   competitive performance (4 pts gap with optimized system
                   message) in professional and academic examinations like the
                   SAT, LSAT, GRE, and GMAT, both in zero-shot settings without
                   CoT; while trailing behind GPT-4. Our research indicates
                   that learning from step-by-step explanations, whether these
                   are generated by humans or more advanced AI models, is a
                   promising direction to improve model capabilities and
                   skills.",
  month         =  jun,
  year          =  2023,
  keywords      = "LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.02707"
}

@ARTICLE{Lee2023-bj,
  title         = "Recursion of Thought: A {Divide-and-Conquer} Approach to
                   {Multi-Context} Reasoning with Language Models",
  author        = "Lee, Soochan and Kim, Gunhee",
  abstract      = "Generating intermediate steps, or Chain of Thought (CoT), is
                   an effective way to significantly improve language models'
                   (LM) multi-step reasoning capability. However, the CoT
                   lengths can grow rapidly with the problem complexity, easily
                   exceeding the maximum context size. Instead of increasing
                   the context limit, which has already been heavily
                   investigated, we explore an orthogonal direction: making LMs
                   divide a problem into multiple contexts. We propose a new
                   inference framework, called Recursion of Thought (RoT),
                   which introduces several special tokens that the models can
                   output to trigger context-related operations. Extensive
                   experiments with multiple architectures including GPT-3 show
                   that RoT dramatically improves LMs' inference capability to
                   solve problems, whose solution consists of hundreds of
                   thousands of tokens.",
  month         =  jun,
  year          =  2023,
  keywords      = "LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.06891"
}

@ARTICLE{Ling2023-pt,
  title         = "Deductive Verification of {Chain-of-Thought} Reasoning",
  author        = "Ling, Zhan and Fang, Yunhao and Li, Xuanlin and Huang, Zhiao
                   and Lee, Mingu and Memisevic, Roland and Su, Hao",
  abstract      = "Large Language Models (LLMs) significantly benefit from
                   Chain-of-Thought (CoT) prompting in performing various
                   reasoning tasks. While CoT allows models to produce more
                   comprehensive reasoning processes, its emphasis on
                   intermediate reasoning steps can inadvertently introduce
                   hallucinations and accumulated errors, thereby limiting
                   models' ability to solve complex reasoning tasks. Inspired
                   by how humans engage in careful and meticulous deductive
                   logical reasoning processes to solve tasks, we seek to
                   enable language models to perform explicit and rigorous
                   deductive reasoning, and also ensure the trustworthiness of
                   their reasoning process through self-verification. However,
                   directly verifying the validity of an entire deductive
                   reasoning process is challenging, even with advanced models
                   like ChatGPT. In light of this, we propose to decompose a
                   reasoning verification process into a series of step-by-step
                   subprocesses, each only receiving their necessary context
                   and premises. To facilitate this procedure, we propose
                   Natural Program, a natural language-based deductive
                   reasoning format. Our approach enables models to generate
                   precise reasoning steps where subsequent steps are more
                   rigorously grounded on prior steps. It also empowers
                   language models to carry out reasoning self-verification in
                   a step-by-step manner. By integrating this verification
                   process into each deductive reasoning stage, we
                   significantly enhance the rigor and trustfulness of
                   generated reasoning steps. Along this process, we also
                   improve the answer correctness on complex reasoning tasks.
                   Code will be released at
                   https://github.com/lz1oceani/verify\_cot.",
  month         =  jun,
  year          =  2023,
  keywords      = "LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.03872"
}

@ARTICLE{Wang2023-uh,
  title         = "Voyager: An {Open-Ended} Embodied Agent with Large Language
                   Models",
  author        = "Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar,
                   Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and
                   Anandkumar, Anima",
  abstract      = "We introduce Voyager, the first LLM-powered embodied
                   lifelong learning agent in Minecraft that continuously
                   explores the world, acquires diverse skills, and makes novel
                   discoveries without human intervention. Voyager consists of
                   three key components: 1) an automatic curriculum that
                   maximizes exploration, 2) an ever-growing skill library of
                   executable code for storing and retrieving complex
                   behaviors, and 3) a new iterative prompting mechanism that
                   incorporates environment feedback, execution errors, and
                   self-verification for program improvement. Voyager interacts
                   with GPT-4 via blackbox queries, which bypasses the need for
                   model parameter fine-tuning. The skills developed by Voyager
                   are temporally extended, interpretable, and compositional,
                   which compounds the agent's abilities rapidly and alleviates
                   catastrophic forgetting. Empirically, Voyager shows strong
                   in-context lifelong learning capability and exhibits
                   exceptional proficiency in playing Minecraft. It obtains
                   3.3x more unique items, travels 2.3x longer distances, and
                   unlocks key tech tree milestones up to 15.3x faster than
                   prior SOTA. Voyager is able to utilize the learned skill
                   library in a new Minecraft world to solve novel tasks from
                   scratch, while other techniques struggle to generalize. We
                   open-source our full codebase and prompts at
                   https://voyager.minedojo.org/.",
  month         =  may,
  year          =  2023,
  keywords      = "Agent;Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2305.16291"
}

@ARTICLE{Zhou2023-so,
  title         = "{LIMA}: Less Is More for Alignment",
  author        = "Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer,
                   Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and
                   Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and
                   Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy,
                   Omer",
  abstract      = "Large language models are trained in two stages: (1)
                   unsupervised pretraining from raw text, to learn
                   general-purpose representations, and (2) large scale
                   instruction tuning and reinforcement learning, to better
                   align to end tasks and user preferences. We measure the
                   relative importance of these two stages by training LIMA, a
                   65B parameter LLaMa language model fine-tuned with the
                   standard supervised loss on only 1,000 carefully curated
                   prompts and responses, without any reinforcement learning or
                   human preference modeling. LIMA demonstrates remarkably
                   strong performance, learning to follow specific response
                   formats from only a handful of examples in the training
                   data, including complex queries that range from planning
                   trip itineraries to speculating about alternate history.
                   Moreover, the model tends to generalize well to unseen tasks
                   that did not appear in the training data. In a controlled
                   human study, responses from LIMA are either equivalent or
                   strictly preferred to GPT-4 in 43\% of cases; this statistic
                   is as high as 58\% when compared to Bard and 65\% versus
                   DaVinci003, which was trained with human feedback. Taken
                   together, these results strongly suggest that almost all
                   knowledge in large language models is learned during
                   pretraining, and only limited instruction tuning data is
                   necessary to teach models to produce high quality output.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.11206"
}

@ARTICLE{Zhang2022-hi,
  title         = "A Survey of Active Learning for Natural Language Processing",
  author        = "Zhang, Zhisong and Strubell, Emma and Hovy, Eduard",
  abstract      = "In this work, we provide a survey of active learning (AL)
                   for its applications in natural language processing (NLP).
                   In addition to a fine-grained categorization of query
                   strategies, we also investigate several other important
                   aspects of applying AL to NLP problems. These include AL for
                   structured prediction tasks, annotation cost, model learning
                   (especially with deep neural models), and starting and
                   stopping AL. Finally, we conclude with a discussion of
                   related topics and future directions.",
  month         =  oct,
  year          =  2022,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.10109"
}

@ARTICLE{Liu2023-nz,
  title         = "Trustworthy {LLMs}: a Survey and Guideline for Evaluating
                   Large Language Models' Alignment",
  author        = "Liu, Yang and Yao, Yuanshun and Ton, Jean-Francois and
                   Zhang, Xiaoying and Cheng, Ruocheng Guo Hao and Klochkov,
                   Yegor and Taufiq, Muhammad Faaiz and Li, Hang",
  abstract      = "Ensuring alignment, which refers to making models behave in
                   accordance with human intentions [1,2], has become a
                   critical task before deploying large language models (LLMs)
                   in real-world applications. For instance, OpenAI devoted six
                   months to iteratively aligning GPT-4 before its release [3].
                   However, a major challenge faced by practitioners is the
                   lack of clear guidance on evaluating whether LLM outputs
                   align with social norms, values, and regulations. This
                   obstacle hinders systematic iteration and deployment of
                   LLMs. To address this issue, this paper presents a
                   comprehensive survey of key dimensions that are crucial to
                   consider when assessing LLM trustworthiness. The survey
                   covers seven major categories of LLM trustworthiness:
                   reliability, safety, fairness, resistance to misuse,
                   explainability and reasoning, adherence to social norms, and
                   robustness. Each major category is further divided into
                   several sub-categories, resulting in a total of 29
                   sub-categories. Additionally, a subset of 8 sub-categories
                   is selected for further investigation, where corresponding
                   measurement studies are designed and conducted on several
                   widely-used LLMs. The measurement results indicate that, in
                   general, more aligned models tend to perform better in terms
                   of overall trustworthiness. However, the effectiveness of
                   alignment varies across the different trustworthiness
                   categories considered. This highlights the importance of
                   conducting more fine-grained analyses, testing, and making
                   continuous improvements on LLM alignment. By shedding light
                   on these key dimensions of LLM trustworthiness, this paper
                   aims to provide valuable insights and guidance to
                   practitioners in the field. Understanding and addressing
                   these concerns will be crucial in achieving reliable and
                   ethically sound deployment of LLMs in various applications.",
  month         =  aug,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2308.05374"
}

@ARTICLE{Ren2023-xz,
  title         = "Robots That Ask For Help: Uncertainty Alignment for Large
                   Language Model Planners",
  author        = "Ren, Allen Z and Dixit, Anushri and Bodrova, Alexandra and
                   Singh, Sumeet and Tu, Stephen and Brown, Noah and Xu, Peng
                   and Takayama, Leila and Xia, Fei and Varley, Jake and Xu,
                   Zhenjia and Sadigh, Dorsa and Zeng, Andy and Majumdar,
                   Anirudha",
  abstract      = "Large language models (LLMs) exhibit a wide range of
                   promising capabilities -- from step-by-step planning to
                   commonsense reasoning -- that may provide utility for
                   robots, but remain prone to confidently hallucinated
                   predictions. In this work, we present KnowNo, which is a
                   framework for measuring and aligning the uncertainty of
                   LLM-based planners such that they know when they don't know
                   and ask for help when needed. KnowNo builds on the theory of
                   conformal prediction to provide statistical guarantees on
                   task completion while minimizing human help in complex
                   multi-step planning settings. Experiments across a variety
                   of simulated and real robot setups that involve tasks with
                   different modes of ambiguity (e.g., from spatial to numeric
                   uncertainties, from human preferences to Winograd schemas)
                   show that KnowNo performs favorably over modern baselines
                   (which may involve ensembles or extensive prompt tuning) in
                   terms of improving efficiency and autonomy, while providing
                   formal assurances. KnowNo can be used with LLMs out of the
                   box without model-finetuning, and suggests a promising
                   lightweight approach to modeling uncertainty that can
                   complement and scale with the growing capabilities of
                   foundation models. Website: https://robot-help.github.io",
  month         =  jul,
  year          =  2023,
  keywords      = "LM Web Browsing",
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2307.01928"
}

@ARTICLE{Lin2023-zf,
  title         = "{SwiftSage}: A Generative Agent with Fast and Slow Thinking
                   for Complex Interactive Tasks",
  author        = "Lin, Bill Yuchen and Fu, Yicheng and Yang, Karina and
                   Ammanabrolu, Prithviraj and Brahman, Faeze and Huang, Shiyu
                   and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang",
  abstract      = "We introduce SwiftSage, a novel agent framework inspired by
                   the dual-process theory of human cognition, designed to
                   excel in action planning for complex interactive reasoning
                   tasks. SwiftSage integrates the strengths of behavior
                   cloning and prompting large language models (LLMs) to
                   enhance task completion performance. The framework comprises
                   two primary modules: the Swift module, representing fast and
                   intuitive thinking, and the Sage module, emulating
                   deliberate thought processes. The Swift module is a small
                   encoder-decoder LM fine-tuned on the oracle agent's action
                   trajectories, while the Sage module employs LLMs such as
                   GPT-4 for subgoal planning and grounding. We develop a
                   heuristic method to harmoniously integrate the two modules,
                   resulting in a more efficient and robust problem-solving
                   process. In 30 tasks from the ScienceWorld benchmark,
                   SwiftSage significantly outperforms other methods such as
                   SayCan, ReAct, and Reflexion, demonstrating its
                   effectiveness in solving complex real-world tasks.",
  month         =  may,
  year          =  2023,
  keywords      = "LM Web Browsing",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.17390"
}

@INPROCEEDINGS{Deng2023-bv,
  title     = "Goal Awareness for Conversational {AI}: Proactivity,
               Non-collaborativity, and Beyond",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for
               Computational Linguistics (Volume 6: Tutorial Abstracts)",
  author    = "Deng, Yang and Lei, Wenqiang and Huang, Minlie and Chua,
               Tat-Seng",
  abstract  = "Conversational systems are envisioned to provide social support
               or functional service to human users via natural language
               interactions. Conventional conversation researches mainly focus
               on the responseability of the system, such as dialogue context
               understanding and response generation, but overlooks the design
               of an essential property in intelligent conversations, i.e.,
               goal awareness. The awareness of goals means the state of not
               only being responsive to the users but also aware of the target
               conversational goal and capable of leading the conversation
               towards the goal, which is a significant step towards
               higher-level intelligence and artificial consciousness. It can
               not only largely improve user engagement and service efficiency
               in the conversation, but also empower the system to handle more
               complicated conversation tasks that involve strategical and
               motivational interactions. In this tutorial, we will introduce
               the recent advances on the design of agent's awareness of goals
               in a wide range of conversational systems.",
  publisher = "Association for Computational Linguistics",
  pages     = "1--10",
  month     =  jul,
  year      =  2023,
  address   = "Toronto, Canada",
  keywords  = "LM Web Browsing"
}

@INPROCEEDINGS{Zhao2023-nr,
  title     = "Complex Reasoning in Natural Language",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for
               Computational Linguistics (Volume 6: Tutorial Abstracts)",
  author    = "Zhao, Wenting and Geva, Mor and Lin, Bill Yuchen and Yasunaga,
               Michihiro and Madaan, Aman and Yu, Tao",
  abstract  = "Teaching machines to reason over texts has been a long-standing
               goal of natural language processing (NLP). To this end,
               researchers have designed a diverse set of complex reasoning
               tasks that involve compositional reasoning, knowledge retrieval,
               grounding, commonsense reasoning, etc. A standard choice for
               building systems that perform a desired type of reasoning is to
               fine-tune a pretrained language model (LM) on specific
               downstream tasks. However, recent research has demonstrated that
               such a straightforward approach is often brittle. For example,
               Elazar et al. (2021) and Branco et al. (2021) show that, on
               question-answering (QA) tasks, similar performance can be
               achieved with questions removed from the inputs. Min et al.
               (2019), Chen and Durrett (2019), and Tang et al. (2021) show
               that models trained on multi-hop QA do not generalize to answer
               single-hop questions. The reasoning capabilities of these models
               thus remain at a surface level, i.e., exploiting data patterns.
               Consequently, augmenting LMs with techniques that make them
               robust and effective becomes an active research area. We will
               start the tutorial by providing an overview of complex reasoning
               tasks where the standard application of pretrained language
               models fails. This tutorial then reviews recent promising
               directions for tackling these tasks. Specifically, we focus on
               the following groups of approaches that explicitly consider
               problem structures: (1) knowledge-augmented methods, where the
               knowledge is either incorporated during fine-tuning or
               pretraining; (2) few-shot prompting methods, which effectively
               guide the models to follow instructions; (3) neuro-symbolic
               methods, which produce explicit intermediate representations;
               and, (4) rationale-based methods, one of the most popular forms
               of the neuro-symbolic methods, which highlight subsets of input
               as explanations for individual model predictions.",
  publisher = "Association for Computational Linguistics",
  pages     = "11--20",
  month     =  jul,
  year      =  2023,
  address   = "Toronto, Canada",
  keywords  = "Survey;Reasoning;Retrieval"
}

@ARTICLE{Chang2023-hh,
  title         = "A survey on evaluation of large language models",
  author        = "Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan
                   and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi,
                   Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and
                   Zhang, Yue and Chang, Yi and Yu, Philip S and Yang, Qiang
                   and Xie, Xing",
  abstract      = "Large language models (LLMs) are gaining increasing
                   popularity in both academia and industry, owing to their
                   unprecedented performance in various applications. As LLMs
                   continue to play a vital role in both research and daily
                   use, their evaluation becomes increasingly critical, not
                   only at the task level, but also at the society level for
                   better understanding of their potential risks. Over the past
                   years, significant efforts have been made to examine LLMs
                   from various perspectives. This paper presents a
                   comprehensive review of these evaluation methods for LLMs,
                   focusing on three key dimensions: what to evaluate, where to
                   evaluate, and how to evaluate. Firstly, we provide an
                   overview from the perspective of evaluation tasks,
                   encompassing general natural language processing tasks,
                   reasoning, medical usage, ethics, educations, natural and
                   social sciences, agent applications, and other areas.
                   Secondly, we answer the `where' and `how' questions by
                   diving into the evaluation methods and benchmarks, which
                   serve as crucial components in assessing performance of
                   LLMs. Then, we summarize the success and failure cases of
                   LLMs in different tasks. Finally, we shed light on several
                   future challenges that lie ahead in LLMs evaluation. Our aim
                   is to offer invaluable insights to researchers in the realm
                   of LLMs evaluation, thereby aiding the development of more
                   proficient LLMs. Our key point is that evaluation should be
                   treated as an essential discipline to better assist the
                   development of LLMs. We consistently maintain the related
                   open-source materials at:
                   https://github.com/MLGroupJLU/LLM-eval-survey.",
  month         =  jul,
  year          =  2023,
  keywords      = "Survey",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.03109"
}

@MISC{Hanneke_undated-es,
  title        = "Active learning from theory to practice",
  author       = "Hanneke, Steve",
  howpublished = "\url{https://icml.cc/media/icml-2019/Slides/4341.pdf}",
  note         = "Accessed: 2023-7-19",
  keywords     = "Active Learning"
}

@ARTICLE{Gur2023-oc,
  title         = "A {Real-World} {WebAgent} with Planning, Long Context
                   Understanding, and Program Synthesis",
  author        = "Gur, Izzeddin and Furuta, Hiroki and Huang, Austin and
                   Safdari, Mustafa and Matsuo, Yutaka and Eck, Douglas and
                   Faust, Aleksandra",
  abstract      = "Pre-trained large language models (LLMs) have recently
                   achieved better generalization and sample efficiency in
                   autonomous web navigation. However, the performance on
                   real-world websites has still suffered from (1) open
                   domainness, (2) limited context length, and (3) lack of
                   inductive bias on HTML. We introduce WebAgent, an LLM-driven
                   agent that can complete the tasks on real websites following
                   natural language instructions. WebAgent plans ahead by
                   decomposing instructions into canonical sub-instructions,
                   summarizes long HTML documents into task-relevant snippets,
                   and acts on websites via generated Python programs from
                   those. We design WebAgent with Flan-U-PaLM, for grounded
                   code generation, and HTML-T5, new pre-trained LLMs for long
                   HTML documents using local and global attention mechanisms
                   and a mixture of long-span denoising objectives, for
                   planning and summarization. We empirically demonstrate that
                   our recipe improves the success on a real website by over
                   50\%, and that HTML-T5 is the best model to solve HTML-based
                   tasks; achieving 14.9\% higher success rate than prior SoTA
                   on the MiniWoB web navigation benchmark and better accuracy
                   on offline task planning evaluation.",
  month         =  jul,
  year          =  2023,
  keywords      = "Web Browsing",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2307.12856"
}

@ARTICLE{Gur2022-gk,
  title         = "Understanding {HTML} with Large Language Models",
  author        = "Gur, Izzeddin and Nachum, Ofir and Miao, Yingjie and
                   Safdari, Mustafa and Huang, Austin and Chowdhery, Aakanksha
                   and Narang, Sharan and Fiedel, Noah and Faust, Aleksandra",
  abstract      = "Large language models (LLMs) have shown exceptional
                   performance on a variety of natural language tasks. Yet,
                   their capabilities for HTML understanding -- i.e., parsing
                   the raw HTML of a webpage, with applications to automation
                   of web-based tasks, crawling, and browser-assisted retrieval
                   -- have not been fully explored. We contribute HTML
                   understanding models (fine-tuned LLMs) and an in-depth
                   analysis of their capabilities under three tasks: (i)
                   Semantic Classification of HTML elements, (ii) Description
                   Generation for HTML inputs, and (iii) Autonomous Web
                   Navigation of HTML pages. While previous work has developed
                   dedicated architectures and training procedures for HTML
                   understanding, we show that LLMs pretrained on standard
                   natural language corpora transfer remarkably well to HTML
                   understanding tasks. For instance, fine-tuned LLMs are 12\%
                   more accurate at semantic classification compared to models
                   trained exclusively on the task dataset. Moreover, when
                   fine-tuned on data from the MiniWoB benchmark, LLMs
                   successfully complete 50\% more tasks using 192x less data
                   compared to the previous best supervised model. Out of the
                   LLMs we evaluate, we show evidence that T5-based models are
                   ideal due to their bidirectional encoder-decoder
                   architecture. To promote further research on LLMs for HTML
                   understanding, we create and open-source a large-scale HTML
                   dataset distilled and auto-labeled from CommonCrawl.",
  month         =  oct,
  year          =  2022,
  keywords      = "Web Browsing;Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2210.03945"
}

@ARTICLE{Chia2023-pi,
  title         = "{INSTRUCTEVAL}: Towards Holistic Evaluation of
                   {Instruction-Tuned} Large Language Models",
  author        = "Chia, Yew Ken and Hong, Pengfei and Bing, Lidong and Poria,
                   Soujanya",
  abstract      = "Instruction-tuned large language models have revolutionized
                   natural language processing and have shown great potential
                   in applications such as conversational agents. These models,
                   such as GPT-4, can not only master language but also solve
                   complex tasks in areas like mathematics, coding, medicine,
                   and law. Despite their impressive capabilities, there is
                   still a lack of comprehensive understanding regarding their
                   full potential, primarily due to the black-box nature of
                   many models and the absence of holistic evaluation studies.
                   To address these challenges, we present INSTRUCTEVAL, a more
                   comprehensive evaluation suite designed specifically for
                   instruction-tuned large language models. Unlike previous
                   works, our evaluation involves a rigorous assessment of
                   models based on problem-solving, writing ability, and
                   alignment to human values. We take a holistic approach to
                   analyze various factors affecting model performance,
                   including the pretraining foundation, instruction-tuning
                   data, and training methods. Our findings reveal that the
                   quality of instruction data is the most crucial factor in
                   scaling model performance. While open-source models
                   demonstrate impressive writing abilities, there is
                   substantial room for improvement in problem-solving and
                   alignment. We are encouraged by the rapid development of
                   models by the open-source community, but we also highlight
                   the need for rigorous evaluation to support claims made
                   about these models. Through INSTRUCTEVAL, we aim to foster a
                   deeper understanding of instruction-tuned models and
                   advancements in their capabilities. INSTRUCTEVAL is publicly
                   available at https://github.com/declare-lab/instruct-eval.",
  month         =  jun,
  year          =  2023,
  keywords      = "Agent",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.04757"
}

@ARTICLE{McKenzie2023-ox,
  title         = "Inverse Scaling: When Bigger Isn't Better",
  author        = "McKenzie, Ian R and Lyzhov, Alexander and Pieler, Michael
                   and Parrish, Alicia and Mueller, Aaron and Prabhu, Ameya and
                   McLean, Euan and Kirtland, Aaron and Ross, Alexis and Liu,
                   Alisa and Gritsevskiy, Andrew and Wurgaft, Daniel and
                   Kauffman, Derik and Recchia, Gabriel and Liu, Jiacheng and
                   Cavanagh, Joe and Weiss, Max and Huang, Sicong and {The
                   Floating Droid} and Tseng, Tom and Korbak, Tomasz and Shen,
                   Xudong and Zhang, Yuhui and Zhou, Zhengping and Kim, Najoung
                   and Bowman, Samuel R and Perez, Ethan",
  abstract      = "Work on scaling laws has found that large language models
                   (LMs) show predictable improvements to overall loss with
                   increased scale (model size, training data, and compute).
                   Here, we present evidence for the claim that LMs may show
                   inverse scaling, or worse task performance with increased
                   scale, e.g., due to flaws in the training objective and
                   data. We present empirical evidence of inverse scaling on 11
                   datasets collected by running a public contest, the Inverse
                   Scaling Prize, with a substantial prize pool. Through
                   analysis of the datasets, along with other examples found in
                   the literature, we identify four potential causes of inverse
                   scaling: (i) preference to repeat memorized sequences over
                   following in-context instructions, (ii) imitation of
                   undesirable patterns in the training data, (iii) tasks
                   containing an easy distractor task which LMs could focus on,
                   rather than the harder real task, and (iv) correct but
                   misleading few-shot demonstrations of the task. We release
                   the winning datasets at https://inversescaling.com/data to
                   allow for further investigation of inverse scaling. Our
                   tasks have helped drive the discovery of U-shaped and
                   inverted-U scaling trends, where an initial trend reverses,
                   suggesting that scaling trends are less reliable at
                   predicting the behavior of larger-scale models than
                   previously understood. Overall, our results suggest that
                   there are tasks for which increased model scale alone may
                   not lead to progress, and that more careful thought needs to
                   go into the data and objectives for training language
                   models.",
  month         =  jun,
  year          =  2023,
  keywords      = "Holistic Evaluation;LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.09479"
}

@ARTICLE{Laskar2023-nk,
  title         = "A Systematic Study and Comprehensive Evaluation of {ChatGPT}
                   on Benchmark Datasets",
  author        = "Laskar, Md Tahmid Rahman and Saiful Bari, M and Rahman,
                   Mizanur and Bhuiyan, Md Amran Hossen and Joty, Shafiq and
                   Huang, Jimmy Xiangji",
  abstract      = "The development of large language models (LLMs) such as
                   ChatGPT has brought a lot of attention recently. However,
                   their evaluation in the benchmark academic datasets remains
                   under-explored due to the difficulty of evaluating the
                   generative outputs produced by this model against the ground
                   truth. In this paper, we aim to present a thorough
                   evaluation of ChatGPT's performance on diverse academic
                   datasets, covering tasks like question-answering, text
                   summarization, code generation, commonsense reasoning,
                   mathematical problem-solving, machine translation, bias
                   detection, and ethical considerations. Specifically, we
                   evaluate ChatGPT across 140 tasks and analyze 255K responses
                   it generates in these datasets. This makes our work the
                   largest evaluation of ChatGPT in NLP benchmarks. In short,
                   our study aims to validate the strengths and weaknesses of
                   ChatGPT in various tasks and provide insights for future
                   research using LLMs. We also report a new emergent ability
                   to follow multi-query instructions that we mostly found in
                   ChatGPT and other instruction-tuned models. Our extensive
                   evaluation shows that even though ChatGPT is capable of
                   performing a wide variety of tasks, and may obtain
                   impressive performance in several benchmark datasets, it is
                   still far from achieving the ability to reliably solve many
                   challenging tasks. By providing a thorough assessment of
                   ChatGPT's performance across diverse NLP tasks, this paper
                   sets the stage for a targeted deployment of ChatGPT-like
                   LLMs in real-world applications.",
  month         =  may,
  year          =  2023,
  keywords      = "Holistic Evaluation;LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.18486"
}

@ARTICLE{Amayuelas2023-zc,
  title         = "Knowledge of Knowledge: Exploring {Known-Unknowns}
                   Uncertainty with Large Language Models",
  author        = "Amayuelas, Alfonso and Pan, Liangming and Chen, Wenhu and
                   Wang, William",
  abstract      = "This paper investigates the capabilities of Large Language
                   Models (LLMs) in the context of understanding their own
                   knowledge and measuring their uncertainty. We argue this is
                   an important feature for mitigating hallucinations.
                   Specifically, we focus on addressing
                   \textbackslashtextit\{known-unknown\} questions,
                   characterized by high uncertainty due to the absence of
                   definitive answers. To facilitate our study, we collect a
                   dataset with new Known-Unknown Questions (KUQ) and propose a
                   novel categorization scheme to elucidate the sources of
                   uncertainty. Subsequently, we assess the LLMs' ability to
                   differentiate between known and unknown questions and
                   classify them accordingly. Moreover, we evaluate the quality
                   of their answers in an Open-Ended QA setting. To quantify
                   the uncertainty expressed in the answers, we create a
                   semantic evaluation method that measures the model's
                   accuracy in expressing uncertainty between known vs unknown
                   questions.",
  month         =  may,
  year          =  2023,
  keywords      = "Holistic Evaluation;LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.13712"
}

@ARTICLE{Schaeffer2023-kw,
  title         = "Are Emergent Abilities of Large Language Models a Mirage?",
  author        = "Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi",
  abstract      = "Recent work claims that large language models display
                   emergent abilities, abilities not present in smaller-scale
                   models that are present in larger-scale models. What makes
                   emergent abilities intriguing is two-fold: their sharpness,
                   transitioning seemingly instantaneously from not present to
                   present, and their unpredictability, appearing at seemingly
                   unforeseeable model scales. Here, we present an alternative
                   explanation for emergent abilities: that for a particular
                   task and model family, when analyzing fixed model outputs,
                   emergent abilities appear due to the researcher's choice of
                   metric rather than due to fundamental changes in model
                   behavior with scale. Specifically, nonlinear or
                   discontinuous metrics produce apparent emergent abilities,
                   whereas linear or continuous metrics produce smooth,
                   continuous predictable changes in model performance. We
                   present our alternative explanation in a simple mathematical
                   model, then test it in three complementary ways: we (1)
                   make, test and confirm three predictions on the effect of
                   metric choice using the InstructGPT/GPT-3 family on tasks
                   with claimed emergent abilities; (2) make, test and confirm
                   two predictions about metric choices in a meta-analysis of
                   emergent abilities on BIG-Bench; and (3) show to choose
                   metrics to produce never-before-seen seemingly emergent
                   abilities in multiple vision tasks across diverse deep
                   networks. Via all three analyses, we provide evidence that
                   alleged emergent abilities evaporate with different metrics
                   or with better statistics, and may not be a fundamental
                   property of scaling AI models.",
  month         =  apr,
  year          =  2023,
  keywords      = "Holistic Evaluation;LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2304.15004"
}

@ARTICLE{Shumailov2023-jk,
  title         = "The Curse of Recursion: Training on Generated Data Makes
                   Models Forget",
  author        = "Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and
                   Gal, Yarin and Papernot, Nicolas and Anderson, Ross",
  abstract      = "Stable Diffusion revolutionised image creation from
                   descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated
                   astonishing performance across a variety of language tasks.
                   ChatGPT introduced such language models to the general
                   public. It is now clear that large language models (LLMs)
                   are here to stay, and will bring about drastic change in the
                   whole ecosystem of online text and images. In this paper we
                   consider what the future might hold. What will happen to
                   GPT-\{n\} once LLMs contribute much of the language found
                   online? We find that use of model-generated content in
                   training causes irreversible defects in the resulting
                   models, where tails of the original content distribution
                   disappear. We refer to this effect as Model Collapse and
                   show that it can occur in Variational Autoencoders, Gaussian
                   Mixture Models and LLMs. We build theoretical intuition
                   behind the phenomenon and portray its ubiquity amongst all
                   learned generative models. We demonstrate that it has to be
                   taken seriously if we are to sustain the benefits of
                   training from large-scale data scraped from the web. Indeed,
                   the value of data collected about genuine human interactions
                   with systems will be increasingly valuable in the presence
                   of content generated by LLMs in data crawled from the
                   Internet.",
  month         =  may,
  year          =  2023,
  keywords      = "Holistic Evaluation;LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2305.17493"
}

@ARTICLE{Wang2023-tt,
  title         = "Large Language Models are not Fair Evaluators",
  author        = "Wang, Peiyi and Li, Lei and Chen, Liang and Zhu, Dawei and
                   Lin, Binghuai and Cao, Yunbo and Liu, Qi and Liu, Tianyu and
                   Sui, Zhifang",
  abstract      = "We uncover a systematic bias in the evaluation paradigm of
                   adopting large language models~(LLMs), e.g., GPT-4, as a
                   referee to score the quality of responses generated by
                   candidate models. We find that the quality ranking of
                   candidate responses can be easily hacked by simply altering
                   their order of appearance in the context. This manipulation
                   allows us to skew the evaluation result, making one model
                   appear considerably superior to the other, e.g., vicuna
                   could beat ChatGPT on 66 over 80 tested queries. To address
                   this issue, we propose two simple yet effective calibration
                   strategies: 1) Multiple Evidence Calibration, which requires
                   the evaluator model to generate multiple detailed pieces of
                   evidence before assigning ratings; 2) Balanced Position
                   Calibration, which aggregates results across various orders
                   to determine the final score. Extensive experiments
                   demonstrate that our approach successfully mitigates
                   evaluation bias, resulting in closer alignment with human
                   judgments. To facilitate future research on more robust
                   large language model comparison, we integrate the techniques
                   in the paper into an easy-to-use toolkit
                   \textbackslashemph\{FairEval\}, along with the human
                   annotations.\textbackslashfootnote\{\textbackslashurl\{https://github.com/i-Eval/FairEval\}\}",
  month         =  may,
  year          =  2023,
  keywords      = "Holistic Evaluation;LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.17926"
}

@ARTICLE{Longpre2023-ly,
  title         = "A Pretrainer's Guide to Training Data: Measuring the Effects
                   of Data Age, Domain Coverage, Quality, \& Toxicity",
  author        = "Longpre, Shayne and Yauney, Gregory and Reif, Emily and Lee,
                   Katherine and Roberts, Adam and Zoph, Barret and Zhou, Denny
                   and Wei, Jason and Robinson, Kevin and Mimno, David and
                   Ippolito, Daphne",
  abstract      = "Pretraining is the preliminary and fundamental step in
                   developing capable language models (LM). Despite this,
                   pretraining data design is critically under-documented and
                   often guided by empirically unsupported intuitions. To
                   address this, we pretrain 28 1.5B parameter decoder-only
                   models, training on data curated (1) at different times, (2)
                   with varying toxicity and quality filters, and (3) with
                   different domain compositions. First, we quantify the effect
                   of pretraining data age. A temporal shift between evaluation
                   data and pretraining data leads to performance degradation,
                   which is not overcome by finetuning. Second, we explore the
                   effect of quality and toxicity filters, showing a trade-off
                   between performance on standard benchmarks and risk of toxic
                   generations. Our findings indicate there does not exist a
                   one-size-fits-all solution to filtering training data. We
                   also find that the effects of different types of filtering
                   are not predictable from text domain characteristics.
                   Lastly, we empirically validate that the inclusion of
                   heterogeneous data sources, like books and web, is broadly
                   beneficial and warrants greater prioritization. These
                   findings constitute the largest set of experiments to
                   validate, quantify, and expose many undocumented intuitions
                   about text pretraining, which we hope will help support more
                   informed data-centric decisions in LM development.",
  month         =  may,
  year          =  2023,
  keywords      = "Pretraining;LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.13169"
}

@ARTICLE{Bowman2023-dl,
  title         = "Eight Things to Know about Large Language Models",
  author        = "Bowman, Samuel R",
  abstract      = "The widespread public deployment of large language models
                   (LLMs) in recent months has prompted a wave of new attention
                   and engagement from advocates, policymakers, and scholars
                   from many fields. This attention is a timely response to the
                   many urgent questions that this technology raises, but it
                   can sometimes miss important considerations. This paper
                   surveys the evidence for eight potentially surprising such
                   points: 1. LLMs predictably get more capable with increasing
                   investment, even without targeted innovation. 2. Many
                   important LLM behaviors emerge unpredictably as a byproduct
                   of increasing investment. 3. LLMs often appear to learn and
                   use representations of the outside world. 4. There are no
                   reliable techniques for steering the behavior of LLMs. 5.
                   Experts are not yet able to interpret the inner workings of
                   LLMs. 6. Human performance on a task isn't an upper bound on
                   LLM performance. 7. LLMs need not express the values of
                   their creators nor the values encoded in web text. 8. Brief
                   interactions with LLMs are often misleading.",
  month         =  apr,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.00612"
}

@ARTICLE{Izacard2022-cg,
  title         = "Atlas: Few-shot Learning with Retrieval Augmented Language
                   Models",
  author        = "Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and
                   Hosseini, Lucas and Petroni, Fabio and Schick, Timo and
                   Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian
                   and Grave, Edouard",
  abstract      = "Large language models have shown impressive few-shot results
                   on a wide range of tasks. However, when knowledge is key for
                   such results, as is the case for tasks such as question
                   answering and fact checking, massive parameter counts to
                   store knowledge seem to be needed. Retrieval augmented
                   models are known to excel at knowledge intensive tasks
                   without the need for as many parameters, but it is unclear
                   whether they work in few-shot settings. In this work we
                   present Atlas, a carefully designed and pre-trained
                   retrieval augmented language model able to learn knowledge
                   intensive tasks with very few training examples. We perform
                   evaluations on a wide range of tasks, including MMLU, KILT
                   and NaturalQuestions, and study the impact of the content of
                   the document index, showing that it can easily be updated.
                   Notably, Atlas reaches over 42\% accuracy on Natural
                   Questions using only 64 examples, outperforming a 540B
                   parameters model by 3\% despite having 50x fewer parameters.",
  month         =  aug,
  year          =  2022,
  keywords      = "Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2208.03299"
}

@ARTICLE{Khattab2022-rr,
  title         = "{Demonstrate-Search-Predict}: Composing retrieval and
                   language models for knowledge-intensive {NLP}",
  author        = "Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and
                   Hall, David and Liang, Percy and Potts, Christopher and
                   Zaharia, Matei",
  abstract      = "Retrieval-augmented in-context learning has emerged as a
                   powerful approach for addressing knowledge-intensive tasks
                   using frozen language models (LM) and retrieval models (RM).
                   Existing work has combined these in simple
                   ``retrieve-then-read'' pipelines in which the RM retrieves
                   passages that are inserted into the LM prompt. To begin to
                   fully realize the potential of frozen LMs and RMs, we
                   propose Demonstrate-Search-Predict (DSP), a framework that
                   relies on passing natural language texts in sophisticated
                   pipelines between an LM and an RM. DSP can express
                   high-level programs that bootstrap pipeline-aware
                   demonstrations, search for relevant passages, and generate
                   grounded predictions, systematically breaking down problems
                   into small transformations that the LM and RM can handle
                   more reliably. We have written novel DSP programs for
                   answering questions in open-domain, multi-hop, and
                   conversational settings, establishing in early evaluations
                   new state-of-the-art in-context learning results and
                   delivering 37-120\%, 8-39\%, and 80-290\% relative gains
                   against the vanilla LM (GPT-3.5), a standard
                   retrieve-then-read pipeline, and a contemporaneous self-ask
                   pipeline, respectively. We release DSP at
                   https://github.com/stanfordnlp/dsp",
  month         =  dec,
  year          =  2022,
  keywords      = "Retrieval",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2212.14024"
}

@ARTICLE{Kim2023-uv,
  title         = "{TaskWeb}: Selecting Better Source Tasks for Multi-task
                   {NLP}",
  author        = "Kim, Joongwon and Asai, Akari and Ilharco, Gabriel and
                   Hajishirzi, Hannaneh",
  abstract      = "Recent work in NLP has shown promising results in training
                   models on large amounts of tasks to achieve better
                   generalization. However, it is not well-understood how tasks
                   are related, and how helpful training tasks can be chosen
                   for a new task. In this work, we investigate whether knowing
                   task relationships via pairwise task transfer improves
                   choosing one or more source tasks that help to learn a new
                   target task. We provide TaskWeb, a large-scale benchmark of
                   pairwise task transfers for 22 NLP tasks using three
                   different model types, sizes, and adaptation methods,
                   spanning about 25,000 experiments. Then, we design a new
                   method TaskShop based on our analysis of TaskWeb. TaskShop
                   uses TaskWeb to estimate the benefit of using a source task
                   for learning a new target, and to choose a subset of helpful
                   training tasks for multi-task learning. Our method improves
                   overall rankings and top-k precision of source tasks by 12\%
                   and 29\%, respectively. We also use TaskShop to build
                   smaller multi-task training sets that improve zero-shot
                   performances across 11 different target tasks by at least
                   4.3\%.",
  month         =  may,
  year          =  2023,
  keywords      = "Instruction Tuning;LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.13256"
}

@ARTICLE{Zhang2023-jn,
  title         = "{LLaMA-Adapter}: Efficient fine-tuning of language models
                   with zero-init attention",
  author        = "Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng
                   and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan
                   and Li, Hongsheng and Qiao, Yu",
  abstract      = "We present LLaMA-Adapter, a lightweight adaption method to
                   efficiently fine-tune LLaMA into an instruction-following
                   model. Using 52K self-instruct demonstrations, LLaMA-Adapter
                   only introduces 1.2M learnable parameters upon the frozen
                   LLaMA 7B model, and costs less than one hour for fine-tuning
                   on 8 A100 GPUs. Specifically, we adopt a set of learnable
                   adaption prompts, and prepend them to the word tokens at
                   higher transformer layers. Then, a zero-initialized
                   attention mechanism with zero gating is proposed, which
                   adaptively injects the new instructional cues into LLaMA,
                   while effectively preserves its pre-trained knowledge. With
                   our efficient training, LLaMA-Adapter can generate
                   high-quality responses, comparable to Alpaca with fully
                   fine-tuned 7B parameters. Besides language commands, our
                   approach can be simply extended to multi-modal instructions
                   for learning image-conditioned LLaMA model, which achieves
                   superior reasoning performance on ScienceQA and COCO Caption
                   benchmarks. Furthermore, we also evaluate the
                   zero-initialized attention mechanism for fine-tuning other
                   pre-trained models (ViT, RoBERTa) on traditional vision and
                   language tasks, demonstrating the superior generalization
                   capacity of our approach. Code is released at
                   https://github.com/OpenGVLab/LLaMA-Adapter.",
  month         =  mar,
  year          =  2023,
  keywords      = "Instruction Tuning;Multimodal;LLM + Prompts Season 2",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2303.16199"
}

@ARTICLE{Gao2023-gs,
  title         = "{LLaMA-Adapter} V2: {Parameter-Efficient} Visual Instruction
                   Model",
  author        = "Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi
                   and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan
                   and He, Conghui and Yue, Xiangyu and Li, Hongsheng and Qiao,
                   Yu",
  abstract      = "How to efficiently transform large language models (LLMs)
                   into instruction followers is recently a popular research
                   direction, while training LLM for multi-modal reasoning
                   remains less explored. Although the recent LLaMA-Adapter
                   demonstrates the potential to handle visual inputs with
                   LLMs, it still cannot generalize well to open-ended visual
                   instructions and lags behind GPT-4. In this paper, we
                   present LLaMA-Adapter V2, a parameter-efficient visual
                   instruction model. Specifically, we first augment
                   LLaMA-Adapter by unlocking more learnable parameters (e.g.,
                   norm, bias and scale), which distribute the
                   instruction-following ability across the entire LLaMA model
                   besides adapters. Secondly, we propose an early fusion
                   strategy to feed visual tokens only into the early LLM
                   layers, contributing to better visual knowledge
                   incorporation. Thirdly, a joint training paradigm of
                   image-text pairs and instruction-following data is
                   introduced by optimizing disjoint groups of learnable
                   parameters. This strategy effectively alleviates the
                   interference between the two tasks of image-text alignment
                   and instruction following and achieves strong multi-modal
                   reasoning with only a small-scale image-text and instruction
                   dataset. During inference, we incorporate additional expert
                   models (e.g. captioning/OCR systems) into LLaMA-Adapter to
                   further enhance its image understanding capability without
                   incurring training costs. Compared to the original
                   LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended
                   multi-modal instructions by merely introducing 14M
                   parameters over LLaMA. The newly designed framework also
                   exhibits stronger language-only instruction-following
                   capabilities and even excels in chat interactions. Our code
                   and models are available at
                   https://github.com/ZrrSkywalker/LLaMA-Adapter.",
  month         =  apr,
  year          =  2023,
  keywords      = "Instruction Tuning;Multimodal;LLM + Prompts Season 2",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2304.15010"
}

@ARTICLE{Kaddour2023-qa,
  title         = "Challenges and Applications of Large Language Models",
  author        = "Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and
                   Bradley, Herbie and Raileanu, Roberta and McHardy, Robert",
  abstract      = "Large Language Models (LLMs) went from non-existent to
                   ubiquitous in the machine learning discourse within a few
                   years. Due to the fast pace of the field, it is difficult to
                   identify the remaining challenges and already fruitful
                   application areas. In this paper, we aim to establish a
                   systematic set of open problems and application successes so
                   that ML researchers can comprehend the field's current state
                   more quickly and become productive.",
  month         =  jul,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.10169"
}
